{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmva_logo.gif\" height=\"20%\" width=\"20%\">\n",
    "\n",
    "# TMVA Classification Example Using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory\n",
    "\n",
    "Create the Factory class. Later you can choose the methods\n",
    "whose performance you'd like to investigate. \n",
    "\n",
    "The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass\n",
    "\n",
    " - The first argument is the base of the name of all the output\n",
    "weightfiles in the directory weight/ that will be created with the \n",
    "method parameters \n",
    "\n",
    " - The second argument is the output file for the training results\n",
    "  \n",
    " - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in the option string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.15/01\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.Tools.Instance()\n",
    "TMVA.PyMethodBase.PyInitialize()\n",
    "\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize();\n",
    "\n",
    "\n",
    "outputFile = ROOT.TFile.Open(\"CNN_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_CNN_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input variables \n",
    "\n",
    "Define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************\n",
      "*Tree    :sig_tree  : signal_tree                                            *\n",
      "*Entries :    10000 : Total =         2601382 bytes  File  Size =    2572423 *\n",
      "*        :          : Tree compression factor =   1.00                       *\n",
      "******************************************************************************\n",
      "*Br    0 :var0      : var0/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    1 :var1      : var1/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    2 :var2      : var2/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    3 :var3      : var3/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    4 :var4      : var4/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    5 :var5      : var5/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    6 :var6      : var6/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    7 :var7      : var7/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    8 :var8      : var8/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br    9 :var9      : var9/F                                                 *\n",
      "*Entries :    10000 : Total  Size=      40631 bytes  File Size  =      40150 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   10 :var10     : var10/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   11 :var11     : var11/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   12 :var12     : var12/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   13 :var13     : var13/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   14 :var14     : var14/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   15 :var15     : var15/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   16 :var16     : var16/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   17 :var17     : var17/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   18 :var18     : var18/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   19 :var19     : var19/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   20 :var20     : var20/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   21 :var21     : var21/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   22 :var22     : var22/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   23 :var23     : var23/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   24 :var24     : var24/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   25 :var25     : var25/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   26 :var26     : var26/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   27 :var27     : var27/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   28 :var28     : var28/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   29 :var29     : var29/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   30 :var30     : var30/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   31 :var31     : var31/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   32 :var32     : var32/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   33 :var33     : var33/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   34 :var34     : var34/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   35 :var35     : var35/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   36 :var36     : var36/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   37 :var37     : var37/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   38 :var38     : var38/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   39 :var39     : var39/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   40 :var40     : var40/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   41 :var41     : var41/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   42 :var42     : var42/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   43 :var43     : var43/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   44 :var44     : var44/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   45 :var45     : var45/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   46 :var46     : var46/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   47 :var47     : var47/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   48 :var48     : var48/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   49 :var49     : var49/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   50 :var50     : var50/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   51 :var51     : var51/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   52 :var52     : var52/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   53 :var53     : var53/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   54 :var54     : var54/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   55 :var55     : var55/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   56 :var56     : var56/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   57 :var57     : var57/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   58 :var58     : var58/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   59 :var59     : var59/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   60 :var60     : var60/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   61 :var61     : var61/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   62 :var62     : var62/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n",
      "*Br   63 :var63     : var63/F                                                *\n",
      "*Entries :    10000 : Total  Size=      40637 bytes  File Size  =      40152 *\n",
      "*Baskets :        2 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n"
     ]
    }
   ],
   "source": [
    "inputFileName = \"images_data.root\"\n",
    "\n",
    "inputFile = ROOT.TFile.Open( inputFileName )\n",
    "\n",
    "# retrieve input trees\n",
    "\n",
    "signalTree     = inputFile.Get(\"sig_tree\")\n",
    "backgroundTree = inputFile.Get(\"bkg_tree\")\n",
    "\n",
    "signalTree.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig_tree of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg_tree of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signalTree,     signalWeight     )\n",
    "loader.AddBackgroundTree( backgroundTree, backgroundWeight )\n",
    "\n",
    "imgSize = 8 * 8; \n",
    "for  i in range(0,imgSize):\n",
    "    varName = \"var\"+str(i)\n",
    "    loader.AddVariable(varName,'F');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Define input data file and signal and background trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "loader.PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=5000:nTrain_Background=5000:SplitMode=Random:\"\n",
    "                                   \"NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking Methods\n",
    "\n",
    "Here we book the TMVA methods. We book a DNN and a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booking Deep Neural Network\n",
    "\n",
    "Here we book the new DNN of TMVA. If using master version you can use the new DL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ROOT.TMVA::MethodDL object (\"DL_DENSE\") at 0x252cc50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_DENSE\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER::Architecture=CPU:InputLayout=1|1|64:BatchLayout=1|32|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=None,WeightDecay=1e-4,DropConfig=0.+0.+0.+0.,MaxEpochs=30,ConvergenceSteps=10,BatchSize=32,TestRepetitions=1\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER::Architecture=CPU:InputLayout=1|1|64:BatchLayout=1|32|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=None,WeightDecay=1e-4,DropConfig=0.+0.+0.+0.,MaxEpochs=30,ConvergenceSteps=10,BatchSize=32,TestRepetitions=1\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"G\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|64\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"1|32|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIER\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=None,WeightDecay=1e-4,DropConfig=0.+0.+0.+0.,MaxEpochs=30,ConvergenceSteps=10,BatchSize=32,TestRepetitions=1\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "DL_DENSE                 : [dataset] : Create Transformation \"G\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'var0' <---> Output : variable 'var0'\n",
      "                         : Input : variable 'var1' <---> Output : variable 'var1'\n",
      "                         : Input : variable 'var2' <---> Output : variable 'var2'\n",
      "                         : Input : variable 'var3' <---> Output : variable 'var3'\n",
      "                         : Input : variable 'var4' <---> Output : variable 'var4'\n",
      "                         : Input : variable 'var5' <---> Output : variable 'var5'\n",
      "                         : Input : variable 'var6' <---> Output : variable 'var6'\n",
      "                         : Input : variable 'var7' <---> Output : variable 'var7'\n",
      "                         : Input : variable 'var8' <---> Output : variable 'var8'\n",
      "                         : Input : variable 'var9' <---> Output : variable 'var9'\n",
      "                         : Input : variable 'var10' <---> Output : variable 'var10'\n",
      "                         : Input : variable 'var11' <---> Output : variable 'var11'\n",
      "                         : Input : variable 'var12' <---> Output : variable 'var12'\n",
      "                         : Input : variable 'var13' <---> Output : variable 'var13'\n",
      "                         : Input : variable 'var14' <---> Output : variable 'var14'\n",
      "                         : Input : variable 'var15' <---> Output : variable 'var15'\n",
      "                         : Input : variable 'var16' <---> Output : variable 'var16'\n",
      "                         : Input : variable 'var17' <---> Output : variable 'var17'\n",
      "                         : Input : variable 'var18' <---> Output : variable 'var18'\n",
      "                         : Input : variable 'var19' <---> Output : variable 'var19'\n",
      "                         : Input : variable 'var20' <---> Output : variable 'var20'\n",
      "                         : Input : variable 'var21' <---> Output : variable 'var21'\n",
      "                         : Input : variable 'var22' <---> Output : variable 'var22'\n",
      "                         : Input : variable 'var23' <---> Output : variable 'var23'\n",
      "                         : Input : variable 'var24' <---> Output : variable 'var24'\n",
      "                         : Input : variable 'var25' <---> Output : variable 'var25'\n",
      "                         : Input : variable 'var26' <---> Output : variable 'var26'\n",
      "                         : Input : variable 'var27' <---> Output : variable 'var27'\n",
      "                         : Input : variable 'var28' <---> Output : variable 'var28'\n",
      "                         : Input : variable 'var29' <---> Output : variable 'var29'\n",
      "                         : Input : variable 'var30' <---> Output : variable 'var30'\n",
      "                         : Input : variable 'var31' <---> Output : variable 'var31'\n",
      "                         : Input : variable 'var32' <---> Output : variable 'var32'\n",
      "                         : Input : variable 'var33' <---> Output : variable 'var33'\n",
      "                         : Input : variable 'var34' <---> Output : variable 'var34'\n",
      "                         : Input : variable 'var35' <---> Output : variable 'var35'\n",
      "                         : Input : variable 'var36' <---> Output : variable 'var36'\n",
      "                         : Input : variable 'var37' <---> Output : variable 'var37'\n",
      "                         : Input : variable 'var38' <---> Output : variable 'var38'\n",
      "                         : Input : variable 'var39' <---> Output : variable 'var39'\n",
      "                         : Input : variable 'var40' <---> Output : variable 'var40'\n",
      "                         : Input : variable 'var41' <---> Output : variable 'var41'\n",
      "                         : Input : variable 'var42' <---> Output : variable 'var42'\n",
      "                         : Input : variable 'var43' <---> Output : variable 'var43'\n",
      "                         : Input : variable 'var44' <---> Output : variable 'var44'\n",
      "                         : Input : variable 'var45' <---> Output : variable 'var45'\n",
      "                         : Input : variable 'var46' <---> Output : variable 'var46'\n",
      "                         : Input : variable 'var47' <---> Output : variable 'var47'\n",
      "                         : Input : variable 'var48' <---> Output : variable 'var48'\n",
      "                         : Input : variable 'var49' <---> Output : variable 'var49'\n",
      "                         : Input : variable 'var50' <---> Output : variable 'var50'\n",
      "                         : Input : variable 'var51' <---> Output : variable 'var51'\n",
      "                         : Input : variable 'var52' <---> Output : variable 'var52'\n",
      "                         : Input : variable 'var53' <---> Output : variable 'var53'\n",
      "                         : Input : variable 'var54' <---> Output : variable 'var54'\n",
      "                         : Input : variable 'var55' <---> Output : variable 'var55'\n",
      "                         : Input : variable 'var56' <---> Output : variable 'var56'\n",
      "                         : Input : variable 'var57' <---> Output : variable 'var57'\n",
      "                         : Input : variable 'var58' <---> Output : variable 'var58'\n",
      "                         : Input : variable 'var59' <---> Output : variable 'var59'\n",
      "                         : Input : variable 'var60' <---> Output : variable 'var60'\n",
      "                         : Input : variable 'var61' <---> Output : variable 'var61'\n",
      "                         : Input : variable 'var62' <---> Output : variable 'var62'\n",
      "                         : Input : variable 'var63' <---> Output : variable 'var63'\n",
      "                         : Will use now the CPU architecture !\n"
     ]
    }
   ],
   "source": [
    "inputLayoutString = \"InputLayout=1|1|64\"; \n",
    "batchLayoutString= \"BatchLayout=1|32|64\";\n",
    "layoutString = (\"Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\")\n",
    "\n",
    "training1  = \"Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=None,WeightDecay=1e-4,\"\n",
    "training1 += \"DropConfig=0.+0.+0.+0.,MaxEpochs=30,ConvergenceSteps=10,BatchSize=32,TestRepetitions=1\"\n",
    "trainingStrategyString = \"TrainingStrategy=\" + training1\n",
    "\n",
    "\n",
    "dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=G:WeightInitialization=XAVIER::Architecture=CPU\"\n",
    "\n",
    "dnnOptions +=  \":\" + inputLayoutString\n",
    "dnnOptions +=  \":\" + batchLayoutString\n",
    "dnnOptions +=  \":\" + layoutString\n",
    "dnnOptions +=  \":\" + trainingStrategyString\n",
    "\n",
    "#we can now book the method\n",
    "              \n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kDL, \"DL_DENSE\", dnnOptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_CNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|8|8\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"128|1|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         : Will use now the CPU architecture !\n"
     ]
    }
   ],
   "source": [
    "#input layout \n",
    "inputLayoutString = \"InputLayout=1|8|8\"\n",
    "                                                                                                \n",
    "## Batch Layout                                                                                                                                     \n",
    "batchLayoutString = \"BatchLayout=128|1|64\"\n",
    "                                                   \n",
    "\n",
    "layoutString = (\"Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,\"\n",
    "            \"RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\")\n",
    "                                                                                                                                              \n",
    "\n",
    "\n",
    "##Training strategies.                                                                                                                          \n",
    "training1 = (\"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"\n",
    "                     \"ConvergenceSteps=10,BatchSize=128,TestRepetitions=1,\"\n",
    "                     \"MaxEpochs=20,WeightDecay=1e-4,Regularization=None,\"\n",
    "                     \"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\")\n",
    " \n",
    "trainingStrategyString = \"TrainingStrategy=\" + training1\n",
    "    \n",
    "## General Options.                                                                                                                              \n",
    "cnnOptions = (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                       \"WeightInitialization=XAVIERUNIFORM\");\n",
    "\n",
    "cnnOptions +=  \":\" + inputLayoutString\n",
    "cnnOptions +=  \":\" + batchLayoutString\n",
    "cnnOptions +=  \":\" + layoutString\n",
    "cnnOptions +=  \":\" + trainingStrategyString\n",
    "cnnOptions +=  \":Architecture=CPU\"\n",
    "\n",
    "##book CNN\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kDL, \"DL_CNN\", cnnOptions);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in Keras using a generated model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to use tensorflow backend\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 8, 8, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 10)          100       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 10)          910       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                10304     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 11,444\n",
      "Trainable params: 11,444\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-20 16:28:47.791863: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((8,8, 1), input_shape=(64,)))\n",
    "model.add(Conv2D(10, kernel_size=(3,3), kernel_initializer='TruncatedNormal', activation='relu', padding='same' ) )\n",
    "model.add(Conv2D(10, kernel_size=(3,3), kernel_initializer='TruncatedNormal', activation='relu', padding='same' ) )\n",
    "#stride for maxpool is equal to pool size\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "model.save('model_cnn.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mPyKeras\u001b[0m\n",
      "                         : \n",
      "                         : Load model from file: model_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, \n",
    "                       \"PyKeras\",\"H:!V:VarTransform=None:FilenameModel=model_cnn.h5:\"\n",
    "                       \"FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=20:BatchSize=128\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.6803 - acc: 0.5723 - val_loss: 0.6358 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63580, saving model to trained_model_cnn.h5\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 2s 178us/step - loss: 0.5900 - acc: 0.6876 - val_loss: 0.5714 - val_acc: 0.7026\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63580 to 0.57143, saving model to trained_model_cnn.h5\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.5447 - acc: 0.7246 - val_loss: 0.5413 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57143 to 0.54130, saving model to trained_model_cnn.h5\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5290 - acc: 0.7385 - val_loss: 0.5517 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54130\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.5315 - acc: 0.7329 - val_loss: 0.5421 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54130\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 0.5210 - acc: 0.7412 - val_loss: 0.5322 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54130 to 0.53215, saving model to trained_model_cnn.h5\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 0.5177 - acc: 0.7428 - val_loss: 0.5284 - val_acc: 0.7313\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.53215 to 0.52839, saving model to trained_model_cnn.h5\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 0.5129 - acc: 0.7430 - val_loss: 0.5410 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52839\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 0.5129 - acc: 0.7454 - val_loss: 0.5303 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.52839\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5078 - acc: 0.7487 - val_loss: 0.5533 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.52839\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5123 - acc: 0.7455 - val_loss: 0.5240 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.52839 to 0.52399, saving model to trained_model_cnn.h5\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5065 - acc: 0.7496 - val_loss: 0.5485 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.52399\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5165 - acc: 0.7447 - val_loss: 0.5263 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.52399\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5003 - acc: 0.7521 - val_loss: 0.5331 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.52399\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.5018 - acc: 0.7548 - val_loss: 0.5271 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.52399\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.5084 - acc: 0.7504 - val_loss: 0.5295 - val_acc: 0.7338\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.52399\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.4980 - acc: 0.7576 - val_loss: 0.5279 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.52399\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.4993 - acc: 0.7536 - val_loss: 0.5637 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.52399\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 0.4958 - acc: 0.7578 - val_loss: 0.5265 - val_acc: 0.7386\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.52399\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 0.4930 - acc: 0.7583 - val_loss: 0.5312 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.52399\n",
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 5000\n",
      "                         : Signal     -- testing events             : 5000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 5000\n",
      "                         : Background -- testing events             : 5000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n",
      "DataSetInfo              : Correlation matrix (Signal):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.270  +0.261  +0.210  +0.091  -0.039  -0.087  -0.103  +0.279  +0.321  +0.296  +0.202  +0.085  -0.098  -0.151  -0.145  +0.308  +0.301  +0.313  +0.190  +0.010  -0.095  -0.191  -0.184  +0.287  +0.276  +0.252  +0.113  -0.070  -0.217  -0.250  -0.221  +0.227  +0.233  +0.188  +0.026  -0.167  -0.252  -0.301  -0.233  +0.177  +0.155  +0.078  -0.048  -0.203  -0.273  -0.293  -0.241  +0.110  +0.075  +0.018  -0.103  -0.219  -0.261  -0.255  -0.226  +0.059  +0.014  -0.029  -0.113  -0.197  -0.236  -0.220  -0.184\n",
      "                         :    var1:  +0.270  +1.000  +0.360  +0.290  +0.176  +0.029  -0.055  -0.073  +0.345  +0.395  +0.380  +0.288  +0.164  -0.016  -0.082  -0.147  +0.337  +0.370  +0.370  +0.274  +0.074  -0.097  -0.178  -0.174  +0.302  +0.325  +0.294  +0.182  -0.076  -0.204  -0.269  -0.239  +0.241  +0.229  +0.188  +0.023  -0.196  -0.292  -0.316  -0.270  +0.154  +0.141  +0.050  -0.135  -0.282  -0.348  -0.323  -0.294  +0.065  +0.037  -0.056  -0.181  -0.303  -0.325  -0.327  -0.266  +0.036  +0.000  -0.084  -0.205  -0.269  -0.302  -0.290  -0.233\n",
      "                         :    var2:  +0.261  +0.360  +1.000  +0.348  +0.251  +0.147  +0.027  +0.004  +0.303  +0.379  +0.395  +0.352  +0.248  +0.072  -0.010  -0.069  +0.279  +0.335  +0.375  +0.293  +0.154  +0.006  -0.074  -0.109  +0.252  +0.262  +0.228  +0.184  +0.006  -0.129  -0.193  -0.186  +0.152  +0.158  +0.122  -0.026  -0.199  -0.271  -0.259  -0.237  +0.070  +0.037  -0.033  -0.190  -0.284  -0.335  -0.297  -0.267  -0.002  -0.043  -0.136  -0.232  -0.344  -0.364  -0.308  -0.262  -0.054  -0.080  -0.163  -0.270  -0.307  -0.353  -0.303  -0.206\n",
      "                         :    var3:  +0.210  +0.290  +0.348  +1.000  +0.336  +0.248  +0.160  +0.080  +0.217  +0.299  +0.342  +0.364  +0.332  +0.211  +0.114  +0.058  +0.167  +0.216  +0.269  +0.298  +0.228  +0.147  +0.057  +0.003  +0.117  +0.136  +0.148  +0.122  +0.071  -0.001  -0.046  -0.053  +0.025  +0.014  -0.035  -0.076  -0.134  -0.153  -0.145  -0.124  -0.030  -0.103  -0.160  -0.238  -0.293  -0.259  -0.203  -0.187  -0.108  -0.170  -0.253  -0.329  -0.361  -0.323  -0.263  -0.224  -0.129  -0.213  -0.274  -0.333  -0.342  -0.308  -0.282  -0.186\n",
      "                         :    var4:  +0.091  +0.176  +0.251  +0.336  +1.000  +0.333  +0.279  +0.189  +0.088  +0.151  +0.215  +0.319  +0.362  +0.333  +0.283  +0.187  +0.033  +0.060  +0.131  +0.258  +0.270  +0.278  +0.207  +0.173  -0.025  -0.038  -0.004  +0.074  +0.123  +0.146  +0.124  +0.090  -0.114  -0.148  -0.161  -0.142  -0.084  -0.039  +0.003  +0.014  -0.165  -0.217  -0.256  -0.283  -0.247  -0.149  -0.093  -0.061  -0.222  -0.253  -0.329  -0.365  -0.315  -0.247  -0.155  -0.111  -0.194  -0.263  -0.321  -0.346  -0.331  -0.275  -0.179  -0.113\n",
      "                         :    var5:  -0.039  +0.029  +0.147  +0.248  +0.333  +1.000  +0.326  +0.283  -0.036  -0.006  +0.087  +0.242  +0.355  +0.386  +0.372  +0.277  -0.102  -0.085  +0.007  +0.162  +0.278  +0.356  +0.311  +0.284  -0.159  -0.172  -0.143  +0.001  +0.161  +0.258  +0.257  +0.218  -0.225  -0.268  -0.261  -0.180  +0.011  +0.109  +0.155  +0.153  -0.260  -0.309  -0.331  -0.273  -0.177  -0.027  +0.062  +0.079  -0.247  -0.316  -0.356  -0.323  -0.248  -0.122  -0.032  -0.011  -0.213  -0.295  -0.321  -0.317  -0.261  -0.162  -0.101  -0.042\n",
      "                         :    var6:  -0.087  -0.055  +0.027  +0.160  +0.279  +0.326  +1.000  +0.280  -0.126  -0.101  -0.015  +0.145  +0.285  +0.384  +0.377  +0.333  -0.170  -0.164  -0.098  +0.073  +0.255  +0.344  +0.372  +0.342  -0.233  -0.253  -0.204  -0.063  +0.151  +0.293  +0.315  +0.284  -0.281  -0.289  -0.304  -0.190  +0.026  +0.145  +0.230  +0.222  -0.268  -0.331  -0.342  -0.263  -0.102  +0.056  +0.152  +0.131  -0.274  -0.318  -0.331  -0.275  -0.140  -0.033  +0.065  +0.082  -0.211  -0.268  -0.308  -0.274  -0.174  -0.090  -0.001  +0.009\n",
      "                         :    var7:  -0.103  -0.073  +0.004  +0.080  +0.189  +0.283  +0.280  +1.000  -0.126  -0.118  -0.062  +0.085  +0.198  +0.295  +0.331  +0.264  -0.181  -0.158  -0.103  +0.021  +0.175  +0.298  +0.311  +0.277  -0.203  -0.222  -0.203  -0.092  +0.124  +0.256  +0.291  +0.252  -0.224  -0.284  -0.261  -0.158  +0.046  +0.166  +0.216  +0.219  -0.244  -0.269  -0.278  -0.208  -0.062  +0.081  +0.158  +0.153  -0.211  -0.265  -0.250  -0.207  -0.094  +0.003  +0.067  +0.095  -0.194  -0.202  -0.214  -0.198  -0.127  -0.034  +0.033  +0.036\n",
      "                         :    var8:  +0.279  +0.345  +0.303  +0.217  +0.088  -0.036  -0.126  -0.126  +1.000  +0.369  +0.340  +0.222  +0.051  -0.128  -0.185  -0.183  +0.364  +0.377  +0.357  +0.213  -0.020  -0.172  -0.240  -0.252  +0.342  +0.371  +0.321  +0.158  -0.105  -0.271  -0.314  -0.280  +0.283  +0.305  +0.227  +0.064  -0.190  -0.323  -0.355  -0.297  +0.227  +0.215  +0.139  -0.058  -0.217  -0.345  -0.351  -0.296  +0.175  +0.132  +0.044  -0.099  -0.258  -0.296  -0.320  -0.257  +0.099  +0.062  -0.001  -0.088  -0.221  -0.277  -0.278  -0.185\n",
      "                         :    var9:  +0.321  +0.395  +0.379  +0.299  +0.151  -0.006  -0.101  -0.118  +0.369  +1.000  +0.428  +0.311  +0.119  -0.077  -0.167  -0.181  +0.385  +0.432  +0.418  +0.271  +0.035  -0.159  -0.238  -0.245  +0.370  +0.385  +0.346  +0.197  -0.096  -0.261  -0.337  -0.306  +0.297  +0.329  +0.248  +0.055  -0.210  -0.354  -0.384  -0.347  +0.226  +0.221  +0.113  -0.079  -0.304  -0.397  -0.396  -0.354  +0.140  +0.092  +0.014  -0.149  -0.332  -0.379  -0.385  -0.313  +0.074  +0.041  -0.051  -0.183  -0.290  -0.345  -0.354  -0.264\n",
      "                         :   var10:  +0.296  +0.380  +0.395  +0.342  +0.215  +0.087  -0.015  -0.062  +0.340  +0.428  +1.000  +0.360  +0.212  +0.037  -0.061  -0.124  +0.325  +0.399  +0.406  +0.309  +0.112  -0.064  -0.172  -0.185  +0.311  +0.340  +0.313  +0.201  -0.027  -0.195  -0.261  -0.260  +0.221  +0.241  +0.169  +0.019  -0.199  -0.324  -0.340  -0.310  +0.158  +0.128  +0.016  -0.141  -0.327  -0.397  -0.376  -0.340  +0.069  +0.025  -0.065  -0.212  -0.344  -0.396  -0.388  -0.317  +0.002  -0.055  -0.125  -0.253  -0.326  -0.365  -0.345  -0.288\n",
      "                         :   var11:  +0.202  +0.288  +0.352  +0.364  +0.319  +0.242  +0.145  +0.085  +0.222  +0.311  +0.360  +1.000  +0.329  +0.186  +0.115  +0.036  +0.166  +0.234  +0.295  +0.299  +0.209  +0.117  +0.034  -0.003  +0.129  +0.148  +0.156  +0.150  +0.041  -0.017  -0.085  -0.095  +0.043  +0.052  +0.010  -0.050  -0.140  -0.193  -0.181  -0.167  +0.003  -0.050  -0.124  -0.222  -0.308  -0.282  -0.235  -0.223  -0.077  -0.140  -0.214  -0.312  -0.348  -0.340  -0.290  -0.230  -0.105  -0.172  -0.229  -0.315  -0.349  -0.343  -0.293  -0.217\n",
      "                         :   var12:  +0.085  +0.164  +0.248  +0.332  +0.362  +0.355  +0.285  +0.198  +0.051  +0.119  +0.212  +0.329  +1.000  +0.347  +0.295  +0.195  -0.009  +0.041  +0.133  +0.257  +0.311  +0.281  +0.237  +0.171  -0.057  -0.048  -0.023  +0.087  +0.142  +0.158  +0.141  +0.122  -0.147  -0.178  -0.180  -0.169  -0.056  -0.006  +0.034  +0.043  -0.190  -0.246  -0.293  -0.298  -0.254  -0.128  -0.054  -0.029  -0.237  -0.292  -0.331  -0.342  -0.295  -0.218  -0.131  -0.096  -0.239  -0.289  -0.338  -0.371  -0.313  -0.272  -0.188  -0.106\n",
      "                         :   var13:  -0.098  -0.016  +0.072  +0.211  +0.333  +0.386  +0.384  +0.295  -0.128  -0.077  +0.037  +0.186  +0.347  +1.000  +0.414  +0.324  -0.195  -0.167  -0.070  +0.135  +0.285  +0.402  +0.377  +0.347  -0.246  -0.239  -0.212  -0.042  +0.184  +0.320  +0.338  +0.282  -0.294  -0.340  -0.337  -0.202  +0.013  +0.175  +0.224  +0.219  -0.305  -0.367  -0.372  -0.304  -0.116  +0.042  +0.142  +0.154  -0.336  -0.362  -0.384  -0.325  -0.188  -0.065  +0.026  +0.051  -0.269  -0.326  -0.347  -0.326  -0.236  -0.124  -0.028  -0.019\n",
      "                         :   var14:  -0.151  -0.082  -0.010  +0.114  +0.283  +0.372  +0.377  +0.331  -0.185  -0.167  -0.061  +0.115  +0.295  +0.414  +1.000  +0.353  -0.241  -0.249  -0.143  +0.051  +0.268  +0.404  +0.418  +0.371  -0.299  -0.307  -0.276  -0.077  +0.185  +0.344  +0.390  +0.342  -0.332  -0.369  -0.353  -0.201  +0.073  +0.236  +0.290  +0.283  -0.324  -0.392  -0.377  -0.270  -0.062  +0.107  +0.212  +0.206  -0.308  -0.378  -0.361  -0.284  -0.115  +0.005  +0.126  +0.120  -0.253  -0.304  -0.320  -0.277  -0.164  -0.044  +0.044  +0.047\n",
      "                         :   var15:  -0.145  -0.147  -0.069  +0.058  +0.187  +0.277  +0.333  +0.264  -0.183  -0.181  -0.124  +0.036  +0.195  +0.324  +0.353  +1.000  -0.246  -0.256  -0.196  -0.029  +0.191  +0.349  +0.378  +0.367  -0.274  -0.311  -0.280  -0.128  +0.149  +0.319  +0.378  +0.345  -0.301  -0.349  -0.318  -0.185  +0.060  +0.256  +0.309  +0.297  -0.303  -0.343  -0.330  -0.221  -0.037  +0.163  +0.240  +0.248  -0.260  -0.312  -0.319  -0.217  -0.056  +0.053  +0.153  +0.164  -0.218  -0.249  -0.252  -0.182  -0.103  +0.026  +0.101  +0.103\n",
      "                         :   var16:  +0.308  +0.337  +0.279  +0.167  +0.033  -0.102  -0.170  -0.181  +0.364  +0.385  +0.325  +0.166  -0.009  -0.195  -0.241  -0.246  +1.000  +0.407  +0.381  +0.197  -0.064  -0.258  -0.323  -0.294  +0.389  +0.399  +0.350  +0.170  -0.131  -0.319  -0.374  -0.326  +0.364  +0.379  +0.292  +0.105  -0.193  -0.365  -0.408  -0.362  +0.306  +0.303  +0.207  +0.012  -0.211  -0.360  -0.377  -0.339  +0.233  +0.221  +0.111  -0.027  -0.222  -0.322  -0.340  -0.316  +0.165  +0.136  +0.048  -0.055  -0.188  -0.263  -0.289  -0.224\n",
      "                         :   var17:  +0.301  +0.370  +0.335  +0.216  +0.060  -0.085  -0.164  -0.158  +0.377  +0.432  +0.399  +0.234  +0.041  -0.167  -0.249  -0.256  +0.407  +1.000  +0.416  +0.217  -0.041  -0.254  -0.335  -0.325  +0.426  +0.440  +0.384  +0.200  -0.141  -0.327  -0.412  -0.361  +0.372  +0.413  +0.304  +0.090  -0.236  -0.391  -0.430  -0.402  +0.306  +0.301  +0.201  -0.012  -0.252  -0.401  -0.417  -0.405  +0.234  +0.222  +0.130  -0.064  -0.265  -0.361  -0.406  -0.337  +0.162  +0.104  +0.045  -0.077  -0.213  -0.333  -0.331  -0.281\n",
      "                         :   var18:  +0.313  +0.370  +0.375  +0.269  +0.131  +0.007  -0.098  -0.103  +0.357  +0.418  +0.406  +0.295  +0.133  -0.070  -0.143  -0.196  +0.381  +0.416  +1.000  +0.264  +0.060  -0.152  -0.242  -0.251  +0.378  +0.368  +0.358  +0.198  -0.058  -0.255  -0.337  -0.300  +0.297  +0.310  +0.258  +0.051  -0.201  -0.345  -0.384  -0.336  +0.227  +0.217  +0.113  -0.050  -0.293  -0.411  -0.393  -0.365  +0.135  +0.117  +0.013  -0.133  -0.323  -0.380  -0.386  -0.336  +0.074  +0.021  -0.033  -0.155  -0.286  -0.364  -0.347  -0.255\n",
      "                         :   var19:  +0.190  +0.274  +0.293  +0.298  +0.258  +0.162  +0.073  +0.021  +0.213  +0.271  +0.309  +0.299  +0.257  +0.135  +0.051  -0.029  +0.197  +0.217  +0.264  +1.000  +0.170  +0.062  -0.019  -0.078  +0.153  +0.173  +0.175  +0.132  +0.038  -0.054  -0.123  -0.121  +0.093  +0.083  +0.071  -0.020  -0.124  -0.191  -0.211  -0.170  +0.027  -0.007  -0.043  -0.168  -0.252  -0.286  -0.249  -0.228  -0.032  -0.064  -0.138  -0.226  -0.289  -0.304  -0.276  -0.240  -0.073  -0.106  -0.163  -0.261  -0.313  -0.308  -0.299  -0.209\n",
      "                         :   var20:  +0.010  +0.074  +0.154  +0.228  +0.270  +0.278  +0.255  +0.175  -0.020  +0.035  +0.112  +0.209  +0.311  +0.285  +0.268  +0.191  -0.064  -0.041  +0.060  +0.170  +1.000  +0.270  +0.220  +0.198  -0.130  -0.116  -0.071  +0.012  +0.125  +0.193  +0.168  +0.136  -0.172  -0.188  -0.191  -0.114  -0.016  +0.063  +0.079  +0.090  -0.215  -0.253  -0.255  -0.222  -0.138  -0.069  +0.018  +0.032  -0.225  -0.272  -0.309  -0.282  -0.222  -0.145  -0.067  -0.025  -0.205  -0.262  -0.268  -0.284  -0.227  -0.156  -0.108  -0.072\n",
      "                         :   var21:  -0.095  -0.097  +0.006  +0.147  +0.278  +0.356  +0.344  +0.298  -0.172  -0.159  -0.064  +0.117  +0.281  +0.402  +0.404  +0.349  -0.258  -0.254  -0.152  +0.062  +0.270  +1.000  +0.418  +0.372  -0.309  -0.321  -0.277  -0.090  +0.212  +0.362  +0.383  +0.329  -0.345  -0.388  -0.359  -0.197  +0.065  +0.246  +0.293  +0.296  -0.365  -0.411  -0.379  -0.263  -0.057  +0.136  +0.212  +0.220  -0.336  -0.386  -0.394  -0.287  -0.113  +0.022  +0.115  +0.160  -0.283  -0.338  -0.324  -0.260  -0.154  -0.015  +0.061  +0.069\n",
      "                         :   var22:  -0.191  -0.178  -0.074  +0.057  +0.207  +0.311  +0.372  +0.311  -0.240  -0.238  -0.172  +0.034  +0.237  +0.377  +0.418  +0.378  -0.323  -0.335  -0.242  -0.019  +0.220  +0.418  +1.000  +0.410  -0.357  -0.386  -0.347  -0.141  +0.184  +0.396  +0.449  +0.407  -0.397  -0.439  -0.410  -0.239  +0.118  +0.298  +0.385  +0.379  -0.378  -0.427  -0.395  -0.238  +0.017  +0.228  +0.308  +0.301  -0.338  -0.377  -0.370  -0.243  -0.039  +0.122  +0.233  +0.210  -0.288  -0.316  -0.302  -0.239  -0.097  +0.042  +0.142  +0.141\n",
      "                         :   var23:  -0.184  -0.174  -0.109  +0.003  +0.173  +0.284  +0.342  +0.277  -0.252  -0.245  -0.185  -0.003  +0.171  +0.347  +0.371  +0.367  -0.294  -0.325  -0.251  -0.078  +0.198  +0.372  +0.410  +1.000  -0.352  -0.367  -0.337  -0.132  +0.170  +0.338  +0.435  +0.389  -0.371  -0.410  -0.376  -0.208  +0.095  +0.299  +0.371  +0.357  -0.330  -0.393  -0.355  -0.220  +0.030  +0.239  +0.307  +0.312  -0.322  -0.343  -0.310  -0.207  -0.018  +0.131  +0.247  +0.241  -0.249  -0.289  -0.269  -0.177  -0.050  +0.089  +0.139  +0.154\n",
      "                         :   var24:  +0.287  +0.302  +0.252  +0.117  -0.025  -0.159  -0.233  -0.203  +0.342  +0.370  +0.311  +0.129  -0.057  -0.246  -0.299  -0.274  +0.389  +0.426  +0.378  +0.153  -0.130  -0.309  -0.357  -0.352  +1.000  +0.432  +0.375  +0.155  -0.158  -0.389  -0.431  -0.351  +0.393  +0.436  +0.360  +0.106  -0.212  -0.388  -0.425  -0.392  +0.354  +0.358  +0.275  +0.068  -0.203  -0.361  -0.392  -0.377  +0.278  +0.285  +0.215  +0.024  -0.188  -0.298  -0.348  -0.312  +0.214  +0.210  +0.138  +0.001  -0.132  -0.254  -0.276  -0.253\n",
      "                         :   var25:  +0.276  +0.325  +0.262  +0.136  -0.038  -0.172  -0.253  -0.222  +0.371  +0.385  +0.340  +0.148  -0.048  -0.239  -0.307  -0.311  +0.399  +0.440  +0.368  +0.173  -0.116  -0.321  -0.386  -0.367  +0.432  +1.000  +0.401  +0.178  -0.174  -0.394  -0.460  -0.388  +0.416  +0.447  +0.371  +0.145  -0.207  -0.405  -0.456  -0.422  +0.381  +0.365  +0.281  +0.070  -0.195  -0.398  -0.432  -0.384  +0.308  +0.289  +0.214  +0.046  -0.175  -0.318  -0.385  -0.341  +0.206  +0.207  +0.152  +0.027  -0.163  -0.270  -0.313  -0.257\n",
      "                         :   var26:  +0.252  +0.294  +0.228  +0.148  -0.004  -0.143  -0.204  -0.203  +0.321  +0.346  +0.313  +0.156  -0.023  -0.212  -0.276  -0.280  +0.350  +0.384  +0.358  +0.175  -0.071  -0.277  -0.347  -0.337  +0.375  +0.401  +1.000  +0.164  -0.140  -0.336  -0.408  -0.364  +0.340  +0.388  +0.332  +0.139  -0.186  -0.376  -0.423  -0.386  +0.312  +0.333  +0.246  +0.062  -0.185  -0.363  -0.387  -0.362  +0.239  +0.245  +0.183  +0.026  -0.179  -0.312  -0.353  -0.331  +0.176  +0.168  +0.121  -0.005  -0.153  -0.239  -0.286  -0.260\n",
      "                         :   var27:  +0.113  +0.182  +0.184  +0.122  +0.074  +0.001  -0.063  -0.092  +0.158  +0.197  +0.201  +0.150  +0.087  -0.042  -0.077  -0.128  +0.170  +0.200  +0.198  +0.132  +0.012  -0.090  -0.141  -0.132  +0.155  +0.178  +0.164  +1.000  -0.025  -0.129  -0.192  -0.179  +0.138  +0.150  +0.134  +0.030  -0.123  -0.175  -0.211  -0.220  +0.113  +0.097  +0.055  -0.003  -0.152  -0.206  -0.223  -0.211  +0.064  +0.048  -0.007  -0.070  -0.150  -0.201  -0.205  -0.190  +0.027  +0.037  -0.009  -0.077  -0.138  -0.176  -0.184  -0.157\n",
      "                         :   var28:  -0.070  -0.076  +0.006  +0.071  +0.123  +0.161  +0.151  +0.124  -0.105  -0.096  -0.027  +0.041  +0.142  +0.184  +0.185  +0.149  -0.131  -0.141  -0.058  +0.038  +0.125  +0.212  +0.184  +0.170  -0.158  -0.174  -0.140  -0.025  +1.000  +0.153  +0.183  +0.139  -0.191  -0.219  -0.167  -0.092  +0.042  +0.122  +0.143  +0.138  -0.183  -0.234  -0.211  -0.136  -0.001  +0.082  +0.120  +0.090  -0.192  -0.222  -0.193  -0.147  -0.031  +0.011  +0.088  +0.075  -0.185  -0.184  -0.165  -0.154  -0.079  -0.023  +0.044  +0.061\n",
      "                         :   var29:  -0.217  -0.204  -0.129  -0.001  +0.146  +0.258  +0.293  +0.256  -0.271  -0.261  -0.195  -0.017  +0.158  +0.320  +0.344  +0.319  -0.319  -0.327  -0.255  -0.054  +0.193  +0.362  +0.396  +0.338  -0.389  -0.394  -0.336  -0.129  +0.153  +1.000  +0.426  +0.369  -0.385  -0.429  -0.386  -0.199  +0.138  +0.319  +0.369  +0.355  -0.374  -0.397  -0.353  -0.160  +0.059  +0.260  +0.333  +0.307  -0.340  -0.352  -0.333  -0.178  +0.019  +0.166  +0.254  +0.243  -0.270  -0.273  -0.275  -0.159  -0.016  +0.107  +0.175  +0.172\n",
      "                         :   var30:  -0.250  -0.269  -0.193  -0.046  +0.124  +0.257  +0.315  +0.291  -0.314  -0.337  -0.261  -0.085  +0.141  +0.338  +0.390  +0.378  -0.374  -0.412  -0.337  -0.123  +0.168  +0.383  +0.449  +0.435  -0.431  -0.460  -0.408  -0.192  +0.183  +0.426  +1.000  +0.455  -0.436  -0.485  -0.450  -0.210  +0.156  +0.385  +0.474  +0.450  -0.419  -0.465  -0.390  -0.209  +0.099  +0.316  +0.401  +0.404  -0.349  -0.393  -0.341  -0.195  +0.066  +0.232  +0.327  +0.317  -0.281  -0.293  -0.262  -0.157  +0.017  +0.164  +0.242  +0.244\n",
      "                         :   var31:  -0.221  -0.239  -0.186  -0.053  +0.090  +0.218  +0.284  +0.252  -0.280  -0.306  -0.260  -0.095  +0.122  +0.282  +0.342  +0.345  -0.326  -0.361  -0.300  -0.121  +0.136  +0.329  +0.407  +0.389  -0.351  -0.388  -0.364  -0.179  +0.139  +0.369  +0.455  +1.000  -0.381  -0.419  -0.398  -0.196  +0.126  +0.329  +0.417  +0.390  -0.347  -0.400  -0.336  -0.164  +0.083  +0.300  +0.382  +0.358  -0.314  -0.333  -0.288  -0.155  +0.051  +0.222  +0.296  +0.290  -0.231  -0.258  -0.215  -0.138  +0.034  +0.154  +0.236  +0.203\n",
      "                         :   var32:  +0.227  +0.241  +0.152  +0.025  -0.114  -0.225  -0.281  -0.224  +0.283  +0.297  +0.221  +0.043  -0.147  -0.294  -0.332  -0.301  +0.364  +0.372  +0.297  +0.093  -0.172  -0.345  -0.397  -0.371  +0.393  +0.416  +0.340  +0.138  -0.191  -0.385  -0.436  -0.381  +1.000  +0.434  +0.371  +0.158  -0.183  -0.368  -0.413  -0.372  +0.382  +0.418  +0.356  +0.123  -0.130  -0.323  -0.381  -0.349  +0.347  +0.342  +0.274  +0.118  -0.110  -0.238  -0.306  -0.291  +0.272  +0.284  +0.206  +0.102  -0.065  -0.180  -0.243  -0.233\n",
      "                         :   var33:  +0.233  +0.229  +0.158  +0.014  -0.148  -0.268  -0.289  -0.284  +0.305  +0.329  +0.241  +0.052  -0.178  -0.340  -0.369  -0.349  +0.379  +0.413  +0.310  +0.083  -0.188  -0.388  -0.439  -0.410  +0.436  +0.447  +0.388  +0.150  -0.219  -0.429  -0.485  -0.419  +0.434  +1.000  +0.410  +0.182  -0.178  -0.389  -0.457  -0.415  +0.418  +0.451  +0.373  +0.174  -0.109  -0.355  -0.426  -0.364  +0.366  +0.379  +0.331  +0.166  -0.092  -0.258  -0.341  -0.298  +0.287  +0.299  +0.260  +0.126  -0.037  -0.181  -0.259  -0.238\n",
      "                         :   var34:  +0.188  +0.188  +0.122  -0.035  -0.161  -0.261  -0.304  -0.261  +0.227  +0.248  +0.169  +0.010  -0.180  -0.337  -0.353  -0.318  +0.292  +0.304  +0.258  +0.071  -0.191  -0.359  -0.410  -0.376  +0.360  +0.371  +0.332  +0.134  -0.167  -0.386  -0.450  -0.398  +0.371  +0.410  +1.000  +0.181  -0.133  -0.339  -0.404  -0.360  +0.388  +0.409  +0.357  +0.177  -0.075  -0.290  -0.339  -0.322  +0.309  +0.338  +0.314  +0.183  -0.049  -0.179  -0.261  -0.267  +0.249  +0.289  +0.252  +0.152  -0.001  -0.123  -0.201  -0.194\n",
      "                         :   var35:  +0.026  +0.023  -0.026  -0.076  -0.142  -0.180  -0.190  -0.158  +0.064  +0.055  +0.019  -0.050  -0.169  -0.202  -0.201  -0.185  +0.105  +0.090  +0.051  -0.020  -0.114  -0.197  -0.239  -0.208  +0.106  +0.145  +0.139  +0.030  -0.092  -0.199  -0.210  -0.196  +0.158  +0.182  +0.181  +1.000  -0.014  -0.148  -0.180  -0.184  +0.181  +0.196  +0.198  +0.135  +0.033  -0.089  -0.134  -0.123  +0.167  +0.200  +0.190  +0.145  +0.055  -0.043  -0.093  -0.094  +0.135  +0.160  +0.168  +0.141  +0.065  -0.001  -0.049  -0.049\n",
      "                         :   var36:  -0.167  -0.196  -0.199  -0.134  -0.084  +0.011  +0.026  +0.046  -0.190  -0.210  -0.199  -0.140  -0.056  +0.013  +0.073  +0.060  -0.193  -0.236  -0.201  -0.124  -0.016  +0.065  +0.118  +0.095  -0.212  -0.207  -0.186  -0.123  +0.042  +0.138  +0.156  +0.126  -0.183  -0.178  -0.133  -0.014  +1.000  +0.180  +0.184  +0.158  -0.167  -0.133  -0.087  +0.025  +0.140  +0.211  +0.190  +0.174  -0.103  -0.083  -0.022  +0.056  +0.158  +0.184  +0.196  +0.163  -0.083  -0.045  +0.001  +0.081  +0.139  +0.166  +0.171  +0.144\n",
      "                         :   var37:  -0.252  -0.292  -0.271  -0.153  -0.039  +0.109  +0.145  +0.166  -0.323  -0.354  -0.324  -0.193  -0.006  +0.175  +0.236  +0.256  -0.365  -0.391  -0.345  -0.191  +0.063  +0.246  +0.298  +0.299  -0.388  -0.405  -0.376  -0.175  +0.122  +0.319  +0.385  +0.329  -0.368  -0.389  -0.339  -0.148  +0.180  +1.000  +0.392  +0.375  -0.322  -0.329  -0.246  -0.042  +0.173  +0.374  +0.388  +0.363  -0.247  -0.259  -0.176  -0.007  +0.206  +0.317  +0.352  +0.328  -0.189  -0.194  -0.136  -0.003  +0.154  +0.268  +0.303  +0.254\n",
      "                         :   var38:  -0.301  -0.316  -0.259  -0.145  +0.003  +0.155  +0.230  +0.216  -0.355  -0.384  -0.340  -0.181  +0.034  +0.224  +0.290  +0.309  -0.408  -0.430  -0.384  -0.211  +0.079  +0.293  +0.385  +0.371  -0.425  -0.456  -0.423  -0.211  +0.143  +0.369  +0.474  +0.417  -0.413  -0.457  -0.404  -0.180  +0.184  +0.392  +1.000  +0.444  -0.391  -0.404  -0.318  -0.114  +0.175  +0.393  +0.458  +0.415  -0.322  -0.332  -0.247  -0.079  +0.195  +0.346  +0.410  +0.369  -0.235  -0.230  -0.177  -0.039  +0.137  +0.269  +0.344  +0.291\n",
      "                         :   var39:  -0.233  -0.270  -0.237  -0.124  +0.014  +0.153  +0.222  +0.219  -0.297  -0.347  -0.310  -0.167  +0.043  +0.219  +0.283  +0.297  -0.362  -0.402  -0.336  -0.170  +0.090  +0.296  +0.379  +0.357  -0.392  -0.422  -0.386  -0.220  +0.138  +0.355  +0.450  +0.390  -0.372  -0.415  -0.360  -0.184  +0.158  +0.375  +0.444  +1.000  -0.357  -0.383  -0.302  -0.124  +0.175  +0.345  +0.425  +0.393  -0.285  -0.322  -0.230  -0.092  +0.150  +0.286  +0.372  +0.341  -0.214  -0.234  -0.168  -0.040  +0.114  +0.245  +0.307  +0.262\n",
      "                         :   var40:  +0.177  +0.154  +0.070  -0.030  -0.165  -0.260  -0.268  -0.244  +0.227  +0.226  +0.158  +0.003  -0.190  -0.305  -0.324  -0.303  +0.306  +0.306  +0.227  +0.027  -0.215  -0.365  -0.378  -0.330  +0.354  +0.381  +0.312  +0.113  -0.183  -0.374  -0.419  -0.347  +0.382  +0.418  +0.388  +0.181  -0.167  -0.322  -0.391  -0.357  +1.000  +0.427  +0.371  +0.190  -0.082  -0.265  -0.345  -0.330  +0.363  +0.381  +0.327  +0.185  -0.052  -0.189  -0.277  -0.269  +0.278  +0.300  +0.279  +0.157  -0.010  -0.116  -0.220  -0.197\n",
      "                         :   var41:  +0.155  +0.141  +0.037  -0.103  -0.217  -0.309  -0.331  -0.269  +0.215  +0.221  +0.128  -0.050  -0.246  -0.367  -0.392  -0.343  +0.303  +0.301  +0.217  -0.007  -0.253  -0.411  -0.427  -0.393  +0.358  +0.365  +0.333  +0.097  -0.234  -0.397  -0.465  -0.400  +0.418  +0.451  +0.409  +0.196  -0.133  -0.329  -0.404  -0.383  +0.427  +1.000  +0.418  +0.243  -0.034  -0.263  -0.348  -0.337  +0.397  +0.439  +0.398  +0.263  +0.001  -0.165  -0.276  -0.264  +0.314  +0.354  +0.331  +0.233  +0.045  -0.102  -0.198  -0.192\n",
      "                         :   var42:  +0.078  +0.050  -0.033  -0.160  -0.256  -0.331  -0.342  -0.278  +0.139  +0.113  +0.016  -0.124  -0.293  -0.372  -0.377  -0.330  +0.207  +0.201  +0.113  -0.043  -0.255  -0.379  -0.395  -0.355  +0.275  +0.281  +0.246  +0.055  -0.211  -0.353  -0.390  -0.336  +0.356  +0.373  +0.357  +0.198  -0.087  -0.246  -0.318  -0.302  +0.371  +0.418  +1.000  +0.260  +0.044  -0.135  -0.258  -0.252  +0.355  +0.414  +0.399  +0.290  +0.092  -0.048  -0.175  -0.197  +0.315  +0.363  +0.348  +0.269  +0.133  -0.006  -0.087  -0.112\n",
      "                         :   var43:  -0.048  -0.135  -0.190  -0.238  -0.283  -0.273  -0.263  -0.208  -0.058  -0.079  -0.141  -0.222  -0.298  -0.304  -0.270  -0.221  +0.012  -0.012  -0.050  -0.168  -0.222  -0.263  -0.238  -0.220  +0.068  +0.070  +0.062  -0.003  -0.136  -0.160  -0.209  -0.164  +0.123  +0.174  +0.177  +0.135  +0.025  -0.042  -0.114  -0.124  +0.190  +0.243  +0.260  +1.000  +0.157  +0.042  -0.054  -0.072  +0.195  +0.269  +0.308  +0.292  +0.212  +0.135  +0.023  -0.018  +0.196  +0.251  +0.279  +0.316  +0.224  +0.152  +0.074  +0.029\n",
      "                         :   var44:  -0.203  -0.282  -0.284  -0.293  -0.247  -0.177  -0.102  -0.062  -0.217  -0.304  -0.327  -0.308  -0.254  -0.116  -0.062  -0.037  -0.211  -0.252  -0.293  -0.252  -0.138  -0.057  +0.017  +0.030  -0.203  -0.195  -0.185  -0.152  -0.001  +0.059  +0.099  +0.083  -0.130  -0.109  -0.075  +0.033  +0.140  +0.173  +0.175  +0.175  -0.082  -0.034  +0.044  +0.157  +1.000  +0.274  +0.232  +0.203  -0.028  +0.032  +0.110  +0.221  +0.316  +0.316  +0.260  +0.227  +0.037  +0.088  +0.135  +0.236  +0.300  +0.314  +0.267  +0.199\n",
      "                         :   var45:  -0.273  -0.348  -0.335  -0.259  -0.149  -0.027  +0.056  +0.081  -0.345  -0.397  -0.397  -0.282  -0.128  +0.042  +0.107  +0.163  -0.360  -0.401  -0.411  -0.286  -0.069  +0.136  +0.228  +0.239  -0.361  -0.398  -0.363  -0.206  +0.082  +0.260  +0.316  +0.300  -0.323  -0.355  -0.290  -0.089  +0.211  +0.374  +0.393  +0.345  -0.265  -0.263  -0.135  +0.042  +0.274  +1.000  +0.426  +0.373  -0.194  -0.174  -0.075  +0.093  +0.309  +0.397  +0.414  +0.369  -0.127  -0.088  -0.009  +0.126  +0.274  +0.362  +0.374  +0.305\n",
      "                         :   var46:  -0.293  -0.323  -0.297  -0.203  -0.093  +0.062  +0.152  +0.158  -0.351  -0.396  -0.376  -0.235  -0.054  +0.142  +0.212  +0.240  -0.377  -0.417  -0.393  -0.249  +0.018  +0.212  +0.308  +0.307  -0.392  -0.432  -0.387  -0.223  +0.120  +0.333  +0.401  +0.382  -0.381  -0.426  -0.339  -0.134  +0.190  +0.388  +0.458  +0.425  -0.345  -0.348  -0.258  -0.054  +0.232  +0.426  +1.000  +0.415  -0.264  -0.265  -0.160  +0.009  +0.250  +0.371  +0.423  +0.384  -0.179  -0.164  -0.100  +0.039  +0.218  +0.326  +0.366  +0.320\n",
      "                         :   var47:  -0.241  -0.294  -0.267  -0.187  -0.061  +0.079  +0.131  +0.153  -0.296  -0.354  -0.340  -0.223  -0.029  +0.154  +0.206  +0.248  -0.339  -0.405  -0.365  -0.228  +0.032  +0.220  +0.301  +0.312  -0.377  -0.384  -0.362  -0.211  +0.090  +0.307  +0.404  +0.358  -0.349  -0.364  -0.322  -0.123  +0.174  +0.363  +0.415  +0.393  -0.330  -0.337  -0.252  -0.072  +0.203  +0.373  +0.415  +1.000  -0.249  -0.252  -0.175  -0.011  +0.209  +0.316  +0.395  +0.351  -0.187  -0.171  -0.112  +0.008  +0.167  +0.292  +0.338  +0.286\n",
      "                         :   var48:  +0.110  +0.065  -0.002  -0.108  -0.222  -0.247  -0.274  -0.211  +0.175  +0.140  +0.069  -0.077  -0.237  -0.336  -0.308  -0.260  +0.233  +0.234  +0.135  -0.032  -0.225  -0.336  -0.338  -0.322  +0.278  +0.308  +0.239  +0.064  -0.192  -0.340  -0.349  -0.314  +0.347  +0.366  +0.309  +0.167  -0.103  -0.247  -0.322  -0.285  +0.363  +0.397  +0.355  +0.195  -0.028  -0.194  -0.264  -0.249  +1.000  +0.377  +0.353  +0.216  +0.023  -0.114  -0.191  -0.207  +0.291  +0.326  +0.299  +0.222  +0.074  -0.043  -0.122  -0.127\n",
      "                         :   var49:  +0.075  +0.037  -0.043  -0.170  -0.253  -0.316  -0.318  -0.265  +0.132  +0.092  +0.025  -0.140  -0.292  -0.362  -0.378  -0.312  +0.221  +0.222  +0.117  -0.064  -0.272  -0.386  -0.377  -0.343  +0.285  +0.289  +0.245  +0.048  -0.222  -0.352  -0.393  -0.333  +0.342  +0.379  +0.338  +0.200  -0.083  -0.259  -0.332  -0.322  +0.381  +0.439  +0.414  +0.269  +0.032  -0.174  -0.265  -0.252  +0.377  +1.000  +0.419  +0.276  +0.083  -0.064  -0.178  -0.189  +0.333  +0.376  +0.373  +0.285  +0.125  -0.006  -0.106  -0.128\n",
      "                         :   var50:  +0.018  -0.056  -0.136  -0.253  -0.329  -0.356  -0.331  -0.250  +0.044  +0.014  -0.065  -0.214  -0.331  -0.384  -0.361  -0.319  +0.111  +0.130  +0.013  -0.138  -0.309  -0.394  -0.370  -0.310  +0.215  +0.214  +0.183  -0.007  -0.193  -0.333  -0.341  -0.288  +0.274  +0.331  +0.314  +0.190  -0.022  -0.176  -0.247  -0.230  +0.327  +0.398  +0.399  +0.308  +0.110  -0.075  -0.160  -0.175  +0.353  +0.419  +1.000  +0.355  +0.190  +0.040  -0.072  -0.128  +0.306  +0.352  +0.411  +0.361  +0.233  +0.071  -0.012  -0.066\n",
      "                         :   var51:  -0.103  -0.181  -0.232  -0.329  -0.365  -0.323  -0.275  -0.207  -0.099  -0.149  -0.212  -0.312  -0.342  -0.325  -0.284  -0.217  -0.027  -0.064  -0.133  -0.226  -0.282  -0.287  -0.243  -0.207  +0.024  +0.046  +0.026  -0.070  -0.147  -0.178  -0.195  -0.155  +0.118  +0.166  +0.183  +0.145  +0.056  -0.007  -0.079  -0.092  +0.185  +0.263  +0.290  +0.292  +0.221  +0.093  +0.009  -0.011  +0.216  +0.276  +0.355  +1.000  +0.293  +0.193  +0.099  +0.063  +0.229  +0.302  +0.340  +0.383  +0.329  +0.222  +0.140  +0.070\n",
      "                         :   var52:  -0.219  -0.303  -0.344  -0.361  -0.315  -0.248  -0.140  -0.094  -0.258  -0.332  -0.344  -0.348  -0.295  -0.188  -0.115  -0.056  -0.222  -0.265  -0.323  -0.289  -0.222  -0.113  -0.039  -0.018  -0.188  -0.175  -0.179  -0.150  -0.031  +0.019  +0.066  +0.051  -0.110  -0.092  -0.049  +0.055  +0.158  +0.206  +0.195  +0.150  -0.052  +0.001  +0.092  +0.212  +0.316  +0.309  +0.250  +0.209  +0.023  +0.083  +0.190  +0.293  +1.000  +0.352  +0.288  +0.229  +0.075  +0.127  +0.217  +0.317  +0.362  +0.351  +0.320  +0.223\n",
      "                         :   var53:  -0.261  -0.325  -0.364  -0.323  -0.247  -0.122  -0.033  +0.003  -0.296  -0.379  -0.396  -0.340  -0.218  -0.065  +0.005  +0.053  -0.322  -0.361  -0.380  -0.304  -0.145  +0.022  +0.122  +0.131  -0.298  -0.318  -0.312  -0.201  +0.011  +0.166  +0.232  +0.222  -0.238  -0.258  -0.179  -0.043  +0.184  +0.317  +0.346  +0.286  -0.189  -0.165  -0.048  +0.135  +0.316  +0.397  +0.371  +0.316  -0.114  -0.064  +0.040  +0.193  +0.352  +1.000  +0.407  +0.333  -0.035  +0.010  +0.076  +0.223  +0.337  +0.388  +0.377  +0.276\n",
      "                         :   var54:  -0.255  -0.327  -0.308  -0.263  -0.155  -0.032  +0.065  +0.067  -0.320  -0.385  -0.388  -0.290  -0.131  +0.026  +0.126  +0.153  -0.340  -0.406  -0.386  -0.276  -0.067  +0.115  +0.233  +0.247  -0.348  -0.385  -0.353  -0.205  +0.088  +0.254  +0.327  +0.296  -0.306  -0.341  -0.261  -0.093  +0.196  +0.352  +0.410  +0.372  -0.277  -0.276  -0.175  +0.023  +0.260  +0.414  +0.423  +0.395  -0.191  -0.178  -0.072  +0.099  +0.288  +0.407  +1.000  +0.375  -0.117  -0.092  -0.024  +0.112  +0.275  +0.353  +0.377  +0.320\n",
      "                         :   var55:  -0.226  -0.266  -0.262  -0.224  -0.111  -0.011  +0.082  +0.095  -0.257  -0.313  -0.317  -0.230  -0.096  +0.051  +0.120  +0.164  -0.316  -0.337  -0.336  -0.240  -0.025  +0.160  +0.210  +0.241  -0.312  -0.341  -0.331  -0.190  +0.075  +0.243  +0.317  +0.290  -0.291  -0.298  -0.267  -0.094  +0.163  +0.328  +0.369  +0.341  -0.269  -0.264  -0.197  -0.018  +0.227  +0.369  +0.384  +0.351  -0.207  -0.189  -0.128  +0.063  +0.229  +0.333  +0.375  +1.000  -0.121  -0.115  -0.057  +0.070  +0.217  +0.309  +0.321  +0.278\n",
      "                         :   var56:  +0.059  +0.036  -0.054  -0.129  -0.194  -0.213  -0.211  -0.194  +0.099  +0.074  +0.002  -0.105  -0.239  -0.269  -0.253  -0.218  +0.165  +0.162  +0.074  -0.073  -0.205  -0.283  -0.288  -0.249  +0.214  +0.206  +0.176  +0.027  -0.185  -0.270  -0.281  -0.231  +0.272  +0.287  +0.249  +0.135  -0.083  -0.189  -0.235  -0.214  +0.278  +0.314  +0.315  +0.196  +0.037  -0.127  -0.179  -0.187  +0.291  +0.333  +0.306  +0.229  +0.075  -0.035  -0.117  -0.121  +1.000  +0.299  +0.272  +0.215  +0.107  -0.015  -0.080  -0.087\n",
      "                         :   var57:  +0.014  +0.000  -0.080  -0.213  -0.263  -0.295  -0.268  -0.202  +0.062  +0.041  -0.055  -0.172  -0.289  -0.326  -0.304  -0.249  +0.136  +0.104  +0.021  -0.106  -0.262  -0.338  -0.316  -0.289  +0.210  +0.207  +0.168  +0.037  -0.184  -0.273  -0.293  -0.258  +0.284  +0.299  +0.289  +0.160  -0.045  -0.194  -0.230  -0.234  +0.300  +0.354  +0.363  +0.251  +0.088  -0.088  -0.164  -0.171  +0.326  +0.376  +0.352  +0.302  +0.127  +0.010  -0.092  -0.115  +0.299  +1.000  +0.340  +0.287  +0.174  +0.046  -0.039  -0.075\n",
      "                         :   var58:  -0.029  -0.084  -0.163  -0.274  -0.321  -0.321  -0.308  -0.214  -0.001  -0.051  -0.125  -0.229  -0.338  -0.347  -0.320  -0.252  +0.048  +0.045  -0.033  -0.163  -0.268  -0.324  -0.302  -0.269  +0.138  +0.152  +0.121  -0.009  -0.165  -0.275  -0.262  -0.215  +0.206  +0.260  +0.252  +0.168  +0.001  -0.136  -0.177  -0.168  +0.279  +0.331  +0.348  +0.279  +0.135  -0.009  -0.100  -0.112  +0.299  +0.373  +0.411  +0.340  +0.217  +0.076  -0.024  -0.057  +0.272  +0.340  +1.000  +0.342  +0.244  +0.135  +0.042  -0.012\n",
      "                         :   var59:  -0.113  -0.205  -0.270  -0.333  -0.346  -0.317  -0.274  -0.198  -0.088  -0.183  -0.253  -0.315  -0.371  -0.326  -0.277  -0.182  -0.055  -0.077  -0.155  -0.261  -0.284  -0.260  -0.239  -0.177  +0.001  +0.027  -0.005  -0.077  -0.154  -0.159  -0.157  -0.138  +0.102  +0.126  +0.152  +0.141  +0.081  -0.003  -0.039  -0.040  +0.157  +0.233  +0.269  +0.316  +0.236  +0.126  +0.039  +0.008  +0.222  +0.285  +0.361  +0.383  +0.317  +0.223  +0.112  +0.070  +0.215  +0.287  +0.342  +1.000  +0.347  +0.265  +0.167  +0.083\n",
      "                         :   var60:  -0.197  -0.269  -0.307  -0.342  -0.331  -0.261  -0.174  -0.127  -0.221  -0.290  -0.326  -0.349  -0.313  -0.236  -0.164  -0.103  -0.188  -0.213  -0.286  -0.313  -0.227  -0.154  -0.097  -0.050  -0.132  -0.163  -0.153  -0.138  -0.079  -0.016  +0.017  +0.034  -0.065  -0.037  -0.001  +0.065  +0.139  +0.154  +0.137  +0.114  -0.010  +0.045  +0.133  +0.224  +0.300  +0.274  +0.218  +0.167  +0.074  +0.125  +0.233  +0.329  +0.362  +0.337  +0.275  +0.217  +0.107  +0.174  +0.244  +0.347  +1.000  +0.354  +0.288  +0.187\n",
      "                         :   var61:  -0.236  -0.302  -0.353  -0.308  -0.275  -0.162  -0.090  -0.034  -0.277  -0.345  -0.365  -0.343  -0.272  -0.124  -0.044  +0.026  -0.263  -0.333  -0.364  -0.308  -0.156  -0.015  +0.042  +0.089  -0.254  -0.270  -0.239  -0.176  -0.023  +0.107  +0.164  +0.154  -0.180  -0.181  -0.123  -0.001  +0.166  +0.268  +0.269  +0.245  -0.116  -0.102  -0.006  +0.152  +0.314  +0.362  +0.326  +0.292  -0.043  -0.006  +0.071  +0.222  +0.351  +0.388  +0.353  +0.309  -0.015  +0.046  +0.135  +0.265  +0.354  +1.000  +0.349  +0.282\n",
      "                         :   var62:  -0.220  -0.290  -0.303  -0.282  -0.179  -0.101  -0.001  +0.033  -0.278  -0.354  -0.345  -0.293  -0.188  -0.028  +0.044  +0.101  -0.289  -0.331  -0.347  -0.299  -0.108  +0.061  +0.142  +0.139  -0.276  -0.313  -0.286  -0.184  +0.044  +0.175  +0.242  +0.236  -0.243  -0.259  -0.201  -0.049  +0.171  +0.303  +0.344  +0.307  -0.220  -0.198  -0.087  +0.074  +0.267  +0.374  +0.366  +0.338  -0.122  -0.106  -0.012  +0.140  +0.320  +0.377  +0.377  +0.321  -0.080  -0.039  +0.042  +0.167  +0.288  +0.349  +1.000  +0.287\n",
      "                         :   var63:  -0.184  -0.233  -0.206  -0.186  -0.113  -0.042  +0.009  +0.036  -0.185  -0.264  -0.288  -0.217  -0.106  -0.019  +0.047  +0.103  -0.224  -0.281  -0.255  -0.209  -0.072  +0.069  +0.141  +0.154  -0.253  -0.257  -0.260  -0.157  +0.061  +0.172  +0.244  +0.203  -0.233  -0.238  -0.194  -0.049  +0.144  +0.254  +0.291  +0.262  -0.197  -0.192  -0.112  +0.029  +0.199  +0.305  +0.320  +0.286  -0.127  -0.128  -0.066  +0.070  +0.223  +0.276  +0.320  +0.278  -0.087  -0.075  -0.012  +0.083  +0.187  +0.282  +0.287  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DataSetInfo              : Correlation matrix (Background):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.279  +0.265  +0.252  +0.153  +0.067  +0.016  -0.014  +0.301  +0.315  +0.310  +0.259  +0.164  +0.044  -0.000  -0.080  +0.281  +0.302  +0.311  +0.223  +0.099  -0.038  -0.120  -0.112  +0.254  +0.256  +0.243  +0.126  -0.008  -0.129  -0.168  -0.184  +0.158  +0.138  +0.081  -0.024  -0.160  -0.222  -0.260  -0.223  +0.093  +0.040  -0.028  -0.120  -0.226  -0.277  -0.276  -0.231  +0.011  -0.016  -0.072  -0.181  -0.243  -0.265  -0.263  -0.220  -0.015  -0.087  -0.123  -0.168  -0.228  -0.231  -0.225  -0.170\n",
      "                         :    var1:  +0.279  +1.000  +0.343  +0.304  +0.232  +0.126  +0.052  +0.009  +0.336  +0.370  +0.367  +0.327  +0.229  +0.093  +0.008  -0.037  +0.303  +0.340  +0.325  +0.288  +0.158  +0.017  -0.080  -0.106  +0.258  +0.269  +0.238  +0.154  +0.015  -0.116  -0.166  -0.186  +0.154  +0.123  +0.066  -0.045  -0.189  -0.248  -0.273  -0.224  +0.070  +0.013  -0.066  -0.185  -0.281  -0.309  -0.340  -0.284  -0.013  -0.067  -0.145  -0.236  -0.307  -0.329  -0.322  -0.268  -0.053  -0.123  -0.166  -0.231  -0.295  -0.284  -0.262  -0.217\n",
      "                         :    var2:  +0.265  +0.343  +1.000  +0.390  +0.314  +0.252  +0.147  +0.069  +0.313  +0.387  +0.416  +0.401  +0.339  +0.211  +0.111  +0.042  +0.292  +0.330  +0.352  +0.323  +0.245  +0.115  +0.035  -0.028  +0.218  +0.213  +0.213  +0.136  +0.069  -0.053  -0.105  -0.112  +0.097  +0.075  +0.010  -0.082  -0.197  -0.258  -0.219  -0.189  -0.009  -0.067  -0.157  -0.258  -0.341  -0.343  -0.311  -0.269  -0.078  -0.164  -0.228  -0.327  -0.382  -0.360  -0.340  -0.267  -0.123  -0.200  -0.252  -0.330  -0.363  -0.333  -0.313  -0.257\n",
      "                         :    var3:  +0.252  +0.304  +0.390  +1.000  +0.360  +0.310  +0.225  +0.186  +0.229  +0.326  +0.383  +0.395  +0.382  +0.279  +0.216  +0.131  +0.198  +0.254  +0.313  +0.331  +0.300  +0.209  +0.120  +0.066  +0.123  +0.118  +0.144  +0.136  +0.106  +0.031  +0.001  -0.036  -0.016  -0.034  -0.075  -0.109  -0.146  -0.167  -0.148  -0.114  -0.095  -0.145  -0.243  -0.285  -0.341  -0.307  -0.268  -0.209  -0.157  -0.221  -0.309  -0.377  -0.388  -0.360  -0.310  -0.221  -0.179  -0.255  -0.317  -0.359  -0.378  -0.340  -0.298  -0.210\n",
      "                         :    var4:  +0.153  +0.232  +0.314  +0.360  +1.000  +0.360  +0.311  +0.235  +0.157  +0.216  +0.306  +0.362  +0.399  +0.363  +0.309  +0.250  +0.069  +0.128  +0.216  +0.287  +0.344  +0.302  +0.254  +0.184  -0.012  +0.016  +0.041  +0.099  +0.135  +0.141  +0.134  +0.096  -0.128  -0.156  -0.147  -0.153  -0.133  -0.097  -0.032  -0.007  -0.185  -0.233  -0.317  -0.320  -0.313  -0.223  -0.168  -0.111  -0.218  -0.289  -0.337  -0.386  -0.355  -0.306  -0.238  -0.165  -0.207  -0.281  -0.347  -0.357  -0.361  -0.318  -0.241  -0.177\n",
      "                         :    var5:  +0.067  +0.126  +0.252  +0.310  +0.360  +1.000  +0.344  +0.279  +0.042  +0.133  +0.203  +0.299  +0.385  +0.374  +0.356  +0.298  -0.024  +0.047  +0.120  +0.223  +0.323  +0.344  +0.300  +0.261  -0.098  -0.102  -0.047  +0.061  +0.171  +0.210  +0.207  +0.192  -0.202  -0.208  -0.222  -0.171  -0.090  -0.001  +0.055  +0.093  -0.249  -0.296  -0.328  -0.314  -0.253  -0.159  -0.090  -0.013  -0.263  -0.323  -0.360  -0.373  -0.299  -0.222  -0.158  -0.077  -0.226  -0.284  -0.349  -0.314  -0.320  -0.253  -0.173  -0.105\n",
      "                         :    var6:  +0.016  +0.052  +0.147  +0.225  +0.311  +0.344  +1.000  +0.292  -0.024  +0.031  +0.121  +0.221  +0.338  +0.375  +0.370  +0.338  -0.085  -0.051  +0.018  +0.173  +0.298  +0.344  +0.347  +0.309  -0.177  -0.165  -0.107  +0.001  +0.178  +0.236  +0.263  +0.244  -0.240  -0.260  -0.246  -0.166  -0.056  +0.063  +0.124  +0.152  -0.272  -0.313  -0.316  -0.295  -0.191  -0.081  +0.012  +0.045  -0.248  -0.308  -0.331  -0.306  -0.230  -0.147  -0.086  -0.012  -0.217  -0.270  -0.315  -0.292  -0.254  -0.190  -0.101  -0.061\n",
      "                         :    var7:  -0.014  +0.009  +0.069  +0.186  +0.235  +0.279  +0.292  +1.000  -0.060  -0.014  +0.049  +0.156  +0.264  +0.285  +0.313  +0.290  -0.098  -0.067  -0.004  +0.082  +0.224  +0.290  +0.315  +0.271  -0.165  -0.177  -0.107  +0.003  +0.131  +0.226  +0.254  +0.243  -0.225  -0.259  -0.233  -0.141  -0.046  +0.094  +0.142  +0.178  -0.240  -0.267  -0.277  -0.239  -0.128  -0.030  +0.076  +0.102  -0.228  -0.259  -0.282  -0.254  -0.169  -0.101  -0.038  +0.013  -0.192  -0.219  -0.257  -0.215  -0.182  -0.105  -0.062  -0.002\n",
      "                         :    var8:  +0.301  +0.336  +0.313  +0.229  +0.157  +0.042  -0.024  -0.060  +1.000  +0.387  +0.356  +0.271  +0.121  +0.019  -0.079  -0.130  +0.344  +0.363  +0.344  +0.230  +0.088  -0.073  -0.159  -0.181  +0.328  +0.336  +0.283  +0.162  -0.046  -0.180  -0.238  -0.247  +0.256  +0.207  +0.150  +0.001  -0.176  -0.294  -0.308  -0.285  +0.173  +0.128  +0.028  -0.104  -0.259  -0.330  -0.346  -0.309  +0.075  +0.006  -0.057  -0.177  -0.254  -0.320  -0.317  -0.259  +0.005  -0.036  -0.086  -0.187  -0.250  -0.271  -0.280  -0.227\n",
      "                         :    var9:  +0.315  +0.370  +0.387  +0.326  +0.216  +0.133  +0.031  -0.014  +0.387  +1.000  +0.433  +0.351  +0.217  +0.074  -0.014  -0.075  +0.365  +0.400  +0.399  +0.321  +0.142  -0.009  -0.117  -0.157  +0.331  +0.341  +0.303  +0.180  +0.006  -0.161  -0.218  -0.236  +0.207  +0.205  +0.118  -0.001  -0.236  -0.321  -0.333  -0.299  +0.114  +0.055  -0.024  -0.165  -0.315  -0.391  -0.385  -0.336  +0.033  -0.045  -0.122  -0.248  -0.336  -0.381  -0.399  -0.308  -0.032  -0.087  -0.159  -0.249  -0.319  -0.342  -0.334  -0.254\n",
      "                         :   var10:  +0.310  +0.367  +0.416  +0.383  +0.306  +0.203  +0.121  +0.049  +0.356  +0.433  +1.000  +0.417  +0.307  +0.177  +0.067  -0.010  +0.321  +0.378  +0.397  +0.367  +0.231  +0.075  -0.019  -0.072  +0.269  +0.295  +0.263  +0.178  +0.059  -0.088  -0.163  -0.167  +0.143  +0.115  +0.058  -0.084  -0.193  -0.286  -0.294  -0.263  +0.046  -0.012  -0.130  -0.233  -0.345  -0.392  -0.382  -0.317  -0.037  -0.124  -0.206  -0.324  -0.395  -0.398  -0.383  -0.308  -0.092  -0.183  -0.243  -0.315  -0.378  -0.379  -0.345  -0.261\n",
      "                         :   var11:  +0.259  +0.327  +0.401  +0.395  +0.362  +0.299  +0.221  +0.156  +0.271  +0.351  +0.417  +1.000  +0.374  +0.287  +0.191  +0.123  +0.214  +0.281  +0.334  +0.329  +0.293  +0.195  +0.122  +0.053  +0.152  +0.159  +0.179  +0.163  +0.098  +0.034  -0.002  -0.033  +0.005  -0.005  -0.057  -0.115  -0.162  -0.191  -0.177  -0.138  -0.087  -0.143  -0.230  -0.295  -0.362  -0.318  -0.295  -0.227  -0.137  -0.235  -0.292  -0.375  -0.403  -0.378  -0.335  -0.241  -0.171  -0.252  -0.319  -0.366  -0.387  -0.370  -0.323  -0.229\n",
      "                         :   var12:  +0.164  +0.229  +0.339  +0.382  +0.399  +0.385  +0.338  +0.264  +0.121  +0.217  +0.307  +0.374  +1.000  +0.378  +0.342  +0.262  +0.072  +0.126  +0.215  +0.294  +0.357  +0.339  +0.268  +0.204  -0.011  -0.007  +0.037  +0.091  +0.174  +0.171  +0.136  +0.112  -0.152  -0.170  -0.187  -0.168  -0.120  -0.076  -0.022  +0.034  -0.214  -0.273  -0.336  -0.336  -0.316  -0.219  -0.154  -0.116  -0.264  -0.329  -0.372  -0.414  -0.391  -0.315  -0.233  -0.163  -0.254  -0.323  -0.379  -0.378  -0.393  -0.323  -0.239  -0.167\n",
      "                         :   var13:  +0.044  +0.093  +0.211  +0.279  +0.363  +0.374  +0.375  +0.285  +0.019  +0.074  +0.177  +0.287  +0.378  +1.000  +0.411  +0.351  -0.073  -0.010  +0.078  +0.205  +0.356  +0.387  +0.395  +0.312  -0.154  -0.137  -0.092  +0.043  +0.183  +0.275  +0.279  +0.255  -0.261  -0.293  -0.285  -0.181  -0.066  +0.051  +0.127  +0.163  -0.319  -0.354  -0.378  -0.349  -0.254  -0.105  -0.011  +0.031  -0.304  -0.385  -0.404  -0.392  -0.299  -0.194  -0.126  -0.037  -0.290  -0.363  -0.367  -0.366  -0.319  -0.235  -0.139  -0.080\n",
      "                         :   var14:  -0.000  +0.008  +0.111  +0.216  +0.309  +0.356  +0.370  +0.313  -0.079  -0.014  +0.067  +0.191  +0.342  +0.411  +1.000  +0.367  -0.136  -0.107  -0.026  +0.131  +0.307  +0.382  +0.408  +0.373  -0.220  -0.214  -0.181  -0.011  +0.190  +0.289  +0.349  +0.308  -0.325  -0.331  -0.308  -0.200  -0.010  +0.134  +0.211  +0.222  -0.319  -0.366  -0.363  -0.301  -0.172  -0.017  +0.089  +0.130  -0.312  -0.354  -0.393  -0.332  -0.220  -0.109  -0.024  +0.039  -0.255  -0.306  -0.334  -0.298  -0.235  -0.151  -0.065  -0.009\n",
      "                         :   var15:  -0.080  -0.037  +0.042  +0.131  +0.250  +0.298  +0.338  +0.290  -0.130  -0.075  -0.010  +0.123  +0.262  +0.351  +0.367  +1.000  -0.179  -0.173  -0.090  +0.054  +0.225  +0.356  +0.394  +0.336  -0.249  -0.261  -0.188  -0.041  +0.160  +0.294  +0.324  +0.334  -0.296  -0.331  -0.291  -0.175  -0.003  +0.164  +0.218  +0.259  -0.304  -0.340  -0.329  -0.261  -0.105  +0.056  +0.139  +0.178  -0.258  -0.320  -0.320  -0.253  -0.161  -0.042  +0.032  +0.076  -0.192  -0.254  -0.277  -0.221  -0.166  -0.075  -0.014  +0.018\n",
      "                         :   var16:  +0.281  +0.303  +0.292  +0.198  +0.069  -0.024  -0.085  -0.098  +0.344  +0.365  +0.321  +0.214  +0.072  -0.073  -0.136  -0.179  +1.000  +0.398  +0.344  +0.212  +0.008  -0.139  -0.234  -0.253  +0.384  +0.370  +0.308  +0.133  -0.092  -0.244  -0.294  -0.307  +0.315  +0.319  +0.203  +0.038  -0.173  -0.326  -0.371  -0.338  +0.244  +0.211  +0.125  -0.053  -0.228  -0.345  -0.372  -0.339  +0.153  +0.095  +0.043  -0.115  -0.229  -0.310  -0.339  -0.288  +0.062  +0.041  -0.026  -0.114  -0.203  -0.253  -0.279  -0.235\n",
      "                         :   var17:  +0.302  +0.340  +0.330  +0.254  +0.128  +0.047  -0.051  -0.067  +0.363  +0.400  +0.378  +0.281  +0.126  -0.010  -0.107  -0.173  +0.398  +1.000  +0.386  +0.265  +0.077  -0.126  -0.233  -0.243  +0.374  +0.384  +0.336  +0.155  -0.048  -0.249  -0.290  -0.297  +0.288  +0.295  +0.197  +0.046  -0.188  -0.341  -0.375  -0.359  +0.201  +0.179  +0.067  -0.093  -0.266  -0.394  -0.398  -0.376  +0.121  +0.067  -0.015  -0.152  -0.280  -0.357  -0.370  -0.331  +0.052  -0.013  -0.071  -0.164  -0.275  -0.299  -0.334  -0.258\n",
      "                         :   var18:  +0.311  +0.325  +0.352  +0.313  +0.216  +0.120  +0.018  -0.004  +0.344  +0.399  +0.397  +0.334  +0.215  +0.078  -0.026  -0.090  +0.344  +0.386  +1.000  +0.314  +0.138  -0.033  -0.127  -0.151  +0.326  +0.334  +0.321  +0.161  -0.001  -0.160  -0.240  -0.242  +0.203  +0.205  +0.139  -0.006  -0.192  -0.306  -0.342  -0.312  +0.115  +0.092  -0.025  -0.147  -0.317  -0.379  -0.390  -0.337  +0.028  -0.021  -0.111  -0.229  -0.320  -0.370  -0.369  -0.311  -0.040  -0.085  -0.138  -0.246  -0.317  -0.344  -0.343  -0.277\n",
      "                         :   var19:  +0.223  +0.288  +0.323  +0.331  +0.287  +0.223  +0.173  +0.082  +0.230  +0.321  +0.367  +0.329  +0.294  +0.205  +0.131  +0.054  +0.212  +0.265  +0.314  +1.000  +0.248  +0.140  +0.046  +0.004  +0.134  +0.179  +0.176  +0.114  +0.087  -0.016  -0.048  -0.088  +0.038  +0.039  -0.011  -0.063  -0.149  -0.208  -0.185  -0.164  -0.028  -0.081  -0.171  -0.215  -0.292  -0.319  -0.282  -0.222  -0.096  -0.170  -0.220  -0.290  -0.348  -0.350  -0.318  -0.222  -0.118  -0.198  -0.258  -0.312  -0.348  -0.333  -0.298  -0.239\n",
      "                         :   var20:  +0.099  +0.158  +0.245  +0.300  +0.344  +0.323  +0.298  +0.224  +0.088  +0.142  +0.231  +0.293  +0.357  +0.356  +0.307  +0.225  +0.008  +0.077  +0.138  +0.248  +1.000  +0.303  +0.268  +0.215  -0.061  -0.057  +0.022  +0.090  +0.154  +0.172  +0.158  +0.150  -0.174  -0.176  -0.196  -0.145  -0.066  -0.014  +0.008  +0.051  -0.223  -0.265  -0.312  -0.292  -0.260  -0.162  -0.113  -0.048  -0.251  -0.317  -0.354  -0.351  -0.321  -0.237  -0.178  -0.094  -0.220  -0.298  -0.323  -0.335  -0.329  -0.266  -0.193  -0.143\n",
      "                         :   var21:  -0.038  +0.017  +0.115  +0.209  +0.302  +0.344  +0.344  +0.290  -0.073  -0.009  +0.075  +0.195  +0.339  +0.387  +0.382  +0.356  -0.139  -0.126  -0.033  +0.140  +0.303  +1.000  +0.399  +0.360  -0.238  -0.228  -0.168  -0.006  +0.154  +0.301  +0.321  +0.292  -0.305  -0.328  -0.316  -0.196  -0.020  +0.127  +0.192  +0.224  -0.321  -0.356  -0.367  -0.306  -0.156  +0.005  +0.075  +0.124  -0.317  -0.355  -0.369  -0.311  -0.224  -0.096  -0.028  +0.029  -0.268  -0.310  -0.340  -0.297  -0.236  -0.146  -0.057  -0.022\n",
      "                         :   var22:  -0.120  -0.080  +0.035  +0.120  +0.254  +0.300  +0.347  +0.315  -0.159  -0.117  -0.019  +0.122  +0.268  +0.395  +0.408  +0.394  -0.234  -0.233  -0.127  +0.046  +0.268  +0.399  +1.000  +0.396  -0.328  -0.305  -0.247  -0.037  +0.183  +0.340  +0.394  +0.389  -0.371  -0.392  -0.350  -0.198  +0.028  +0.225  +0.317  +0.311  -0.389  -0.397  -0.393  -0.280  -0.083  +0.093  +0.193  +0.231  -0.340  -0.367  -0.360  -0.272  -0.127  -0.015  +0.079  +0.124  -0.259  -0.309  -0.314  -0.239  -0.171  -0.080  +0.035  +0.060\n",
      "                         :   var23:  -0.112  -0.106  -0.028  +0.066  +0.184  +0.261  +0.309  +0.271  -0.181  -0.157  -0.072  +0.053  +0.204  +0.312  +0.373  +0.336  -0.253  -0.243  -0.151  +0.004  +0.215  +0.360  +0.396  +1.000  -0.325  -0.318  -0.277  -0.091  +0.163  +0.323  +0.375  +0.374  -0.377  -0.361  -0.322  -0.174  +0.046  +0.235  +0.301  +0.333  -0.352  -0.372  -0.335  -0.213  -0.042  +0.130  +0.239  +0.254  -0.298  -0.317  -0.302  -0.221  -0.077  +0.060  +0.125  +0.154  -0.224  -0.253  -0.264  -0.189  -0.081  -0.013  +0.072  +0.093\n",
      "                         :   var24:  +0.254  +0.258  +0.218  +0.123  -0.012  -0.098  -0.177  -0.165  +0.328  +0.331  +0.269  +0.152  -0.011  -0.154  -0.220  -0.249  +0.384  +0.374  +0.326  +0.134  -0.061  -0.238  -0.328  -0.325  +1.000  +0.417  +0.316  +0.143  -0.130  -0.308  -0.367  -0.355  +0.386  +0.365  +0.286  +0.091  -0.154  -0.319  -0.390  -0.374  +0.309  +0.302  +0.193  +0.031  -0.174  -0.317  -0.365  -0.358  +0.233  +0.214  +0.143  -0.013  -0.168  -0.265  -0.344  -0.274  +0.119  +0.126  +0.076  -0.035  -0.138  -0.208  -0.242  -0.209\n",
      "                         :   var25:  +0.256  +0.269  +0.213  +0.118  +0.016  -0.102  -0.165  -0.177  +0.336  +0.341  +0.295  +0.159  -0.007  -0.137  -0.214  -0.261  +0.370  +0.384  +0.334  +0.179  -0.057  -0.228  -0.305  -0.318  +0.417  +1.000  +0.350  +0.161  -0.103  -0.321  -0.396  -0.362  +0.378  +0.362  +0.309  +0.095  -0.171  -0.347  -0.403  -0.399  +0.331  +0.292  +0.196  +0.035  -0.180  -0.349  -0.401  -0.386  +0.231  +0.199  +0.134  -0.016  -0.175  -0.283  -0.342  -0.315  +0.123  +0.115  +0.063  -0.052  -0.163  -0.235  -0.280  -0.247\n",
      "                         :   var26:  +0.243  +0.238  +0.213  +0.144  +0.041  -0.047  -0.107  -0.107  +0.283  +0.303  +0.263  +0.179  +0.037  -0.092  -0.181  -0.188  +0.308  +0.336  +0.321  +0.176  +0.022  -0.168  -0.247  -0.277  +0.316  +0.350  +1.000  +0.140  -0.077  -0.241  -0.303  -0.301  +0.307  +0.294  +0.250  +0.085  -0.149  -0.307  -0.350  -0.359  +0.226  +0.222  +0.113  +0.018  -0.195  -0.304  -0.350  -0.344  +0.168  +0.117  +0.070  -0.047  -0.163  -0.288  -0.319  -0.302  +0.091  +0.068  -0.005  -0.078  -0.168  -0.224  -0.282  -0.212\n",
      "                         :   var27:  +0.126  +0.154  +0.136  +0.136  +0.099  +0.061  +0.001  +0.003  +0.162  +0.180  +0.178  +0.163  +0.091  +0.043  -0.011  -0.041  +0.133  +0.155  +0.161  +0.114  +0.090  -0.006  -0.037  -0.091  +0.143  +0.161  +0.140  +1.000  +0.026  -0.090  -0.098  -0.135  +0.081  +0.094  +0.045  -0.005  -0.088  -0.139  -0.148  -0.158  +0.035  +0.021  -0.003  -0.072  -0.154  -0.170  -0.192  -0.178  -0.014  -0.002  -0.061  -0.109  -0.175  -0.205  -0.220  -0.162  -0.024  -0.091  -0.082  -0.141  -0.173  -0.172  -0.186  -0.139\n",
      "                         :   var28:  -0.008  +0.015  +0.069  +0.106  +0.135  +0.171  +0.178  +0.131  -0.046  +0.006  +0.059  +0.098  +0.174  +0.183  +0.190  +0.160  -0.092  -0.048  -0.001  +0.087  +0.154  +0.154  +0.183  +0.163  -0.130  -0.103  -0.077  +0.026  +1.000  +0.150  +0.172  +0.134  -0.172  -0.177  -0.151  -0.084  +0.010  +0.055  +0.099  +0.102  -0.191  -0.198  -0.217  -0.136  -0.097  -0.010  +0.060  +0.062  -0.184  -0.197  -0.201  -0.159  -0.119  -0.064  -0.032  -0.008  -0.150  -0.195  -0.197  -0.172  -0.131  -0.089  -0.059  -0.034\n",
      "                         :   var29:  -0.129  -0.116  -0.053  +0.031  +0.141  +0.210  +0.236  +0.226  -0.180  -0.161  -0.088  +0.034  +0.171  +0.275  +0.289  +0.294  -0.244  -0.249  -0.160  -0.016  +0.172  +0.301  +0.340  +0.323  -0.308  -0.321  -0.241  -0.090  +0.150  +1.000  +0.335  +0.320  -0.339  -0.354  -0.316  -0.165  +0.074  +0.248  +0.287  +0.305  -0.338  -0.339  -0.306  -0.171  -0.029  +0.160  +0.205  +0.244  -0.285  -0.311  -0.286  -0.182  -0.034  +0.077  +0.144  +0.154  -0.223  -0.245  -0.233  -0.139  -0.070  +0.017  +0.087  +0.103\n",
      "                         :   var30:  -0.168  -0.166  -0.105  +0.001  +0.134  +0.207  +0.263  +0.254  -0.238  -0.218  -0.163  -0.002  +0.136  +0.279  +0.349  +0.324  -0.294  -0.290  -0.240  -0.048  +0.158  +0.321  +0.394  +0.375  -0.367  -0.396  -0.303  -0.098  +0.172  +0.335  +1.000  +0.419  -0.389  -0.407  -0.334  -0.156  +0.101  +0.296  +0.365  +0.358  -0.387  -0.372  -0.323  -0.184  +0.031  +0.218  +0.302  +0.317  -0.307  -0.306  -0.289  -0.183  -0.020  +0.123  +0.201  +0.212  -0.252  -0.262  -0.232  -0.140  -0.033  +0.054  +0.138  +0.131\n",
      "                         :   var31:  -0.184  -0.186  -0.112  -0.036  +0.096  +0.192  +0.244  +0.243  -0.247  -0.236  -0.167  -0.033  +0.112  +0.255  +0.308  +0.334  -0.307  -0.297  -0.242  -0.088  +0.150  +0.292  +0.389  +0.374  -0.355  -0.362  -0.301  -0.135  +0.134  +0.320  +0.419  +1.000  -0.399  -0.386  -0.344  -0.171  +0.087  +0.290  +0.367  +0.381  -0.352  -0.353  -0.304  -0.178  +0.025  +0.224  +0.322  +0.322  -0.299  -0.291  -0.272  -0.135  +0.004  +0.154  +0.235  +0.223  -0.202  -0.230  -0.207  -0.108  -0.009  +0.088  +0.151  +0.160\n",
      "                         :   var32:  +0.158  +0.154  +0.097  -0.016  -0.128  -0.202  -0.240  -0.225  +0.256  +0.207  +0.143  +0.005  -0.152  -0.261  -0.325  -0.296  +0.315  +0.288  +0.203  +0.038  -0.174  -0.305  -0.371  -0.377  +0.386  +0.378  +0.307  +0.081  -0.172  -0.339  -0.389  -0.399  +1.000  +0.410  +0.336  +0.152  -0.117  -0.315  -0.381  -0.373  +0.390  +0.384  +0.306  +0.133  -0.077  -0.251  -0.331  -0.324  +0.334  +0.321  +0.288  +0.134  -0.040  -0.176  -0.248  -0.257  +0.241  +0.261  +0.217  +0.100  +0.011  -0.105  -0.189  -0.182\n",
      "                         :   var33:  +0.138  +0.123  +0.075  -0.034  -0.156  -0.208  -0.260  -0.259  +0.207  +0.205  +0.115  -0.005  -0.170  -0.293  -0.331  -0.331  +0.319  +0.295  +0.205  +0.039  -0.176  -0.328  -0.392  -0.361  +0.365  +0.362  +0.294  +0.094  -0.177  -0.354  -0.407  -0.386  +0.410  +1.000  +0.345  +0.172  -0.100  -0.294  -0.362  -0.371  +0.385  +0.375  +0.312  +0.185  -0.059  -0.251  -0.326  -0.316  +0.321  +0.317  +0.283  +0.151  -0.023  -0.175  -0.233  -0.239  +0.229  +0.257  +0.209  +0.095  +0.001  -0.096  -0.192  -0.171\n",
      "                         :   var34:  +0.081  +0.066  +0.010  -0.075  -0.147  -0.222  -0.246  -0.233  +0.150  +0.118  +0.058  -0.057  -0.187  -0.285  -0.308  -0.291  +0.203  +0.197  +0.139  -0.011  -0.196  -0.316  -0.350  -0.322  +0.286  +0.309  +0.250  +0.045  -0.151  -0.316  -0.334  -0.344  +0.336  +0.345  +1.000  +0.170  -0.083  -0.242  -0.316  -0.304  +0.351  +0.343  +0.326  +0.187  -0.001  -0.175  -0.234  -0.257  +0.298  +0.305  +0.272  +0.175  +0.035  -0.107  -0.173  -0.200  +0.244  +0.242  +0.235  +0.134  +0.068  -0.052  -0.121  -0.114\n",
      "                         :   var35:  -0.024  -0.045  -0.082  -0.109  -0.153  -0.171  -0.166  -0.141  +0.001  -0.001  -0.084  -0.115  -0.168  -0.181  -0.200  -0.175  +0.038  +0.046  -0.006  -0.063  -0.145  -0.196  -0.198  -0.174  +0.091  +0.095  +0.085  -0.005  -0.084  -0.165  -0.156  -0.171  +0.152  +0.172  +0.170  +1.000  -0.008  -0.079  -0.111  -0.141  +0.156  +0.157  +0.187  +0.144  +0.086  -0.021  -0.045  -0.088  +0.147  +0.183  +0.175  +0.170  +0.093  +0.022  -0.007  -0.043  +0.116  +0.145  +0.145  +0.143  +0.077  +0.052  +0.020  -0.009\n",
      "                         :   var36:  -0.160  -0.189  -0.197  -0.146  -0.133  -0.090  -0.056  -0.046  -0.176  -0.236  -0.193  -0.162  -0.120  -0.066  -0.010  -0.003  -0.173  -0.188  -0.192  -0.149  -0.066  -0.020  +0.028  +0.046  -0.154  -0.171  -0.149  -0.088  +0.010  +0.074  +0.101  +0.087  -0.117  -0.100  -0.083  -0.008  +1.000  +0.162  +0.157  +0.140  -0.080  -0.058  +0.008  +0.093  +0.153  +0.186  +0.187  +0.155  -0.061  +0.011  +0.034  +0.106  +0.164  +0.177  +0.188  +0.140  -0.015  -0.001  +0.068  +0.109  +0.167  +0.161  +0.154  +0.125\n",
      "                         :   var37:  -0.222  -0.248  -0.258  -0.167  -0.097  -0.001  +0.063  +0.094  -0.294  -0.321  -0.286  -0.191  -0.076  +0.051  +0.134  +0.164  -0.326  -0.341  -0.306  -0.208  -0.014  +0.127  +0.225  +0.235  -0.319  -0.347  -0.307  -0.139  +0.055  +0.248  +0.296  +0.290  -0.315  -0.294  -0.242  -0.079  +0.162  +1.000  +0.330  +0.323  -0.236  -0.229  -0.148  +0.014  +0.181  +0.321  +0.356  +0.331  -0.193  -0.153  -0.081  +0.037  +0.163  +0.286  +0.303  +0.271  -0.122  -0.095  -0.047  +0.045  +0.155  +0.216  +0.251  +0.210\n",
      "                         :   var38:  -0.260  -0.273  -0.219  -0.148  -0.032  +0.055  +0.124  +0.142  -0.308  -0.333  -0.294  -0.177  -0.022  +0.127  +0.211  +0.218  -0.371  -0.375  -0.342  -0.185  +0.008  +0.192  +0.317  +0.301  -0.390  -0.403  -0.350  -0.148  +0.099  +0.287  +0.365  +0.367  -0.381  -0.362  -0.316  -0.111  +0.157  +0.330  +1.000  +0.409  -0.320  -0.305  -0.211  -0.046  +0.173  +0.353  +0.397  +0.389  -0.240  -0.206  -0.144  +0.007  +0.174  +0.280  +0.352  +0.326  -0.157  -0.148  -0.115  +0.017  +0.124  +0.229  +0.282  +0.238\n",
      "                         :   var39:  -0.223  -0.224  -0.189  -0.114  -0.007  +0.093  +0.152  +0.178  -0.285  -0.299  -0.263  -0.138  +0.034  +0.163  +0.222  +0.259  -0.338  -0.359  -0.312  -0.164  +0.051  +0.224  +0.311  +0.333  -0.374  -0.399  -0.359  -0.158  +0.102  +0.305  +0.358  +0.381  -0.373  -0.371  -0.304  -0.141  +0.140  +0.323  +0.409  +1.000  -0.313  -0.307  -0.256  -0.084  +0.138  +0.319  +0.378  +0.368  -0.244  -0.226  -0.186  -0.055  +0.120  +0.270  +0.313  +0.288  -0.159  -0.157  -0.128  -0.023  +0.106  +0.201  +0.249  +0.218\n",
      "                         :   var40:  +0.093  +0.070  -0.009  -0.095  -0.185  -0.249  -0.272  -0.240  +0.173  +0.114  +0.046  -0.087  -0.214  -0.319  -0.319  -0.304  +0.244  +0.201  +0.115  -0.028  -0.223  -0.321  -0.389  -0.352  +0.309  +0.331  +0.226  +0.035  -0.191  -0.338  -0.387  -0.352  +0.390  +0.385  +0.351  +0.156  -0.080  -0.236  -0.320  -0.313  +1.000  +0.387  +0.354  +0.204  +0.010  -0.157  -0.247  -0.263  +0.356  +0.349  +0.327  +0.208  +0.055  -0.081  -0.170  -0.182  +0.280  +0.325  +0.281  +0.173  +0.069  -0.022  -0.106  -0.116\n",
      "                         :   var41:  +0.040  +0.013  -0.067  -0.145  -0.233  -0.296  -0.313  -0.267  +0.128  +0.055  -0.012  -0.143  -0.273  -0.354  -0.366  -0.340  +0.211  +0.179  +0.092  -0.081  -0.265  -0.356  -0.397  -0.372  +0.302  +0.292  +0.222  +0.021  -0.198  -0.339  -0.372  -0.353  +0.384  +0.375  +0.343  +0.157  -0.058  -0.229  -0.305  -0.307  +0.387  +1.000  +0.384  +0.255  +0.061  -0.127  -0.228  -0.229  +0.368  +0.387  +0.384  +0.267  +0.107  -0.046  -0.113  -0.165  +0.305  +0.338  +0.323  +0.243  +0.131  +0.025  -0.075  -0.090\n",
      "                         :   var42:  -0.028  -0.066  -0.157  -0.243  -0.317  -0.328  -0.316  -0.277  +0.028  -0.024  -0.130  -0.230  -0.336  -0.378  -0.363  -0.329  +0.125  +0.067  -0.025  -0.171  -0.312  -0.367  -0.393  -0.335  +0.193  +0.196  +0.113  -0.003  -0.217  -0.306  -0.323  -0.304  +0.306  +0.312  +0.326  +0.187  +0.008  -0.148  -0.211  -0.256  +0.354  +0.384  +1.000  +0.300  +0.151  -0.037  -0.111  -0.173  +0.344  +0.370  +0.403  +0.334  +0.214  +0.069  -0.000  -0.069  +0.302  +0.351  +0.371  +0.305  +0.227  +0.132  +0.026  -0.013\n",
      "                         :   var43:  -0.120  -0.185  -0.258  -0.285  -0.320  -0.314  -0.295  -0.239  -0.104  -0.165  -0.233  -0.295  -0.336  -0.349  -0.301  -0.261  -0.053  -0.093  -0.147  -0.215  -0.292  -0.306  -0.280  -0.213  +0.031  +0.035  +0.018  -0.072  -0.136  -0.171  -0.184  -0.178  +0.133  +0.185  +0.187  +0.144  +0.093  +0.014  -0.046  -0.084  +0.204  +0.255  +0.300  +1.000  +0.242  +0.111  +0.054  +0.013  +0.221  +0.293  +0.336  +0.343  +0.275  +0.208  +0.130  +0.045  +0.235  +0.272  +0.335  +0.316  +0.279  +0.216  +0.157  +0.083\n",
      "                         :   var44:  -0.226  -0.281  -0.341  -0.341  -0.313  -0.253  -0.191  -0.128  -0.259  -0.315  -0.345  -0.362  -0.316  -0.254  -0.172  -0.105  -0.228  -0.266  -0.317  -0.292  -0.260  -0.156  -0.083  -0.042  -0.174  -0.180  -0.195  -0.154  -0.097  -0.029  +0.031  +0.025  -0.077  -0.059  -0.001  +0.086  +0.153  +0.181  +0.173  +0.138  +0.010  +0.061  +0.151  +0.242  +1.000  +0.317  +0.283  +0.220  +0.054  +0.134  +0.212  +0.302  +0.340  +0.356  +0.310  +0.234  +0.107  +0.148  +0.239  +0.298  +0.350  +0.338  +0.305  +0.209\n",
      "                         :   var45:  -0.277  -0.309  -0.343  -0.307  -0.223  -0.159  -0.081  -0.030  -0.330  -0.391  -0.392  -0.318  -0.219  -0.105  -0.017  +0.056  -0.345  -0.394  -0.379  -0.319  -0.162  +0.005  +0.093  +0.130  -0.317  -0.349  -0.304  -0.170  -0.010  +0.160  +0.218  +0.224  -0.251  -0.251  -0.175  -0.021  +0.186  +0.321  +0.353  +0.319  -0.157  -0.127  -0.037  +0.111  +0.317  +1.000  +0.413  +0.347  -0.093  -0.014  +0.070  +0.202  +0.335  +0.408  +0.403  +0.335  -0.036  +0.027  +0.102  +0.192  +0.312  +0.349  +0.358  +0.290\n",
      "                         :   var46:  -0.276  -0.340  -0.311  -0.268  -0.168  -0.090  +0.012  +0.076  -0.346  -0.385  -0.382  -0.295  -0.154  -0.011  +0.089  +0.139  -0.372  -0.398  -0.390  -0.282  -0.113  +0.075  +0.193  +0.239  -0.365  -0.401  -0.350  -0.192  +0.060  +0.205  +0.302  +0.322  -0.331  -0.326  -0.234  -0.045  +0.187  +0.356  +0.397  +0.378  -0.247  -0.228  -0.111  +0.054  +0.283  +0.413  +1.000  +0.421  -0.161  -0.114  -0.042  +0.131  +0.283  +0.383  +0.422  +0.375  -0.088  -0.058  +0.020  +0.140  +0.272  +0.336  +0.356  +0.290\n",
      "                         :   var47:  -0.231  -0.284  -0.269  -0.209  -0.111  -0.013  +0.045  +0.102  -0.309  -0.336  -0.317  -0.227  -0.116  +0.031  +0.130  +0.178  -0.339  -0.376  -0.337  -0.222  -0.048  +0.124  +0.231  +0.254  -0.358  -0.386  -0.344  -0.178  +0.062  +0.244  +0.317  +0.322  -0.324  -0.316  -0.257  -0.088  +0.155  +0.331  +0.389  +0.368  -0.263  -0.229  -0.173  +0.013  +0.220  +0.347  +0.421  +1.000  -0.185  -0.147  -0.098  +0.066  +0.224  +0.337  +0.373  +0.329  -0.110  -0.091  -0.036  +0.080  +0.190  +0.261  +0.326  +0.257\n",
      "                         :   var48:  +0.011  -0.013  -0.078  -0.157  -0.218  -0.263  -0.248  -0.228  +0.075  +0.033  -0.037  -0.137  -0.264  -0.304  -0.312  -0.258  +0.153  +0.121  +0.028  -0.096  -0.251  -0.317  -0.340  -0.298  +0.233  +0.231  +0.168  -0.014  -0.184  -0.285  -0.307  -0.299  +0.334  +0.321  +0.298  +0.147  -0.061  -0.193  -0.240  -0.244  +0.356  +0.368  +0.344  +0.221  +0.054  -0.093  -0.161  -0.185  +1.000  +0.340  +0.345  +0.252  +0.102  -0.005  -0.086  -0.133  +0.265  +0.317  +0.305  +0.219  +0.133  +0.041  -0.049  -0.066\n",
      "                         :   var49:  -0.016  -0.067  -0.164  -0.221  -0.289  -0.323  -0.308  -0.259  +0.006  -0.045  -0.124  -0.235  -0.329  -0.385  -0.354  -0.320  +0.095  +0.067  -0.021  -0.170  -0.317  -0.355  -0.367  -0.317  +0.214  +0.199  +0.117  -0.002  -0.197  -0.311  -0.306  -0.291  +0.321  +0.317  +0.305  +0.183  +0.011  -0.153  -0.206  -0.226  +0.349  +0.387  +0.370  +0.293  +0.134  -0.014  -0.114  -0.147  +0.340  +1.000  +0.404  +0.349  +0.211  +0.069  -0.011  -0.068  +0.293  +0.369  +0.370  +0.314  +0.234  +0.122  +0.022  -0.015\n",
      "                         :   var50:  -0.072  -0.145  -0.228  -0.309  -0.337  -0.360  -0.331  -0.282  -0.057  -0.122  -0.206  -0.292  -0.372  -0.404  -0.393  -0.320  +0.043  -0.015  -0.111  -0.220  -0.354  -0.369  -0.360  -0.302  +0.143  +0.134  +0.070  -0.061  -0.201  -0.286  -0.289  -0.272  +0.288  +0.283  +0.272  +0.175  +0.034  -0.081  -0.144  -0.186  +0.327  +0.384  +0.403  +0.336  +0.212  +0.070  -0.042  -0.098  +0.345  +0.404  +1.000  +0.379  +0.278  +0.144  +0.072  -0.007  +0.303  +0.374  +0.401  +0.367  +0.289  +0.203  +0.098  +0.035\n",
      "                         :   var51:  -0.181  -0.236  -0.327  -0.377  -0.386  -0.373  -0.306  -0.254  -0.177  -0.248  -0.324  -0.375  -0.414  -0.392  -0.332  -0.253  -0.115  -0.152  -0.229  -0.290  -0.351  -0.311  -0.272  -0.221  -0.013  -0.016  -0.047  -0.109  -0.159  -0.182  -0.183  -0.135  +0.134  +0.151  +0.175  +0.170  +0.106  +0.037  +0.007  -0.055  +0.208  +0.267  +0.334  +0.343  +0.302  +0.202  +0.131  +0.066  +0.252  +0.349  +0.379  +1.000  +0.371  +0.277  +0.228  +0.120  +0.251  +0.354  +0.402  +0.409  +0.371  +0.303  +0.217  +0.139\n",
      "                         :   var52:  -0.243  -0.307  -0.382  -0.388  -0.355  -0.299  -0.230  -0.169  -0.254  -0.336  -0.395  -0.403  -0.391  -0.299  -0.220  -0.161  -0.229  -0.280  -0.320  -0.348  -0.321  -0.224  -0.127  -0.077  -0.168  -0.175  -0.163  -0.175  -0.119  -0.034  -0.020  +0.004  -0.040  -0.023  +0.035  +0.093  +0.164  +0.163  +0.174  +0.120  +0.055  +0.107  +0.214  +0.275  +0.340  +0.335  +0.283  +0.224  +0.102  +0.211  +0.278  +0.371  +1.000  +0.397  +0.345  +0.242  +0.154  +0.225  +0.316  +0.357  +0.422  +0.385  +0.325  +0.270\n",
      "                         :   var53:  -0.265  -0.329  -0.360  -0.360  -0.306  -0.222  -0.147  -0.101  -0.320  -0.381  -0.398  -0.378  -0.315  -0.194  -0.109  -0.042  -0.310  -0.357  -0.370  -0.350  -0.237  -0.096  -0.015  +0.060  -0.265  -0.283  -0.288  -0.205  -0.064  +0.077  +0.123  +0.154  -0.176  -0.175  -0.107  +0.022  +0.177  +0.286  +0.280  +0.270  -0.081  -0.046  +0.069  +0.208  +0.356  +0.408  +0.383  +0.337  -0.005  +0.069  +0.144  +0.277  +0.397  +1.000  +0.413  +0.338  +0.033  +0.116  +0.203  +0.299  +0.393  +0.403  +0.389  +0.326\n",
      "                         :   var54:  -0.263  -0.322  -0.340  -0.310  -0.238  -0.158  -0.086  -0.038  -0.317  -0.399  -0.383  -0.335  -0.233  -0.126  -0.024  +0.032  -0.339  -0.370  -0.369  -0.318  -0.178  -0.028  +0.079  +0.125  -0.344  -0.342  -0.319  -0.220  -0.032  +0.144  +0.201  +0.235  -0.248  -0.233  -0.173  -0.007  +0.188  +0.303  +0.352  +0.313  -0.170  -0.113  -0.000  +0.130  +0.310  +0.403  +0.422  +0.373  -0.086  -0.011  +0.072  +0.228  +0.345  +0.413  +1.000  +0.363  -0.014  +0.039  +0.115  +0.225  +0.346  +0.377  +0.379  +0.324\n",
      "                         :   var55:  -0.220  -0.268  -0.267  -0.221  -0.165  -0.077  -0.012  +0.013  -0.259  -0.308  -0.308  -0.241  -0.163  -0.037  +0.039  +0.076  -0.288  -0.331  -0.311  -0.222  -0.094  +0.029  +0.124  +0.154  -0.274  -0.315  -0.302  -0.162  -0.008  +0.154  +0.212  +0.223  -0.257  -0.239  -0.200  -0.043  +0.140  +0.271  +0.326  +0.288  -0.182  -0.165  -0.069  +0.045  +0.234  +0.335  +0.375  +0.329  -0.133  -0.068  -0.007  +0.120  +0.242  +0.338  +0.363  +1.000  -0.065  -0.019  +0.045  +0.133  +0.244  +0.276  +0.330  +0.239\n",
      "                         :   var56:  -0.015  -0.053  -0.123  -0.179  -0.207  -0.226  -0.217  -0.192  +0.005  -0.032  -0.092  -0.171  -0.254  -0.290  -0.255  -0.192  +0.062  +0.052  -0.040  -0.118  -0.220  -0.268  -0.259  -0.224  +0.119  +0.123  +0.091  -0.024  -0.150  -0.223  -0.252  -0.202  +0.241  +0.229  +0.244  +0.116  -0.015  -0.122  -0.157  -0.159  +0.280  +0.305  +0.302  +0.235  +0.107  -0.036  -0.088  -0.110  +0.265  +0.293  +0.303  +0.251  +0.154  +0.033  -0.014  -0.065  +1.000  +0.287  +0.294  +0.232  +0.173  +0.104  -0.009  -0.010\n",
      "                         :   var57:  -0.087  -0.123  -0.200  -0.255  -0.281  -0.284  -0.270  -0.219  -0.036  -0.087  -0.183  -0.252  -0.323  -0.363  -0.306  -0.254  +0.041  -0.013  -0.085  -0.198  -0.298  -0.310  -0.309  -0.253  +0.126  +0.115  +0.068  -0.091  -0.195  -0.245  -0.262  -0.230  +0.261  +0.257  +0.242  +0.145  -0.001  -0.095  -0.148  -0.157  +0.325  +0.338  +0.351  +0.272  +0.148  +0.027  -0.058  -0.091  +0.317  +0.369  +0.374  +0.354  +0.225  +0.116  +0.039  -0.019  +0.287  +1.000  +0.362  +0.314  +0.262  +0.152  +0.069  +0.034\n",
      "                         :   var58:  -0.123  -0.166  -0.252  -0.317  -0.347  -0.349  -0.315  -0.257  -0.086  -0.159  -0.243  -0.319  -0.379  -0.367  -0.334  -0.277  -0.026  -0.071  -0.138  -0.258  -0.323  -0.340  -0.314  -0.264  +0.076  +0.063  -0.005  -0.082  -0.197  -0.233  -0.232  -0.207  +0.217  +0.209  +0.235  +0.145  +0.068  -0.047  -0.115  -0.128  +0.281  +0.323  +0.371  +0.335  +0.239  +0.102  +0.020  -0.036  +0.305  +0.370  +0.401  +0.402  +0.316  +0.203  +0.115  +0.045  +0.294  +0.362  +1.000  +0.383  +0.324  +0.232  +0.142  +0.084\n",
      "                         :   var59:  -0.168  -0.231  -0.330  -0.359  -0.357  -0.314  -0.292  -0.215  -0.187  -0.249  -0.315  -0.366  -0.378  -0.366  -0.298  -0.221  -0.114  -0.164  -0.246  -0.312  -0.335  -0.297  -0.239  -0.189  -0.035  -0.052  -0.078  -0.141  -0.172  -0.139  -0.140  -0.108  +0.100  +0.095  +0.134  +0.143  +0.109  +0.045  +0.017  -0.023  +0.173  +0.243  +0.305  +0.316  +0.298  +0.192  +0.140  +0.080  +0.219  +0.314  +0.367  +0.409  +0.357  +0.299  +0.225  +0.133  +0.232  +0.314  +0.383  +1.000  +0.365  +0.329  +0.227  +0.158\n",
      "                         :   var60:  -0.228  -0.295  -0.363  -0.378  -0.361  -0.320  -0.254  -0.182  -0.250  -0.319  -0.378  -0.387  -0.393  -0.319  -0.235  -0.166  -0.203  -0.275  -0.317  -0.348  -0.329  -0.236  -0.171  -0.081  -0.138  -0.163  -0.168  -0.173  -0.131  -0.070  -0.033  -0.009  +0.011  +0.001  +0.068  +0.077  +0.167  +0.155  +0.124  +0.106  +0.069  +0.131  +0.227  +0.279  +0.350  +0.312  +0.272  +0.190  +0.133  +0.234  +0.289  +0.371  +0.422  +0.393  +0.346  +0.244  +0.173  +0.262  +0.324  +0.365  +1.000  +0.393  +0.326  +0.243\n",
      "                         :   var61:  -0.231  -0.284  -0.333  -0.340  -0.318  -0.253  -0.190  -0.105  -0.271  -0.342  -0.379  -0.370  -0.323  -0.235  -0.151  -0.075  -0.253  -0.299  -0.344  -0.333  -0.266  -0.146  -0.080  -0.013  -0.208  -0.235  -0.224  -0.172  -0.089  +0.017  +0.054  +0.088  -0.105  -0.096  -0.052  +0.052  +0.161  +0.216  +0.229  +0.201  -0.022  +0.025  +0.132  +0.216  +0.338  +0.349  +0.336  +0.261  +0.041  +0.122  +0.203  +0.303  +0.385  +0.403  +0.377  +0.276  +0.104  +0.152  +0.232  +0.329  +0.393  +1.000  +0.361  +0.272\n",
      "                         :   var62:  -0.225  -0.262  -0.313  -0.298  -0.241  -0.173  -0.101  -0.062  -0.280  -0.334  -0.345  -0.323  -0.239  -0.139  -0.065  -0.014  -0.279  -0.334  -0.343  -0.298  -0.193  -0.057  +0.035  +0.072  -0.242  -0.280  -0.282  -0.186  -0.059  +0.087  +0.138  +0.151  -0.189  -0.192  -0.121  +0.020  +0.154  +0.251  +0.282  +0.249  -0.106  -0.075  +0.026  +0.157  +0.305  +0.358  +0.356  +0.326  -0.049  +0.022  +0.098  +0.217  +0.325  +0.389  +0.379  +0.330  -0.009  +0.069  +0.142  +0.227  +0.326  +0.361  +1.000  +0.289\n",
      "                         :   var63:  -0.170  -0.217  -0.257  -0.210  -0.177  -0.105  -0.061  -0.002  -0.227  -0.254  -0.261  -0.229  -0.167  -0.080  -0.009  +0.018  -0.235  -0.258  -0.277  -0.239  -0.143  -0.022  +0.060  +0.093  -0.209  -0.247  -0.212  -0.139  -0.034  +0.103  +0.131  +0.160  -0.182  -0.171  -0.114  -0.009  +0.125  +0.210  +0.238  +0.218  -0.116  -0.090  -0.013  +0.083  +0.209  +0.290  +0.290  +0.257  -0.066  -0.015  +0.035  +0.139  +0.270  +0.326  +0.324  +0.239  -0.010  +0.034  +0.084  +0.158  +0.243  +0.272  +0.289  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DataSetFactory           : [dataset] :  \n",
      "                         : \n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'var0' <---> Output : variable 'var0'\n",
      "                         : Input : variable 'var1' <---> Output : variable 'var1'\n",
      "                         : Input : variable 'var2' <---> Output : variable 'var2'\n",
      "                         : Input : variable 'var3' <---> Output : variable 'var3'\n",
      "                         : Input : variable 'var4' <---> Output : variable 'var4'\n",
      "                         : Input : variable 'var5' <---> Output : variable 'var5'\n",
      "                         : Input : variable 'var6' <---> Output : variable 'var6'\n",
      "                         : Input : variable 'var7' <---> Output : variable 'var7'\n",
      "                         : Input : variable 'var8' <---> Output : variable 'var8'\n",
      "                         : Input : variable 'var9' <---> Output : variable 'var9'\n",
      "                         : Input : variable 'var10' <---> Output : variable 'var10'\n",
      "                         : Input : variable 'var11' <---> Output : variable 'var11'\n",
      "                         : Input : variable 'var12' <---> Output : variable 'var12'\n",
      "                         : Input : variable 'var13' <---> Output : variable 'var13'\n",
      "                         : Input : variable 'var14' <---> Output : variable 'var14'\n",
      "                         : Input : variable 'var15' <---> Output : variable 'var15'\n",
      "                         : Input : variable 'var16' <---> Output : variable 'var16'\n",
      "                         : Input : variable 'var17' <---> Output : variable 'var17'\n",
      "                         : Input : variable 'var18' <---> Output : variable 'var18'\n",
      "                         : Input : variable 'var19' <---> Output : variable 'var19'\n",
      "                         : Input : variable 'var20' <---> Output : variable 'var20'\n",
      "                         : Input : variable 'var21' <---> Output : variable 'var21'\n",
      "                         : Input : variable 'var22' <---> Output : variable 'var22'\n",
      "                         : Input : variable 'var23' <---> Output : variable 'var23'\n",
      "                         : Input : variable 'var24' <---> Output : variable 'var24'\n",
      "                         : Input : variable 'var25' <---> Output : variable 'var25'\n",
      "                         : Input : variable 'var26' <---> Output : variable 'var26'\n",
      "                         : Input : variable 'var27' <---> Output : variable 'var27'\n",
      "                         : Input : variable 'var28' <---> Output : variable 'var28'\n",
      "                         : Input : variable 'var29' <---> Output : variable 'var29'\n",
      "                         : Input : variable 'var30' <---> Output : variable 'var30'\n",
      "                         : Input : variable 'var31' <---> Output : variable 'var31'\n",
      "                         : Input : variable 'var32' <---> Output : variable 'var32'\n",
      "                         : Input : variable 'var33' <---> Output : variable 'var33'\n",
      "                         : Input : variable 'var34' <---> Output : variable 'var34'\n",
      "                         : Input : variable 'var35' <---> Output : variable 'var35'\n",
      "                         : Input : variable 'var36' <---> Output : variable 'var36'\n",
      "                         : Input : variable 'var37' <---> Output : variable 'var37'\n",
      "                         : Input : variable 'var38' <---> Output : variable 'var38'\n",
      "                         : Input : variable 'var39' <---> Output : variable 'var39'\n",
      "                         : Input : variable 'var40' <---> Output : variable 'var40'\n",
      "                         : Input : variable 'var41' <---> Output : variable 'var41'\n",
      "                         : Input : variable 'var42' <---> Output : variable 'var42'\n",
      "                         : Input : variable 'var43' <---> Output : variable 'var43'\n",
      "                         : Input : variable 'var44' <---> Output : variable 'var44'\n",
      "                         : Input : variable 'var45' <---> Output : variable 'var45'\n",
      "                         : Input : variable 'var46' <---> Output : variable 'var46'\n",
      "                         : Input : variable 'var47' <---> Output : variable 'var47'\n",
      "                         : Input : variable 'var48' <---> Output : variable 'var48'\n",
      "                         : Input : variable 'var49' <---> Output : variable 'var49'\n",
      "                         : Input : variable 'var50' <---> Output : variable 'var50'\n",
      "                         : Input : variable 'var51' <---> Output : variable 'var51'\n",
      "                         : Input : variable 'var52' <---> Output : variable 'var52'\n",
      "                         : Input : variable 'var53' <---> Output : variable 'var53'\n",
      "                         : Input : variable 'var54' <---> Output : variable 'var54'\n",
      "                         : Input : variable 'var55' <---> Output : variable 'var55'\n",
      "                         : Input : variable 'var56' <---> Output : variable 'var56'\n",
      "                         : Input : variable 'var57' <---> Output : variable 'var57'\n",
      "                         : Input : variable 'var58' <---> Output : variable 'var58'\n",
      "                         : Input : variable 'var59' <---> Output : variable 'var59'\n",
      "                         : Input : variable 'var60' <---> Output : variable 'var60'\n",
      "                         : Input : variable 'var61' <---> Output : variable 'var61'\n",
      "                         : Input : variable 'var62' <---> Output : variable 'var62'\n",
      "                         : Input : variable 'var63' <---> Output : variable 'var63'\n",
      "TFHandler_Factory        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4991     3.8926   [    -10.257     22.057 ]\n",
      "                         :     var1:     3.9787     4.5075   [    -10.122     25.016 ]\n",
      "                         :     var2:     5.5671     4.8848   [    -9.4795     26.928 ]\n",
      "                         :     var3:     6.5379     5.0192   [    -9.7689     29.230 ]\n",
      "                         :     var4:     6.3871     4.9931   [    -11.476     28.503 ]\n",
      "                         :     var5:     5.4269     4.8235   [    -9.1293     30.122 ]\n",
      "                         :     var6:     3.9107     4.4655   [    -11.141     25.933 ]\n",
      "                         :     var7:     2.4388     3.8766   [    -9.8326     22.021 ]\n",
      "                         :     var8:     3.7375     4.4169   [    -9.0364     24.243 ]\n",
      "                         :     var9:     6.0120     5.1559   [    -9.2157     28.235 ]\n",
      "                         :    var10:     8.3558     5.4951   [    -7.3822     29.694 ]\n",
      "                         :    var11:     9.8310     5.5008   [    -7.2496     32.294 ]\n",
      "                         :    var12:     9.7061     5.5019   [    -9.2380     31.239 ]\n",
      "                         :    var13:     8.2453     5.5157   [    -8.3455     31.442 ]\n",
      "                         :    var14:     5.9694     5.0886   [    -9.2506     29.027 ]\n",
      "                         :    var15:     3.6435     4.3955   [    -10.587     23.653 ]\n",
      "                         :    var16:     4.8733     4.7778   [    -11.709     25.905 ]\n",
      "                         :    var17:     7.9688     5.5580   [    -7.5348     33.189 ]\n",
      "                         :    var18:     10.951     5.7539   [    -7.3453     34.627 ]\n",
      "                         :    var19:     12.853     5.4921   [    -4.1117     33.786 ]\n",
      "                         :    var20:     12.820     5.4867   [    -5.3808     32.315 ]\n",
      "                         :    var21:     10.889     5.7651   [    -6.3743     36.072 ]\n",
      "                         :    var22:     7.9053     5.5376   [    -7.9329     32.559 ]\n",
      "                         :    var23:     4.7962     4.7362   [    -10.919     23.587 ]\n",
      "                         :    var24:     5.6555     5.0342   [    -9.8326     29.659 ]\n",
      "                         :    var25:     9.1977     5.6658   [    -6.3310     34.142 ]\n",
      "                         :    var26:     12.554     5.6514   [    -5.9403     33.692 ]\n",
      "                         :    var27:     14.795     5.1093   [    -2.1615     37.430 ]\n",
      "                         :    var28:     14.660     5.2013   [    -2.8283     38.805 ]\n",
      "                         :    var29:     12.449     5.6721   [    -4.6051     37.288 ]\n",
      "                         :    var30:     9.0063     5.7010   [    -10.813     33.124 ]\n",
      "                         :    var31:     5.4970     4.9834   [    -10.336     30.010 ]\n",
      "                         :    var32:     5.6520     5.0125   [    -9.2211     27.428 ]\n",
      "                         :    var33:     9.1908     5.7259   [    -8.0987     33.206 ]\n",
      "                         :    var34:     12.684     5.6427   [    -5.9139     36.323 ]\n",
      "                         :    var35:     14.812     5.1435   [    -3.2133     34.607 ]\n",
      "                         :    var36:     14.707     5.1390   [    -2.8380     35.026 ]\n",
      "                         :    var37:     12.530     5.5950   [    -6.1929     33.746 ]\n",
      "                         :    var38:     9.0409     5.7810   [    -8.0031     32.626 ]\n",
      "                         :    var39:     5.5780     4.9512   [    -8.8630     29.784 ]\n",
      "                         :    var40:     4.9770     4.7758   [    -10.145     25.266 ]\n",
      "                         :    var41:     7.8714     5.5548   [    -8.5314     37.635 ]\n",
      "                         :    var42:     11.016     5.7577   [    -8.1448     36.124 ]\n",
      "                         :    var43:     12.821     5.4932   [    -6.6966     43.293 ]\n",
      "                         :    var44:     12.887     5.5346   [    -4.9059     34.631 ]\n",
      "                         :    var45:     10.988     5.8018   [    -6.8219     34.300 ]\n",
      "                         :    var46:     7.9529     5.5516   [    -10.776     32.909 ]\n",
      "                         :    var47:     4.8126     4.7994   [    -9.0429     27.598 ]\n",
      "                         :    var48:     3.8114     4.3913   [    -10.010     25.702 ]\n",
      "                         :    var49:     6.0972     5.0664   [    -8.8429     30.707 ]\n",
      "                         :    var50:     8.4053     5.4578   [    -8.4494     30.113 ]\n",
      "                         :    var51:     9.8185     5.5505   [    -8.6899     33.313 ]\n",
      "                         :    var52:     9.8044     5.5527   [    -6.3026     33.478 ]\n",
      "                         :    var53:     8.2980     5.4937   [    -8.6684     35.304 ]\n",
      "                         :    var54:     5.9857     5.0800   [    -8.5084     28.695 ]\n",
      "                         :    var55:     3.5967     4.2971   [    -8.7073     21.957 ]\n",
      "                         :    var56:     2.5031     3.9146   [    -11.497     20.361 ]\n",
      "                         :    var57:     3.9950     4.4431   [    -8.8077     25.394 ]\n",
      "                         :    var58:     5.6013     4.8508   [    -11.076     26.883 ]\n",
      "                         :    var59:     6.5296     5.0636   [    -9.0219     28.571 ]\n",
      "                         :    var60:     6.6129     5.0677   [    -9.9130     27.121 ]\n",
      "                         :    var61:     5.5612     4.9134   [    -9.6182     26.950 ]\n",
      "                         :    var62:     4.0187     4.5265   [    -11.569     26.781 ]\n",
      "                         :    var63:     2.4755     3.8720   [    -9.2495     22.465 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Separation\n",
      "                         : ------------------------------\n",
      "                         :    1 : var31     : 1.987e-02\n",
      "                         :    2 : var60     : 1.452e-02\n",
      "                         :    3 : var39     : 1.448e-02\n",
      "                         :    4 : var38     : 1.352e-02\n",
      "                         :    5 : var11     : 1.318e-02\n",
      "                         :    6 : var4      : 1.276e-02\n",
      "                         :    7 : var24     : 1.248e-02\n",
      "                         :    8 : var23     : 1.231e-02\n",
      "                         :    9 : var59     : 1.229e-02\n",
      "                         :   10 : var30     : 1.203e-02\n",
      "                         :   11 : var51     : 1.037e-02\n",
      "                         :   12 : var3      : 9.927e-03\n",
      "                         :   13 : var32     : 9.479e-03\n",
      "                         :   14 : var47     : 8.533e-03\n",
      "                         :   15 : var40     : 8.417e-03\n",
      "                         :   16 : var58     : 8.149e-03\n",
      "                         :   17 : var25     : 8.039e-03\n",
      "                         :   18 : var2      : 8.016e-03\n",
      "                         :   19 : var12     : 7.996e-03\n",
      "                         :   20 : var46     : 7.747e-03\n",
      "                         :   21 : var33     : 7.741e-03\n",
      "                         :   22 : var16     : 7.662e-03\n",
      "                         :   23 : var5      : 7.407e-03\n",
      "                         :   24 : var41     : 6.987e-03\n",
      "                         :   25 : var55     : 6.755e-03\n",
      "                         :   26 : var43     : 6.707e-03\n",
      "                         :   27 : var26     : 6.565e-03\n",
      "                         :   28 : var13     : 6.508e-03\n",
      "                         :   29 : var52     : 6.318e-03\n",
      "                         :   30 : var15     : 6.001e-03\n",
      "                         :   31 : var61     : 5.907e-03\n",
      "                         :   32 : var22     : 5.683e-03\n",
      "                         :   33 : var29     : 5.676e-03\n",
      "                         :   34 : var54     : 5.618e-03\n",
      "                         :   35 : var20     : 5.605e-03\n",
      "                         :   36 : var37     : 5.505e-03\n",
      "                         :   37 : var19     : 5.274e-03\n",
      "                         :   38 : var17     : 4.922e-03\n",
      "                         :   39 : var49     : 4.797e-03\n",
      "                         :   40 : var8      : 4.496e-03\n",
      "                         :   41 : var7      : 4.491e-03\n",
      "                         :   42 : var34     : 4.484e-03\n",
      "                         :   43 : var48     : 4.478e-03\n",
      "                         :   44 : var9      : 4.435e-03\n",
      "                         :   45 : var50     : 4.398e-03\n",
      "                         :   46 : var18     : 4.369e-03\n",
      "                         :   47 : var53     : 4.348e-03\n",
      "                         :   48 : var27     : 4.313e-03\n",
      "                         :   49 : var57     : 4.163e-03\n",
      "                         :   50 : var10     : 4.135e-03\n",
      "                         :   51 : var45     : 4.055e-03\n",
      "                         :   52 : var35     : 3.998e-03\n",
      "                         :   53 : var1      : 3.670e-03\n",
      "                         :   54 : var44     : 3.477e-03\n",
      "                         :   55 : var14     : 3.376e-03\n",
      "                         :   56 : var56     : 3.157e-03\n",
      "                         :   57 : var6      : 3.078e-03\n",
      "                         :   58 : var62     : 2.999e-03\n",
      "                         :   59 : var21     : 2.938e-03\n",
      "                         :   60 : var36     : 2.937e-03\n",
      "                         :   61 : var42     : 2.906e-03\n",
      "                         :   62 : var28     : 2.700e-03\n",
      "                         :   63 : var63     : 2.451e-03\n",
      "                         :   64 : var0      : 2.151e-03\n",
      "                         : ------------------------------\n",
      "Factory                  : Train method: DL_DENSE for Classification\n",
      "                         : \n",
      "                         : Preparing the Gaussian transformation...\n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:  0.0057930    0.99712   [    -3.1847     5.7307 ]\n",
      "                         :     var1:  0.0058083    0.99728   [    -3.1847     5.7307 ]\n",
      "                         :     var2:  0.0059004    0.99725   [    -3.1847     5.7307 ]\n",
      "                         :     var3:  0.0058396    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :     var4:  0.0059544    0.99700   [    -3.1847     5.7307 ]\n",
      "                         :     var5:  0.0057245    0.99680   [    -3.1847     5.7307 ]\n",
      "                         :     var6:  0.0066096     1.0003   [    -3.1847     5.7307 ]\n",
      "                         :     var7:  0.0062151    0.99941   [    -3.1847     5.7307 ]\n",
      "                         :     var8:  0.0066540     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :     var9:  0.0067823     1.0013   [    -3.1847     5.7307 ]\n",
      "                         :    var10:  0.0065501     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var11:  0.0068591     1.0016   [    -3.1847     5.7307 ]\n",
      "                         :    var12:  0.0063219    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var13:  0.0063785    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var14:  0.0065936     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var15:  0.0066219     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var16:  0.0067364     1.0012   [    -3.1847     5.7307 ]\n",
      "                         :    var17:  0.0061154    0.99827   [    -3.1847     5.7307 ]\n",
      "                         :    var18:  0.0061471    0.99841   [    -3.1847     5.7307 ]\n",
      "                         :    var19:  0.0064147    0.99954   [    -3.1847     5.7307 ]\n",
      "                         :    var20:  0.0070218     1.0024   [    -3.1847     5.7307 ]\n",
      "                         :    var21:  0.0057432    0.99688   [    -3.1847     5.7307 ]\n",
      "                         :    var22:  0.0059601    0.99819   [    -3.1847     5.7307 ]\n",
      "                         :    var23:  0.0068817     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var24:  0.0064032    0.99957   [    -3.1847     5.7307 ]\n",
      "                         :    var25:  0.0063012    0.99948   [    -3.1847     5.7307 ]\n",
      "                         :    var26:  0.0064047    0.99962   [    -3.1847     5.7307 ]\n",
      "                         :    var27:  0.0062421    0.99892   [    -3.1847     5.7307 ]\n",
      "                         :    var28:  0.0057685    0.99691   [    -3.1847     5.7307 ]\n",
      "                         :    var29:  0.0066603     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var30:  0.0060916    0.99794   [    -3.1847     5.7307 ]\n",
      "                         :    var31:  0.0057694    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :    var32:  0.0067821     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var33:  0.0073669     1.0037   [    -3.1847     5.7307 ]\n",
      "                         :    var34:  0.0061607    0.99837   [    -3.1847     5.7307 ]\n",
      "                         :    var35:  0.0061378    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var36:  0.0062884    0.99977   [    -3.1847     5.7307 ]\n",
      "                         :    var37:  0.0065205    0.99998   [    -3.1847     5.7307 ]\n",
      "                         :    var38:  0.0066631     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var39:  0.0057950    0.99679   [    -3.1847     5.7307 ]\n",
      "                         :    var40:  0.0064812    0.99958   [    -3.1847     5.7307 ]\n",
      "                         :    var41:  0.0061854    0.99925   [    -3.1847     5.7307 ]\n",
      "                         :    var42:  0.0061370    0.99813   [    -3.1847     5.7307 ]\n",
      "                         :    var43:  0.0057640    0.99668   [    -3.1847     5.7307 ]\n",
      "                         :    var44:  0.0070901     1.0028   [    -3.1847     5.7307 ]\n",
      "                         :    var45:  0.0060098    0.99803   [    -3.1847     5.7307 ]\n",
      "                         :    var46:  0.0060669    0.99820   [    -3.1847     5.7307 ]\n",
      "                         :    var47:  0.0067749     1.0020   [    -3.1847     5.7307 ]\n",
      "                         :    var48:  0.0057983    0.99693   [    -3.1847     5.7307 ]\n",
      "                         :    var49:  0.0059339    0.99801   [    -3.1847     5.7307 ]\n",
      "                         :    var50:  0.0067546     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var51:  0.0063016    0.99923   [    -3.1847     5.7307 ]\n",
      "                         :    var52:  0.0060385    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var53:  0.0065581     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var54:  0.0065533     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :    var55:  0.0069978     1.0026   [    -3.1846     5.7307 ]\n",
      "                         :    var56:  0.0059293    0.99710   [    -3.1847     5.7307 ]\n",
      "                         :    var57:  0.0063542    0.99967   [    -3.1847     5.7307 ]\n",
      "                         :    var58:  0.0062839    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var59:  0.0063015    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var60:  0.0071258     1.0027   [    -3.1847     5.7307 ]\n",
      "                         :    var61:  0.0063313    0.99942   [    -3.1847     5.7307 ]\n",
      "                         :    var62:  0.0063288    0.99931   [    -3.1847     5.7307 ]\n",
      "                         :    var63:  0.0059676    0.99805   [    -3.1847     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:  0.0057930    0.99712   [    -3.1847     5.7307 ]\n",
      "                         :     var1:  0.0058083    0.99728   [    -3.1847     5.7307 ]\n",
      "                         :     var2:  0.0059004    0.99725   [    -3.1847     5.7307 ]\n",
      "                         :     var3:  0.0058396    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :     var4:  0.0059544    0.99700   [    -3.1847     5.7307 ]\n",
      "                         :     var5:  0.0057245    0.99680   [    -3.1847     5.7307 ]\n",
      "                         :     var6:  0.0066096     1.0003   [    -3.1847     5.7307 ]\n",
      "                         :     var7:  0.0062151    0.99941   [    -3.1847     5.7307 ]\n",
      "                         :     var8:  0.0066540     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :     var9:  0.0067823     1.0013   [    -3.1847     5.7307 ]\n",
      "                         :    var10:  0.0065501     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var11:  0.0068591     1.0016   [    -3.1847     5.7307 ]\n",
      "                         :    var12:  0.0063219    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var13:  0.0063785    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var14:  0.0065936     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var15:  0.0066219     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var16:  0.0067364     1.0012   [    -3.1847     5.7307 ]\n",
      "                         :    var17:  0.0061154    0.99827   [    -3.1847     5.7307 ]\n",
      "                         :    var18:  0.0061471    0.99841   [    -3.1847     5.7307 ]\n",
      "                         :    var19:  0.0064147    0.99954   [    -3.1847     5.7307 ]\n",
      "                         :    var20:  0.0070218     1.0024   [    -3.1847     5.7307 ]\n",
      "                         :    var21:  0.0057432    0.99688   [    -3.1847     5.7307 ]\n",
      "                         :    var22:  0.0059601    0.99819   [    -3.1847     5.7307 ]\n",
      "                         :    var23:  0.0068817     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var24:  0.0064032    0.99957   [    -3.1847     5.7307 ]\n",
      "                         :    var25:  0.0063012    0.99948   [    -3.1847     5.7307 ]\n",
      "                         :    var26:  0.0064047    0.99962   [    -3.1847     5.7307 ]\n",
      "                         :    var27:  0.0062421    0.99892   [    -3.1847     5.7307 ]\n",
      "                         :    var28:  0.0057685    0.99691   [    -3.1847     5.7307 ]\n",
      "                         :    var29:  0.0066603     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var30:  0.0060916    0.99794   [    -3.1847     5.7307 ]\n",
      "                         :    var31:  0.0057694    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :    var32:  0.0067821     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var33:  0.0073669     1.0037   [    -3.1847     5.7307 ]\n",
      "                         :    var34:  0.0061607    0.99837   [    -3.1847     5.7307 ]\n",
      "                         :    var35:  0.0061378    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var36:  0.0062884    0.99977   [    -3.1847     5.7307 ]\n",
      "                         :    var37:  0.0065205    0.99998   [    -3.1847     5.7307 ]\n",
      "                         :    var38:  0.0066631     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var39:  0.0057950    0.99679   [    -3.1847     5.7307 ]\n",
      "                         :    var40:  0.0064812    0.99958   [    -3.1847     5.7307 ]\n",
      "                         :    var41:  0.0061854    0.99925   [    -3.1847     5.7307 ]\n",
      "                         :    var42:  0.0061370    0.99813   [    -3.1847     5.7307 ]\n",
      "                         :    var43:  0.0057640    0.99668   [    -3.1847     5.7307 ]\n",
      "                         :    var44:  0.0070901     1.0028   [    -3.1847     5.7307 ]\n",
      "                         :    var45:  0.0060098    0.99803   [    -3.1847     5.7307 ]\n",
      "                         :    var46:  0.0060669    0.99820   [    -3.1847     5.7307 ]\n",
      "                         :    var47:  0.0067749     1.0020   [    -3.1847     5.7307 ]\n",
      "                         :    var48:  0.0057983    0.99693   [    -3.1847     5.7307 ]\n",
      "                         :    var49:  0.0059339    0.99801   [    -3.1847     5.7307 ]\n",
      "                         :    var50:  0.0067546     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var51:  0.0063016    0.99923   [    -3.1847     5.7307 ]\n",
      "                         :    var52:  0.0060385    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var53:  0.0065581     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var54:  0.0065533     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :    var55:  0.0069978     1.0026   [    -3.1846     5.7307 ]\n",
      "                         :    var56:  0.0059293    0.99710   [    -3.1847     5.7307 ]\n",
      "                         :    var57:  0.0063542    0.99967   [    -3.1847     5.7307 ]\n",
      "                         :    var58:  0.0062839    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var59:  0.0063015    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var60:  0.0071258     1.0027   [    -3.1847     5.7307 ]\n",
      "                         :    var61:  0.0063313    0.99942   [    -3.1847     5.7307 ]\n",
      "                         :    var62:  0.0063288    0.99931   [    -3.1847     5.7307 ]\n",
      "                         :    var63:  0.0059676    0.99805   [    -3.1847     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0: -0.0095330    0.98960   [    -3.1011     5.7307 ]\n",
      "                         :     var1:  0.0051908    0.98827   [    -3.0819     5.7307 ]\n",
      "                         :     var2:  -0.012209     1.0051   [    -2.9940     5.7307 ]\n",
      "                         :     var3: -0.0077444     1.0017   [    -2.9487     5.7307 ]\n",
      "                         :     var4:   0.032363     1.0029   [    -3.0402     5.7307 ]\n",
      "                         :     var5:   0.017022     1.0003   [    -2.9642     5.7307 ]\n",
      "                         :     var6:   0.018034     1.0069   [    -2.9881     5.7307 ]\n",
      "                         :     var7:  0.0048645     1.0053   [    -3.2664     5.7307 ]\n",
      "                         :     var8: -0.0032578    0.99210   [    -4.0772     5.7307 ]\n",
      "                         :     var9:   0.021695    0.99335   [    -3.0462     5.7307 ]\n",
      "                         :    var10: 0.00026812     1.0183   [    -5.7307     5.7307 ]\n",
      "                         :    var11:  -0.029610     1.0036   [    -3.3082     5.7307 ]\n",
      "                         :    var12: -0.0076649     1.0063   [    -3.1145     5.7307 ]\n",
      "                         :    var13:   0.019686    0.99738   [    -2.9630     5.7307 ]\n",
      "                         :    var14:   0.022015     1.0020   [    -3.2442     5.7307 ]\n",
      "                         :    var15:  0.0048294     1.0013   [    -5.7307     5.7307 ]\n",
      "                         :    var16:-0.00052943    0.99487   [    -3.1147     5.7307 ]\n",
      "                         :    var17:  0.0017536    0.99378   [    -5.7307     5.7307 ]\n",
      "                         :    var18: -0.0051237     1.0022   [    -5.7307     5.7307 ]\n",
      "                         :    var19:  0.0065580     1.0038   [    -5.7307     5.7307 ]\n",
      "                         :    var20:   0.018176     1.0165   [    -3.0984     5.7307 ]\n",
      "                         :    var21:   0.016093    0.99921   [    -3.1905     5.7307 ]\n",
      "                         :    var22: -0.0041778    0.99578   [    -2.9864     5.7307 ]\n",
      "                         :    var23:   0.013143     1.0148   [    -2.9490     5.7307 ]\n",
      "                         :    var24:  0.0060337    0.97521   [    -2.9590     3.4374 ]\n",
      "                         :    var25:  -0.018589     1.0008   [    -5.7307     3.4852 ]\n",
      "                         :    var26:  0.0057727     1.0204   [    -5.7307     5.7307 ]\n",
      "                         :    var27:  -0.018895     1.0017   [    -2.9445     5.7307 ]\n",
      "                         :    var28:   0.011385    0.99187   [    -3.0082     5.7307 ]\n",
      "                         :    var29:  0.0088995     1.0081   [    -5.7307     5.7307 ]\n",
      "                         :    var30:  0.0056477    0.99603   [    -2.9480     5.7307 ]\n",
      "                         :    var31:   0.027005    0.99818   [    -3.0540     5.7307 ]\n",
      "                         :    var32: -0.0069123    0.99381   [    -5.7307     5.7307 ]\n",
      "                         :    var33:  0.0020646    0.99297   [    -3.1347     5.7307 ]\n",
      "                         :    var34:  0.0027272     1.0099   [    -5.7307     5.7307 ]\n",
      "                         :    var35:  -0.013270    0.99462   [    -3.0901     5.7307 ]\n",
      "                         :    var36:   0.019274     1.0013   [    -3.0362     5.7307 ]\n",
      "                         :    var37:  0.0056244     1.0079   [    -3.4400     5.7307 ]\n",
      "                         :    var38:  0.0069643    0.98228   [    -5.7307     5.7307 ]\n",
      "                         :    var39:  0.0016025    0.99666   [    -5.7307     5.7307 ]\n",
      "                         :    var40: 0.00055175     1.0097   [    -3.2601     5.7307 ]\n",
      "                         :    var41:   0.019092    0.99409   [    -3.1673     3.4511 ]\n",
      "                         :    var42:  0.0094501    0.99496   [    -3.0118     5.7307 ]\n",
      "                         :    var43:   0.010563    0.99084   [    -3.0916     3.4255 ]\n",
      "                         :    var44:  0.0057154     1.0020   [    -3.3622     5.7307 ]\n",
      "                         :    var45:   0.013756    0.99252   [    -5.7307     5.7307 ]\n",
      "                         :    var46:  0.0023129     1.0004   [    -3.0491     5.7307 ]\n",
      "                         :    var47:   0.033810    0.99825   [    -3.2081     3.7228 ]\n",
      "                         :    var48:  -0.014985    0.99946   [    -3.7496     5.7307 ]\n",
      "                         :    var49:  0.0078714     1.0128   [    -3.0852     5.7307 ]\n",
      "                         :    var50: -0.0053581     1.0077   [    -3.1462     5.7307 ]\n",
      "                         :    var51:   0.014792     1.0023   [    -3.1302     5.7307 ]\n",
      "                         :    var52:   0.014085    0.99832   [    -5.7307     5.7307 ]\n",
      "                         :    var53:   0.028836    0.99597   [    -3.6136     5.7307 ]\n",
      "                         :    var54:   0.025281     1.0115   [    -5.7307     5.7307 ]\n",
      "                         :    var55:   0.029288     1.0234   [    -5.7307     5.7307 ]\n",
      "                         :    var56: -0.0044719     1.0103   [    -2.9389     5.7307 ]\n",
      "                         :    var57:   0.014309     1.0190   [    -3.0058     5.7307 ]\n",
      "                         :    var58:-0.00090534     1.0093   [    -2.8753     5.7307 ]\n",
      "                         :    var59:   0.011805    0.99902   [    -3.1524     5.7307 ]\n",
      "                         :    var60: -0.0065903     1.0041   [    -2.9618     5.7307 ]\n",
      "                         :    var61:   0.029037    0.99592   [    -3.0625     5.7307 ]\n",
      "                         :    var62:   0.015083    0.99356   [    -2.9314     3.4363 ]\n",
      "                         :    var63:   0.018129     1.0077   [    -5.7307     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : Start of deep neural network training on CPU.\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 5  Input = ( 1, 1, 64 )  Batch size = 32  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 32 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 1\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 32 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 2\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 32 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 32 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 32 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 0 minimum error = 0.727319\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.579685    0.598459      0.7934    0.397758     25234.9           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.549862    0.573903    0.753182    0.386726     27244.7           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.526782    0.557337    0.758974    0.390371     27086.1           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.515026    0.550149     0.76185    0.386009     26564.4           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.506099     0.54766    0.753817    0.389283     27388.4           0\n",
      "                         :          6 |     0.501436     0.54892     0.75392    0.384535     27028.7           1\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.495232    0.547062    0.764611    0.398844       27296           0\n",
      "                         :          8 |     0.492558    0.550108    0.748829    0.382106     27224.9           1\n",
      "                         :          9 |     0.487075    0.547295    0.747514    0.382315     27338.6           2\n",
      "                         :         10 |     0.486563    0.552801    0.750552    0.382523     27128.3           3\n",
      "                         :         11 |     0.479847    0.548232    0.747407     0.38267     27373.1           4\n",
      "                         :         12 |     0.477728    0.551054    0.752606    0.384904     27152.4           5\n",
      "                         :         13 |     0.472777    0.548241    0.788859     0.41316     26574.5           6\n",
      "                         :         14 |     0.474832    0.552801     0.81184    0.415423     25185.6           7\n",
      "                         :         15 |     0.467355      0.5527    0.791413    0.396355     25272.3           8\n",
      "                         :         16 |     0.464497    0.555597    0.754405    0.385993       27100           9\n",
      "                         :         17 |      0.46432    0.561994    0.750507    0.381198     27034.3          10\n",
      "                         :         18 |     0.455704    0.554191    0.750133    0.381943     27116.4          11\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 14.5 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 32\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.19 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_CNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU.\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 6  Input = ( 1, 8, 8 )  Batch size = 128  Loss function = C\n",
      "\tLayer 0\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 1\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 2\t POOL Layer: \t( W = 7 ,  H = 7 ,  D = 10 ) \t Filter ( W = 2 ,  H = 2 ) \tOutput = ( 128 , 10 , 49 ) \n",
      "\tLayer 3\t RESHAPE Layer \t Input = ( 10 , 7 , 7 ) \tOutput = ( 1 , 128 , 490 ) \n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 490 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 5\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 128 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 0 minimum error = 0.724566\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.682284    0.683779     6.33363     2.66726     2723.13           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.664542    0.666299     5.93049     2.43823     2858.89           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.657923    0.662226      4.5551     1.22932        3002           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.644903    0.649433     2.88959     1.23792      6044.8           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.636815    0.641468     2.88706     1.22981     6024.47           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.624781    0.631931      2.8777     1.23329     6071.49           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.615373    0.623854     2.87863     1.23845     6087.14           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.609463    0.620342     2.93505     1.26359     5973.23           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.602371    0.612362     2.88639     1.22848     6022.02           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.594221    0.607292     2.97163     1.26441     5848.13           0\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.591111    0.605896     3.02428     1.24837     5621.91           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |      0.58638    0.601029     3.04167     1.24949     5570.87           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.588371    0.600554     2.99392     1.22993     5659.89           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.580936    0.595939      2.9973     1.23099     5652.46           0\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |     0.574149    0.591977     3.03486      1.2447     5577.13           0\n",
      "                         :         16 Minimum Test error found - save the configuration \n",
      "                         :         16 |     0.574376    0.591629     3.04371     1.25164     5571.19           0\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.568765    0.588576     2.99712     1.22925     5647.46           0\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.560806    0.581376     3.00231     1.24025     5666.08           0\n",
      "                         :         19 Minimum Test error found - save the configuration \n",
      "                         :         19 |     0.559501    0.581207     2.99448     1.22882     5654.54           0\n",
      "                         :         20 |     0.556095    0.582247     3.07304     1.28644     5588.27           1\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 68.7 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 128\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.659 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: PyKeras for Classification\n",
      "                         : \n",
      "                         : \n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \u001b[1mH e l p   f o r   M V A   m e t h o d   [ PyKeras ] :\u001b[0m\n",
      "                         : \n",
      "                         : Keras is a high-level API for the Theano and Tensorflow packages.\n",
      "                         : This method wraps the training and predictions steps of the Keras\n",
      "                         : Python package for TMVA, so that dataloading, preprocessing and\n",
      "                         : evaluation can be done within the TMVA system. To use this Keras\n",
      "                         : interface, you have to generate a model with Keras first. Then,\n",
      "                         : this model can be loaded and trained in TMVA.\n",
      "                         : \n",
      "                         : \n",
      "                         : <Suppress this message by specifying \"!H\" in the booking option>\n",
      "                         : \u001b[1m================================================================\u001b[0m\n",
      "                         : \n",
      "                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored\n",
      "                         : Elapsed time for training with 10000 events: 36.7 sec         \n",
      "PyKeras                  : [dataset] : Evaluation of PyKeras on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.617 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "                         : No variable ranking supplied by classifier: DL_DENSE\n",
      "                         : No variable ranking supplied by classifier: DL_CNN\n",
      "                         : No variable ranking supplied by classifier: PyKeras\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_PyKeras.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: DL_DENSE for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0: -0.0095330    0.98960   [    -3.1011     5.7307 ]\n",
      "                         :     var1:  0.0051908    0.98827   [    -3.0819     5.7307 ]\n",
      "                         :     var2:  -0.012209     1.0051   [    -2.9940     5.7307 ]\n",
      "                         :     var3: -0.0077444     1.0017   [    -2.9487     5.7307 ]\n",
      "                         :     var4:   0.032363     1.0029   [    -3.0402     5.7307 ]\n",
      "                         :     var5:   0.017022     1.0003   [    -2.9642     5.7307 ]\n",
      "                         :     var6:   0.018034     1.0069   [    -2.9881     5.7307 ]\n",
      "                         :     var7:  0.0048645     1.0053   [    -3.2664     5.7307 ]\n",
      "                         :     var8: -0.0032578    0.99210   [    -4.0772     5.7307 ]\n",
      "                         :     var9:   0.021695    0.99335   [    -3.0462     5.7307 ]\n",
      "                         :    var10: 0.00026812     1.0183   [    -5.7307     5.7307 ]\n",
      "                         :    var11:  -0.029610     1.0036   [    -3.3082     5.7307 ]\n",
      "                         :    var12: -0.0076649     1.0063   [    -3.1145     5.7307 ]\n",
      "                         :    var13:   0.019686    0.99738   [    -2.9630     5.7307 ]\n",
      "                         :    var14:   0.022015     1.0020   [    -3.2442     5.7307 ]\n",
      "                         :    var15:  0.0048294     1.0013   [    -5.7307     5.7307 ]\n",
      "                         :    var16:-0.00052943    0.99487   [    -3.1147     5.7307 ]\n",
      "                         :    var17:  0.0017536    0.99378   [    -5.7307     5.7307 ]\n",
      "                         :    var18: -0.0051237     1.0022   [    -5.7307     5.7307 ]\n",
      "                         :    var19:  0.0065580     1.0038   [    -5.7307     5.7307 ]\n",
      "                         :    var20:   0.018176     1.0165   [    -3.0984     5.7307 ]\n",
      "                         :    var21:   0.016093    0.99921   [    -3.1905     5.7307 ]\n",
      "                         :    var22: -0.0041778    0.99578   [    -2.9864     5.7307 ]\n",
      "                         :    var23:   0.013143     1.0148   [    -2.9490     5.7307 ]\n",
      "                         :    var24:  0.0060337    0.97521   [    -2.9590     3.4374 ]\n",
      "                         :    var25:  -0.018589     1.0008   [    -5.7307     3.4852 ]\n",
      "                         :    var26:  0.0057727     1.0204   [    -5.7307     5.7307 ]\n",
      "                         :    var27:  -0.018895     1.0017   [    -2.9445     5.7307 ]\n",
      "                         :    var28:   0.011385    0.99187   [    -3.0082     5.7307 ]\n",
      "                         :    var29:  0.0088995     1.0081   [    -5.7307     5.7307 ]\n",
      "                         :    var30:  0.0056477    0.99603   [    -2.9480     5.7307 ]\n",
      "                         :    var31:   0.027005    0.99818   [    -3.0540     5.7307 ]\n",
      "                         :    var32: -0.0069123    0.99381   [    -5.7307     5.7307 ]\n",
      "                         :    var33:  0.0020646    0.99297   [    -3.1347     5.7307 ]\n",
      "                         :    var34:  0.0027272     1.0099   [    -5.7307     5.7307 ]\n",
      "                         :    var35:  -0.013270    0.99462   [    -3.0901     5.7307 ]\n",
      "                         :    var36:   0.019274     1.0013   [    -3.0362     5.7307 ]\n",
      "                         :    var37:  0.0056244     1.0079   [    -3.4400     5.7307 ]\n",
      "                         :    var38:  0.0069643    0.98228   [    -5.7307     5.7307 ]\n",
      "                         :    var39:  0.0016025    0.99666   [    -5.7307     5.7307 ]\n",
      "                         :    var40: 0.00055175     1.0097   [    -3.2601     5.7307 ]\n",
      "                         :    var41:   0.019092    0.99409   [    -3.1673     3.4511 ]\n",
      "                         :    var42:  0.0094501    0.99496   [    -3.0118     5.7307 ]\n",
      "                         :    var43:   0.010563    0.99084   [    -3.0916     3.4255 ]\n",
      "                         :    var44:  0.0057154     1.0020   [    -3.3622     5.7307 ]\n",
      "                         :    var45:   0.013756    0.99252   [    -5.7307     5.7307 ]\n",
      "                         :    var46:  0.0023129     1.0004   [    -3.0491     5.7307 ]\n",
      "                         :    var47:   0.033810    0.99825   [    -3.2081     3.7228 ]\n",
      "                         :    var48:  -0.014985    0.99946   [    -3.7496     5.7307 ]\n",
      "                         :    var49:  0.0078714     1.0128   [    -3.0852     5.7307 ]\n",
      "                         :    var50: -0.0053581     1.0077   [    -3.1462     5.7307 ]\n",
      "                         :    var51:   0.014792     1.0023   [    -3.1302     5.7307 ]\n",
      "                         :    var52:   0.014085    0.99832   [    -5.7307     5.7307 ]\n",
      "                         :    var53:   0.028836    0.99597   [    -3.6136     5.7307 ]\n",
      "                         :    var54:   0.025281     1.0115   [    -5.7307     5.7307 ]\n",
      "                         :    var55:   0.029288     1.0234   [    -5.7307     5.7307 ]\n",
      "                         :    var56: -0.0044719     1.0103   [    -2.9389     5.7307 ]\n",
      "                         :    var57:   0.014309     1.0190   [    -3.0058     5.7307 ]\n",
      "                         :    var58:-0.00090534     1.0093   [    -2.8753     5.7307 ]\n",
      "                         :    var59:   0.011805    0.99902   [    -3.1524     5.7307 ]\n",
      "                         :    var60: -0.0065903     1.0041   [    -2.9618     5.7307 ]\n",
      "                         :    var61:   0.029037    0.99592   [    -3.0625     5.7307 ]\n",
      "                         :    var62:   0.015083    0.99356   [    -2.9314     3.4363 ]\n",
      "                         :    var63:   0.018129     1.0077   [    -5.7307     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.274 sec       \n",
      "Factory                  : Test method: DL_CNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.546 sec       \n",
      "Factory                  : Test method: PyKeras for Classification performance\n",
      "                         : \n",
      "                         : Load model from file: trained_model_cnn.h5\n",
      "PyKeras                  : [dataset] : Evaluation of PyKeras on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.627 sec       \n"
     ]
    }
   ],
   "source": [
    "factory.TestAllMethods();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: DL_DENSE\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:  0.0057930    0.99712   [    -3.1847     5.7307 ]\n",
      "                         :     var1:  0.0058083    0.99728   [    -3.1847     5.7307 ]\n",
      "                         :     var2:  0.0059004    0.99725   [    -3.1847     5.7307 ]\n",
      "                         :     var3:  0.0058396    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :     var4:  0.0059544    0.99700   [    -3.1847     5.7307 ]\n",
      "                         :     var5:  0.0057245    0.99680   [    -3.1847     5.7307 ]\n",
      "                         :     var6:  0.0066096     1.0003   [    -3.1847     5.7307 ]\n",
      "                         :     var7:  0.0062151    0.99941   [    -3.1847     5.7307 ]\n",
      "                         :     var8:  0.0066540     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :     var9:  0.0067823     1.0013   [    -3.1847     5.7307 ]\n",
      "                         :    var10:  0.0065501     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var11:  0.0068591     1.0016   [    -3.1847     5.7307 ]\n",
      "                         :    var12:  0.0063219    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var13:  0.0063785    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var14:  0.0065936     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var15:  0.0066219     1.0005   [    -3.1847     5.7307 ]\n",
      "                         :    var16:  0.0067364     1.0012   [    -3.1847     5.7307 ]\n",
      "                         :    var17:  0.0061154    0.99827   [    -3.1847     5.7307 ]\n",
      "                         :    var18:  0.0061471    0.99841   [    -3.1847     5.7307 ]\n",
      "                         :    var19:  0.0064147    0.99954   [    -3.1847     5.7307 ]\n",
      "                         :    var20:  0.0070218     1.0024   [    -3.1847     5.7307 ]\n",
      "                         :    var21:  0.0057432    0.99688   [    -3.1847     5.7307 ]\n",
      "                         :    var22:  0.0059601    0.99819   [    -3.1847     5.7307 ]\n",
      "                         :    var23:  0.0068817     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var24:  0.0064032    0.99957   [    -3.1847     5.7307 ]\n",
      "                         :    var25:  0.0063012    0.99948   [    -3.1847     5.7307 ]\n",
      "                         :    var26:  0.0064047    0.99962   [    -3.1847     5.7307 ]\n",
      "                         :    var27:  0.0062421    0.99892   [    -3.1847     5.7307 ]\n",
      "                         :    var28:  0.0057685    0.99691   [    -3.1847     5.7307 ]\n",
      "                         :    var29:  0.0066603     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var30:  0.0060916    0.99794   [    -3.1847     5.7307 ]\n",
      "                         :    var31:  0.0057694    0.99696   [    -3.1847     5.7307 ]\n",
      "                         :    var32:  0.0067821     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var33:  0.0073669     1.0037   [    -3.1847     5.7307 ]\n",
      "                         :    var34:  0.0061607    0.99837   [    -3.1847     5.7307 ]\n",
      "                         :    var35:  0.0061378    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var36:  0.0062884    0.99977   [    -3.1847     5.7307 ]\n",
      "                         :    var37:  0.0065205    0.99998   [    -3.1847     5.7307 ]\n",
      "                         :    var38:  0.0066631     1.0007   [    -3.1847     5.7307 ]\n",
      "                         :    var39:  0.0057950    0.99679   [    -3.1847     5.7307 ]\n",
      "                         :    var40:  0.0064812    0.99958   [    -3.1847     5.7307 ]\n",
      "                         :    var41:  0.0061854    0.99925   [    -3.1847     5.7307 ]\n",
      "                         :    var42:  0.0061370    0.99813   [    -3.1847     5.7307 ]\n",
      "                         :    var43:  0.0057640    0.99668   [    -3.1847     5.7307 ]\n",
      "                         :    var44:  0.0070901     1.0028   [    -3.1847     5.7307 ]\n",
      "                         :    var45:  0.0060098    0.99803   [    -3.1847     5.7307 ]\n",
      "                         :    var46:  0.0060669    0.99820   [    -3.1847     5.7307 ]\n",
      "                         :    var47:  0.0067749     1.0020   [    -3.1847     5.7307 ]\n",
      "                         :    var48:  0.0057983    0.99693   [    -3.1847     5.7307 ]\n",
      "                         :    var49:  0.0059339    0.99801   [    -3.1847     5.7307 ]\n",
      "                         :    var50:  0.0067546     1.0015   [    -3.1847     5.7307 ]\n",
      "                         :    var51:  0.0063016    0.99923   [    -3.1847     5.7307 ]\n",
      "                         :    var52:  0.0060385    0.99838   [    -3.1847     5.7307 ]\n",
      "                         :    var53:  0.0065581     1.0004   [    -3.1847     5.7307 ]\n",
      "                         :    var54:  0.0065533     1.0008   [    -3.1847     5.7307 ]\n",
      "                         :    var55:  0.0069978     1.0026   [    -3.1846     5.7307 ]\n",
      "                         :    var56:  0.0059293    0.99710   [    -3.1847     5.7307 ]\n",
      "                         :    var57:  0.0063542    0.99967   [    -3.1847     5.7307 ]\n",
      "                         :    var58:  0.0062839    0.99935   [    -3.1847     5.7307 ]\n",
      "                         :    var59:  0.0063015    0.99936   [    -3.1847     5.7307 ]\n",
      "                         :    var60:  0.0071258     1.0027   [    -3.1847     5.7307 ]\n",
      "                         :    var61:  0.0063313    0.99942   [    -3.1847     5.7307 ]\n",
      "                         :    var62:  0.0063288    0.99931   [    -3.1847     5.7307 ]\n",
      "                         :    var63:  0.0059676    0.99805   [    -3.1847     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0: -0.0095330    0.98960   [    -3.1011     5.7307 ]\n",
      "                         :     var1:  0.0051908    0.98827   [    -3.0819     5.7307 ]\n",
      "                         :     var2:  -0.012209     1.0051   [    -2.9940     5.7307 ]\n",
      "                         :     var3: -0.0077444     1.0017   [    -2.9487     5.7307 ]\n",
      "                         :     var4:   0.032363     1.0029   [    -3.0402     5.7307 ]\n",
      "                         :     var5:   0.017022     1.0003   [    -2.9642     5.7307 ]\n",
      "                         :     var6:   0.018034     1.0069   [    -2.9881     5.7307 ]\n",
      "                         :     var7:  0.0048645     1.0053   [    -3.2664     5.7307 ]\n",
      "                         :     var8: -0.0032578    0.99210   [    -4.0772     5.7307 ]\n",
      "                         :     var9:   0.021695    0.99335   [    -3.0462     5.7307 ]\n",
      "                         :    var10: 0.00026812     1.0183   [    -5.7307     5.7307 ]\n",
      "                         :    var11:  -0.029610     1.0036   [    -3.3082     5.7307 ]\n",
      "                         :    var12: -0.0076649     1.0063   [    -3.1145     5.7307 ]\n",
      "                         :    var13:   0.019686    0.99738   [    -2.9630     5.7307 ]\n",
      "                         :    var14:   0.022015     1.0020   [    -3.2442     5.7307 ]\n",
      "                         :    var15:  0.0048294     1.0013   [    -5.7307     5.7307 ]\n",
      "                         :    var16:-0.00052943    0.99487   [    -3.1147     5.7307 ]\n",
      "                         :    var17:  0.0017536    0.99378   [    -5.7307     5.7307 ]\n",
      "                         :    var18: -0.0051237     1.0022   [    -5.7307     5.7307 ]\n",
      "                         :    var19:  0.0065580     1.0038   [    -5.7307     5.7307 ]\n",
      "                         :    var20:   0.018176     1.0165   [    -3.0984     5.7307 ]\n",
      "                         :    var21:   0.016093    0.99921   [    -3.1905     5.7307 ]\n",
      "                         :    var22: -0.0041778    0.99578   [    -2.9864     5.7307 ]\n",
      "                         :    var23:   0.013143     1.0148   [    -2.9490     5.7307 ]\n",
      "                         :    var24:  0.0060337    0.97521   [    -2.9590     3.4374 ]\n",
      "                         :    var25:  -0.018589     1.0008   [    -5.7307     3.4852 ]\n",
      "                         :    var26:  0.0057727     1.0204   [    -5.7307     5.7307 ]\n",
      "                         :    var27:  -0.018895     1.0017   [    -2.9445     5.7307 ]\n",
      "                         :    var28:   0.011385    0.99187   [    -3.0082     5.7307 ]\n",
      "                         :    var29:  0.0088995     1.0081   [    -5.7307     5.7307 ]\n",
      "                         :    var30:  0.0056477    0.99603   [    -2.9480     5.7307 ]\n",
      "                         :    var31:   0.027005    0.99818   [    -3.0540     5.7307 ]\n",
      "                         :    var32: -0.0069123    0.99381   [    -5.7307     5.7307 ]\n",
      "                         :    var33:  0.0020646    0.99297   [    -3.1347     5.7307 ]\n",
      "                         :    var34:  0.0027272     1.0099   [    -5.7307     5.7307 ]\n",
      "                         :    var35:  -0.013270    0.99462   [    -3.0901     5.7307 ]\n",
      "                         :    var36:   0.019274     1.0013   [    -3.0362     5.7307 ]\n",
      "                         :    var37:  0.0056244     1.0079   [    -3.4400     5.7307 ]\n",
      "                         :    var38:  0.0069643    0.98228   [    -5.7307     5.7307 ]\n",
      "                         :    var39:  0.0016025    0.99666   [    -5.7307     5.7307 ]\n",
      "                         :    var40: 0.00055175     1.0097   [    -3.2601     5.7307 ]\n",
      "                         :    var41:   0.019092    0.99409   [    -3.1673     3.4511 ]\n",
      "                         :    var42:  0.0094501    0.99496   [    -3.0118     5.7307 ]\n",
      "                         :    var43:   0.010563    0.99084   [    -3.0916     3.4255 ]\n",
      "                         :    var44:  0.0057154     1.0020   [    -3.3622     5.7307 ]\n",
      "                         :    var45:   0.013756    0.99252   [    -5.7307     5.7307 ]\n",
      "                         :    var46:  0.0023129     1.0004   [    -3.0491     5.7307 ]\n",
      "                         :    var47:   0.033810    0.99825   [    -3.2081     3.7228 ]\n",
      "                         :    var48:  -0.014985    0.99946   [    -3.7496     5.7307 ]\n",
      "                         :    var49:  0.0078714     1.0128   [    -3.0852     5.7307 ]\n",
      "                         :    var50: -0.0053581     1.0077   [    -3.1462     5.7307 ]\n",
      "                         :    var51:   0.014792     1.0023   [    -3.1302     5.7307 ]\n",
      "                         :    var52:   0.014085    0.99832   [    -5.7307     5.7307 ]\n",
      "                         :    var53:   0.028836    0.99597   [    -3.6136     5.7307 ]\n",
      "                         :    var54:   0.025281     1.0115   [    -5.7307     5.7307 ]\n",
      "                         :    var55:   0.029288     1.0234   [    -5.7307     5.7307 ]\n",
      "                         :    var56: -0.0044719     1.0103   [    -2.9389     5.7307 ]\n",
      "                         :    var57:   0.014309     1.0190   [    -3.0058     5.7307 ]\n",
      "                         :    var58:-0.00090534     1.0093   [    -2.8753     5.7307 ]\n",
      "                         :    var59:   0.011805    0.99902   [    -3.1524     5.7307 ]\n",
      "                         :    var60: -0.0065903     1.0041   [    -2.9618     5.7307 ]\n",
      "                         :    var61:   0.029037    0.99592   [    -3.0625     5.7307 ]\n",
      "                         :    var62:   0.015083    0.99356   [    -2.9314     3.4363 ]\n",
      "                         :    var63:   0.018129     1.0077   [    -5.7307     5.7307 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "Factory                  : Evaluate classifier: DL_CNN\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_CNN         : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "Factory                  : Evaluate classifier: PyKeras\n",
      "                         : \n",
      "PyKeras                  : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_PyKeras        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       PyKeras        : 0.816\n",
      "                         : dataset       DL_DENSE       : 0.797\n",
      "                         : dataset       DL_CNN         : 0.769\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              PyKeras        : 0.136 (0.169)       0.475 (0.516)      0.777 (0.797)\n",
      "                         : dataset              DL_DENSE       : 0.108 (0.134)       0.446 (0.517)      0.749 (0.782)\n",
      "                         : dataset              DL_CNN         : 0.087 (0.107)       0.394 (0.422)      0.709 (0.732)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TestTree' with 10000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 10000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHYCAIAAAApvgy/AAAABmJLR0QAAAAAAAD5Q7t/AAAgAElEQVR4nO2dbbasKNJGsVfPS+2BtdrzqldzYuX7I+6hOAho+Ink3uuuKo+piI+kRAZBUM3zbAAAAABC/OvpCgAAAEC+YCgAAABAFAwFAAAAiIKhAAAAAFEwFAAAACAKhsIL6Pu+aZrKoe/75WFVVTVNc3flHPq+r6pqmqazSpO7tnumaZI9cvs7Ltc0zbMSZY5I6tE0Td/3Zz3WJd5TLpurv6Sxl8PpnPtlh9yZIW8Sz24cx+WRD1Vznue5rutlrfbRdd2yido9XdfZy8n2Rh6XKHNE0hh1XV930VOaTf5c2gLHcbzoMY3j6D2gr3pqgEcha+yPj+C3tG1b9+C6rt3+9dXILxW5a3ePvATlN1PTNHVdq36f1XWd7gvBLGyvcRylXX0+n3t+rcI+dnwjNjJNU9u27tOXa51+IciTfz9dAUjx+XyMMfPCrzBNk3hr+763396S3IBy4+4rT+7O3bOj0ypJotuw4zXDMAzDgK2QM7e1cJrBV4GhkC/p73zXdcMwbHkvTNMko/urPzVWj5TLbSxtS8XM777/LKws2wvfqNKWw3Zc/RTcxnD6pfu+H4Yhdl3RJH3dU2RRNebltc59NFtK217hWPnLE3d8cTbe+Clfye0iP/VNATU3DXGAHhlx3P6MlgfbEizLMo0xdV0Hj0wXZRY+6o3Dlsui3FHV5ejJvHCoyHXlyGAdYi08qOfylFjwR/r2g5W3RcUiKoJ34V16KamcZXULPp1ggWkSYR/B1hgc6lrWdlk99xLLZmOPD+50L+RWSf7sus7ud6+yrOrG1uvdtf3TazbLyIBgY1h9NLao5XNMf3HmSIzC6lnu7QePCeoWlGujyFvUg6zAUMga9xWz8WD7p31BSOyC9810z5KP5LBxHO2Ry8LlGLe09Bt/iVerZdCiHRG3x8zzbK9oKzmHulj39ZQIhwwq7N2Xexfei8+VyD3M1cQtyrvx4FOLiWbvOnHWqqTbSRgKy4+W1w22HO9pLluO12zSVkKiMdtu0h7pWWmJOiyrYQk2IauG21yXpaUrHMQt3y0z9pTdtrE0FGLiey3KVizYfoLfvqVc20UOqoetkDMYClnjWei24wwSfKPFfpN5hwVfHN5LyjvG+1E7bzMUlu+OYFWX79OlWeDtWdZnXnRvXrHBU5Y7g+93rwtP/OC2xywPWP2JGTzA27lUJnitLbj9gSXWuwStiuXORDt0T/GMnqBTJ2immIUpFvtJnajDrDQU0g961SMyx7HHBBVI71x+TxNnxSofPHHZxmLmnVtITOTVZwG5wbPJHff3q8vSYnC/bMEv/+x8UYNnWbz3ghj+q+/QVUMhaHDMoTdF7F2cMBSC70TvilvuPXZYuuTYb3G3qOUxiV/wiat7Nx4sZBzHLV4oj2BLizU5MSO8ErZYb95N2WYTsxJiHclGn01MZG+/ylBYqr180Fu+fUuCCsRk9Bqh92fQglyWFqySd8yqoaASOdYeZsgVns1rkN464cNM90m2kOCLzzss9opxy1m+9VYNhUSxq7XaaCjELr08JvE7xruR4GFpEyRI7Adf+qzljXtnWbdT2uDYgn2Pj7/Z6B+2kqaNmOBFXeeZd0Cs2XhPMPZAYyIHTb2NhkL6sFhRW346Bw9IyLhUYPWOZicSIma7e6waCrH78gyOLfY0ZAh5FF6DTZA3O31DOqXdubHENjFiVVVt28oMRm0JxphhGJbp/47XzTgjuyqWlZFbO32mmTwOq9vGOtupibGz+r6XP62wTdMcqXyzoO/7eZ5NKJWCmzbUS+zh3UIad0qFd4nYvQSL3dcGtKTvaDm5d8tZq2i/OFKNtm2DZ+34/u4geMvMcXgdTI/Ml8RUJXk7x97LV9TEvVZd19J/7KvAPa/y7cTqc8XrrK5r6WttAozV+ei2GmKoBc+SOXh938vb//P5tG1b1/W5tk73e0auO2HSNolpmtwuX9UbzfMsLe3ObA339JcHkUru++KkzyKzCGwBQyFfpBsexzFtldsp7EsSH+2oyfGOp2maz+dje7sT8X6sb+f03jRN3/dt28oVY787l9geWgQMnuV6EaQLtxbJedX/hRgEXde5l/DEFMNoS2niA2+aRk5xb8fetccpD+4Ks1VuYfnt211ha1+qvs5bzmqaJpYe4xQwRMqAoYd8kVfY9l+cy53Lb+mRbsMrbfcrIHjiFdaD+UlhmZAo2APtS49jQrfm+YftReVmN/ZScrD9Ne+dtewJ+r6Xfvfc17Rbmq1/+qlJxZbHyGhFsHrWilptbxvvLvY9sonAE+ceEXB3hZfEZHSdTKpq2LNi34LEF0d7LcNYQwE8HCMBcZahYS7L+GHvgcqfwZlRy8O8wrfECe4IZozVahnitLziajBjMKDaKzlY4XQEe7AycyTCPFjnYJWCOiRInBXcubx0cJJC8CrBw+wNjs48xlj4+u7pkd610vfoNebtc2qCO4Mhk8tGHmwMcyROdvXbly5nuT/9xQlOgliW5hUVfOjeztVgxmD7X1ZgY8Qo5AbPJmvc7qFepLXxvlrBF59ZJDbZ8uILGgpuMLxblPfGSXd+Xq2CsyeCtVo1FNx6dpHMUbELLU9x7yIoUWwKw1LwMTLZT/VmXJ0UEJTUXjrWdXmsejiWatuLujX0DJSgLJ6FF+xTvY5qWYgrSCKAP/Zo3Ntx7XJPRqM0FFwl09++1XJWZXQPXioQq4Z7jL1x+wVffnGs3Zn4sm8RGUPhpfBscsd7xVjcL62w/LJ559Y/qZoTfgj3xGWv4Bbl7hwjydqCeHaGW8LqvaQNhdWSl8Xuq8wc6ZaWHW1QDflo9ff99rOCHXzwR/xuQ2HZ3pbSdU76ZBOyWd3SvIvGCl/+ZnVv0L1QwlAI3tpSyeDjk532GO/WEvuDpcVOXy0/qIB3s0EFVs+aN3wL3AO6eArnVZExFF5KNS/6AMgQCWu3f6qG8+1gpASI7Y6Ht3Vwh/A3hu5vLO0UdpR8YmW2FCVRC6d/9exTOF3S1Yu6V7QKBKdmLPerkEL2NeYtj8b9oh0Pmjm3NK/M5V0kBFHde/CYjY/vuu81PMnTlgpcgvwSiv1m0v6WhRPZ8rMSPIKNORhV8M2kfSoAu8GjUCzys3V0ZldOP+kQeOgPIs+l+z2rENLQmLcg02L3+QsBEjA9sljk95bkZZPZaPJijQU9wNW4UyWxElTEGvNq9OX3YDMi4PCH83nYowFX4kVuL+PR4E7sU3i6Iq+ExpzAnbLxdF2gQBh6AAAAgCgMPQAAAEAUDAUAAACIgqEAAAAAUTAUAAAAIAqGAgAAAETBUAAAAIAoGAoAAAAQBUMBAAAAomAoAAAAQBQMBQAAAIiCoQAAAABRMBQAAAAgCoYCAAAARMFQAAAAgCgYCgAAABAFQwEAAACiYCgAAABAFAwFAAAAiPLv40VM0zRNkzGmaZqmaY4XeISqqp6tAAAAfDnzPD9dhTOpdt9P3/fDMAQ/6rqu7/v9lTpAVe2/IwAAgIOU1w3tGXqYpqmqqmmaxnGcF4zjKAc8ZSsAAADAWew0FMQaCA40NE0zTVMB9hSjGFpQTAVyqUAuFcilBcUSlOYhKc/nAwAAL6K8bujQrIeqqhhiAAAAKJhDhsI8z13XDcNQVZWMOJxUqyzAE6UFxVQglwrkUoFcWlAswdE8Cn3fSwCjMaZt25IshsJ8RzeAYiqQSwVyqUAuLSiW4LShlGma+r7/fD52zziO96dVKG9wCAAAXkR53dBRj4LYB1VVtW37+Xy6rpNJknVdt217ShWfAk+UFhRTgVwqkEsFcmlBsQSHDB+rbDDDUlVV9zsVyjPlAADgRZTXDR1K4Zy2AwpTCgAA4As5NPQQnBhZTDAjnigtKKYCuVQglwrk0oJiCXZ6FMRE+Hw+3kJQ0zS58Yw30Pf9RYkc8IhoQTEVyKUCuVQglxYUS7DTUHB9Bp7/oOu62+ISpmkahiGHVSsBAACK5JCh8OAow3I25umUF5ByNSimArlUIJcK5NKCYgkOxSg8G4vQNE3XddeVT6PRgmIqkEsFcqlALi0olmCPDVVVVV3Xsnpk8Df9nYp7kzBPtAorc21sy2xolwAApVGec2LP0IONQuj7PsMJDvuCV+W52gdcVdXV/fhuQ0QsDLeqmWwYY+Z5frwabBS5YRtYJvXJfIMv47OKFcbr76q6zKOwvQbqMy6rIF4KAIBnKc9cOJrCuaqq6qenbJqmqqoMfQzXMs+6f8bM1Z5/WxwQlalW/10uCAAAFMShzIzyO16WjjTGSNRC27ZlGFNXWYWJMpPOidkEbIXq18eb2G4raP0T5dnRl4JcKpBLBXJpQbEEhzwKn8/Hy+Is7oQynAoPNJq4EyJ6hv1nvQ6Jf0q0/gm+ZiqQSwVyqUAuLSiW4JBHAW4i2IJD7gfvOP+IiK2w/IZs9zoEjyRUAgCgGA4ZCrKWtOtUkI078yReZwbm7ony6rbBbvh1+K9TQ+eu3XvCmEh8hA1hyb2BZQZyqUAuLSiW4Kg0y1QK9y8t7cLD/sPeBU60px3xRvwpAdMBAAqivG7ohPuZpslmdH58zYXyntAlbDYjjs+RcB/HvjkXWBIA8CLK64aKu5/znlB5D3uFVevhtxrBAYuN/ElOcsAOKcB6+LoGdgzkUoFcWug7EuyZ9VBVlXgOqggn1/EhCnvS66xOtfg942E2Zk6SuNSfdvIzHSOWdCJVwvvzQ3xdAzsGcqlALi0olmBPMKONQrAZFKBA3K9NzPgL7v85MfjFC9qR3k57YtBWiNkEy/0FeB0AAB5nj6EgiZXMz1rP51YoH8pzH+1nqUPCb+R+tDhx3jB+kZ6Fsez+t5sOwdOfggamArlUIJcWFEuwR5qqqmRdKJkbuTyAWQ9fypZRp7UhiS3XOTJ1M1BaNqYDABRAed3Qnvvp+34YhsQBD2pU3hN6K8rQyGRJ673+loeeth4wFwDgFMrrhg7dj7dyYw4QufogK4rF+nuNyNtDZQ+aDjfYDTQwFcilArm00HckKO5+intCBZLo7HcNhG05LNEqVscpcDYAwHbK64b2BDP2fS+5lWKRjAVHOMIJ2K/Qso+3ezZ/zQKpITUTK8zCDljaDe4ejAYA+DZ2znowxjRNk+cqkbGfmFoTrzyr8GrUiiVmYCanTqyVqp5YEbMb0kbD8ngVNDAVyKUCubSgWILSpOFhv570UMIZDzcxWsEIBQAcpLxuaE9mRhcZhpDtHNZ6gNejyQ75zz/dFaLpIxPZRVcTR743RyQAQIJDhkLTNMMwuIbC5/MpJoVzMTdyG+crtppV2rn2DovBxI2GLSnJVy2GtNFAA1OBXCqQSwuKJTh/euSzcybL8/nACqfOoTBr74uVNSw2uBMYoQAom/K6oT3BjC6MNcDDnDqHwnjrYi/KdPcs3wXpQMjgfuwGAMicE2IU3D/thIiDxeYAnigtDyuWGKS4IJrBrA1PbFwMk8iGjfB9VIFcWlAswSEPyTRNbdsaY+q6ltmSn8+n67oH8yiU5/OBQ1wzh2L1nbI7LyQOBoC3U143dML99H1vEyq4kyAeobwnBKfxhNFwJJM0RgPAGymvGyrufsjX/RxvUuyybA3b7QZPro2jD19rPbypdWUAcmmh70hwQoyCjNH2ff+4O+FcCnvSN/AmxbTZGhQFR2MazO88Dd4xq9EMf0r41oCGN7WuDEAuLSiW4NCsB1lvuus6GXqwRgOKw5tIZJKO7d/QwtOZpLfPnvjnFMc+YPkJALiNQx6FYRjGcXQdCfLKy3MNCC0EwWopQTF/pkKkD9b7G7bMnlgpIeJs+JJ5EyW0rhtBLi0oloA8ClHwi2gpUzHvpoJvE42/IZGnIe1m+LM/nqqhbDdDma3rMpBLC4olII8CgAaVv2G9sBU3w8rpawtPrFYAAGCVE/Io1HVt93w+n7quHxx6IHL1QVAsahxETIGYiZC+iDaNdBneBVqXCuTSQt+R4OiiUOM4GmM+n8/n8zHGjONYRoCCwROlB8U2pYb8sQOigwvxSRM/hf3CP33hYCgjiIHWpQK5tKBYgtIMn/JMOXg3pyZsSC1lGXROrBkHZTgbALKivG7oaIzCNE3uj5sHkzdbqgg7yrmiegWDYgHSMQ17kzQk1sX+dfxaYoaNK2LnAK1LBXJpQbEEhwwfyaMgCz3IWg/yZxkxCgAXoolmWCspkgF614rY+BgADlJeN3Tofqqq8paAkvDGBzUq7wlBydwyMLH6jWB5KoATKa8bOmooLE+vqmocx6dmSBK5+iAopsKXK2E0YDHQupQglxb6jgQn51EQysijUNiTvgEUU+HLlQ5lUJYcnDqxGq+Tc0oGWpcK5NKCYgkOZWYcx7Ft22maxFxYxiiUYTEA3I19Z9lOXTaU7zL77gumgFxN/hhbXcJk42YAgBs4OvSQPuD+MQjcRw+CYiq2yhX8lu3VOfid3Rf2+OfcuywGWpcK5NJC35GguPsp7gkBGHPmLAmzy1z459yM4xgAcqC8bujoolAWVnkAuJDlYMTyz83vpuCQRHo84p9zkwMTWAwA5bEzmLHv+6qqbCxCVVVt27Zt6+58O+Tf0IJiKnbKlViPSpm+yYTSRW/PTpaIfLwi+JHWpQK5tKBYgj2GguRZ6rpO/Afy33Ec53mu67pt21Nr+BiF+Y5uAMVUHJUrNkviPHNhy6vztuUraV0qkEsLiiXYM5Ti5Vmqqsqd6fDs8Ex5g0MAW1n263flefxVQqHLVwJspLxuaP/Qg2yIfeBlUyhj9AFPlBYUU3G+XDHvgsbHkM7BsKmEa5avpHWpQC4tKJbgaDBjwTGMhZmEN4BiKq6SazXscdt1EwGP5omYR1qXCuTSgmIJjnoUJMOS3V+w3QDwMtJLVh7zMZwV85hDzkcASLPHUOi6bhiGpmnkTeEOQ7Rt69oNrwZPlBYUU3GfXKsWw8Yu/4KYR7M57JHWpQK5tKBYgj1DDzZhszHGzn2wS06XEaBg8ETpQTEVD8gVG5Xw9iQrlh6SMGv35doKqhTRtC4VyKUFxRKUFpxZXrgpwLWs/pBKJ3i+YJbEnxKYKwHvpLxuaM/Qw0afwVOuhSrCjnKuqF7BoJiKXORKrFopuCMUizrPDr9P0gUxLMcmPAMiF7leAnJpQbEEOxMuVVUVXGBamKapaZqnMi/NEXaUc0X1CgbFVOQo1zwr7IbA2fvNhX8K+W0r/DNRIkO5Mga5tKBYgj0xCtM0ydLSkmrJneMwTdPn8zHGdF1XTLACwJeyzMqw/HPxepUX7r5JlX8OM/MyfIGRCICnODqU0ve9GASfz0eMBuGUyu2ApUIfBMVUvFiuzSkgE+6ELfdOksfdvLh1PQR9R4Li7qe4JwSQKZqM0UcsBswFeBfldUOnLTMNAN+FvApdCyCe/NF9b3pGw+ry1mIWsKo1wFPszMz4DRAEqwXFVBQiVyKVU/SM6CyJlCZVwCwgt2OMQlrXjaBYgtI8JOX5fADexK4VLJfv6B3jEQYHA+RBed0QHgUAOI/gCpbrJ6kdDLElJBRVBYBtYChEwROlBcVUlCyXZy5sW04inbgpKFcwTRMrTpmyW9c1oFiCQ4bCNE2n5EDMk8J8RzeAYirKl0vvWvg5L2AumOSrPDbo8LW2Qvmt62xQLMGhWQ+Se9GuCwUA8AtvZkQkR1PkVD9xk0mO/nq2gjURmCIBcJCj0yPHcSzVSigvIOVqUEzFF8k1z4FZlBpzwTgWw+p0yj8n/k7vaIypTPU9tsIXta6TQLEEh6S5QllZQiKd3lHSQTZNs1xvgocNkC+75kQ4Z6snR/w5kZRNcCPldUOHYhTOdSdIxIMsJNG2bWzRqaqqhmFwjz+rAgBwLcvlpuLrUobO3rno1HKKxNfGLgDs4JDh0zSNLAHlsa9MsTmk4+/7fhiGZTnefvcUgXzdD4JiKr5drmAHn0wCnUjv+HM260f84dtblx76jgSHYhQSK03v4PP5jONoSx6GQcYXTryEisKe9A2gmIpvl8vzK9iNWKzi7/2xUMfgwb9ODMUu2I821fwNfHvr0oNiCXIxfGS4wfvF0HVdMArBzrNo29Y7pjxTDuAreCJ8ITYAUZLFAPdTYDc0H2Mcx7qubWld1+0ux6uMMaau6+WRXdfZyy0POKiD+T0O6u1hI72BYmwc2lj+c46JfU+XG973etNZgWs/rcaxDb6MDypm/yyGQ8GMfd/bVArCMAwnDhYsi5IhiXEc53kex/Hz+SyP2SeEPdctxNvDRnoDxdg4tDGHQx3nnwNUBf4U8I/PIHXWz7V/nWuqyvG5vmuDL+OzihXGoRiFYRg8z3/TNGI6XIRcUYyDpmnGcbz0cgDwAPPsj0Qcy9S0MfWCMdElrRmMgG/m6FoPXgzBchrCRoInPpvKqZhc1LeBYiqQK8XSu2AUSaDNbw/Ez9lbE8wXMJ2S1qUFxRIcNRROTGNQ17V1D1ifgf1TLiSjG/aUc6ddeJTqRLoOFFOBXJtYDkbozg6bCzuyL7xroSlalxYUS3Bo6KHrOpl3YP0BwzDUdb3PE2CXmJI/7VTJaZpstgbJyeh+ye1hAFAs7mCE3dj8Zl8ORpjN4xGx6ZQMRsD3cHQWh0QX2j/ruj7oY5DTV02N2GEkzXgQFFOBXCqqn6hGH72G35AKmtalhb4jQXH3U9wTAgCfY0kXfso4J7djtrYCPEV53dCeoQc3YWLQf1DqepIAkAXyFt67IuVPGTvHI7yZEYxEQPHsMXyqqpIhhlhA0IPGFO6jB0ExFcilIirXGd4Fs2s8ImfXAq1LC31HguLup7gnBADreN383peAZy4wEgE7KK8bOpqZMbiTpZ8B4FaWsyh3TYv3plNumUgZzOe449IA2bJzeqSYAjLfwY1IkBmSZcQolGcVXg2KqUAuFZvk8lI66gMXfoqZvcSOqqgFk0HgAq1LC4ol2ClNwso+PkPyCDxsAMgncMEwGPF9lNcNHbqfDOXIsEoA8AzXzKIkcAHSlNcNHYpR8LQoLDSB1N9aUEwFcqnYI9ex1SJ+ypiXgQvrZ4XSP2svfQRalxYUS3DIUPCSLvd9X1VVMeZCYSbhDaCYCuRSsV+u8+IcnTL+IXVKKM7xHouB1qUFxRIcMhTatq3r2uo7TZOs/nBGxQAAzuOwa8FE+pK0xbBciNK8cC1K+HKOxiiM4+jNcQjuvI2Ega+90/LGma4GxVQgl4rT5Dop44LZmwTatRKui12gdWkh4VKCQ6tH5slZT6iwJ30DKKYCuVScJtdyCuXekm2VVHMpXSpTXWQr0Lq0oFiCQ0MPdV23bWuDEuwaEGXkUQCAMllGLRwtTxHt+HicI4CWox6Spmk+n4+758FxB4P76FFQTAVyqbhErvOGIX7Kc4YVHh2DoHVpoe9IcM79WKfC476E8p4QAFzLqeaCylYwjrlAroViKK8bOjT0IEzTJIbC41YCAICaMyZEOIXNbuzC9tn5DEBAtpyQR6Ft22EYpmmSPArBlaLeCPk3tKCYCuRSca1cwVwLJ11xe66FE20FWpcWFEtwdHqkrOwgxkHf933fD8PwoNelPJ8PANzKSetEGM0wxD1zJuEeyuuGjg49eHkYxWIoJjkjAHwdwcTP5yVzDB/JPAjImJMNhZLAE6UFxVQgl4q75TrJXFj+stxiK5jD5gKtSwuKJTghj4K7p6Q8CoX5jm4AxVQgl4pn5Lp3Wan5z/VO8C7QurSgWALyKAAAbODsRau3Ry0YAhdeRXnd0An3406PfNyXQNKMB0ExFcilIgu5gp4A/SIyzqlXJWXKQq5XQd+RoLj7Ke4JAUB2HPMueEMPTIgojPK6oT0xClVVieegSvK4dwEA4BKWsQu6s3/HLSbzMl2UaAFgO3sMH7v4U3rKQ9u298cr4D56EBRTgVwq8pXrQAbopX0Qu0dtyEK+cuUKfUeCC+9H8i9dVHiM8p4QAGTN4SDHjSMRhDe+hfK6oRPyKDRNI5mbbYpGoZhczgAAUebZH4lQTqH0Jk9GD1vMnFRdBWA3hwyFvu8lj0Jd17JnGIbHQxNiMRM7yrmiegWDYiqQS8UL5Dr2I9JdSip12DZHwgvkygwUS3DIUBiGoes6G7LQNM04jl5ahfuZI+wo54rqFQyKqUAuFe+Qy1by2JpSG22FRDqmd8iVEyiW4OjQgze+sCXIEQDgK9CPQTinsj415MIlaz08PvpwCniitKCYCuRS8Rq5DscrOKemFpFKxyu8Rq5sQLEEhwyFruvatpUwRkEWnj6rcs+CJ0oLiqlALhUvk8uzFTSd0PYsCwlb4WVyZQCKJTg6i6Pv+2EY7J91XT877lDevBQAeCvnJXBMvNaYNpkb5XVDxd0PSTOeA8VUIJeKF8t1ICOTccwFla3wYrkegr4jwaGhh6qqCo5bLOxJ3wCKqUAuFS+W61jUgnNeKs2zPwzxWrWe4sUN7HoOGT4ZznEoz5QDgELY61rYvogUwxA5UF43dOh+pmlq27aua2+aw4M5GXEfPQiKqUAuFYXI5Xb5LDiZE/QdCY56FILplR7UqLwnBAClYXv9a1aFMNgKj1JeN1Tc/RT3hACgNG5fcBJb4U7K64aOJlwqGPJvaEExFciloii5vF5EmWVhmWhheVhVVawgpaKoBnY2pRk+5ZlyAFAsJ82cdArwS7AmAk6F2yivG8KjAADwECctOGkJmA7OClJHrgXfDIZCFDxRWlBMBXKpKFYuN8uC/h6Xq+OKUOHxCGyFOMU2sDPY4yFZTZzw4KJQ5fl8AKB8jo1B/JQRnRNBYOOdlNcN7bmfVcuL6ZEAADqwFUqhvG5oz9CDdXaN42iM6brO+/PkOiqpIuwo54rqFQyKqUAuFeXLtcz0vHckwikjbBwwBrGk/AZ2gEOGT1VV4zh6Aw3PGlPlmXIA8HXsTeDoFBBeSgq/wg2U1w0dDWYMhiNktfoDAMDLOLyIlO2oPH+q51fAtQBbONlQEBPhwWDGE8ETpQXFVHpv5h0AACAASURBVCCXiq+T69gwhCdXzFYAy9c1MA3/PnLyOI5t21ZVJXEJ0zR9Pp/HYxTOojDf0Q2gmArkUvGNcs3z7jWpXY9C4FMzm59hiMpUmA7mOxvYZo4OpUzT1Pe9LA1V13Xf98+6E8obHAKAb2fvIlI/Z6/EK2AonEt53VBx98NSoc+BYiqQS8VXy6WPbXTl+jXugK0Qgb4jwdH76ft+Gbr4YDBjeU8IAODgPIiYrcAkiCsorxs6FKMgja+u65MqAwAAIdx4BdnQdEXzPFtbwe3GZjNbW4FgBYhxyFAwxizzKBRDeVbh1aCYCuRSgVx+bGPSXFjKha2QhgaW4GjCpdyUzbBKAABnspzIsPmlxxjEDZTXDR3Ko9B1XanuBACATPGSPRtFUiYSPMMOjg49fD6fqqq8MIUyMjOWZxVeDYqpQC4VyOUjakRMhIRcjEEEoYElOGoonB7J2Pe9MaZpmoSvQpI3ND+cWwELjUYLiqlALhXIFcYGLvwOWUjL5dkK9njPVjDfNAxBA0uQkQ01TVPbtmJ5SIZHMRo8+r4fhqGua8ny5EVTYhUCwNexKyPTlngF8022wlmU1w0dup/YEMO+X/lylpQp1kCwbu6SlU3TfD4fb9SNpBlPgWIqkEsFcq3wO9HCdrlYZ1Kg70hwdNZDcP++Mr1Fq4NrWCcMCHtWYU8IAGATu5Iybcnb+OfTLzAXTqG8buhQjMJSi90RA8FlJ6dpWu6p63qaJns80y4AAIzZuYhULLbR/A5ZMF8Z4Qh/mM9mX5njOHonGmPqul4WLtR1LdEMXdcFD9ing7thbIDP4iM2UIyNmzdi31M2/A1j7D+tvBb/GKfQ52/wmg2rw/EC7Z/FcCiPQoyzpkfGvAXzPItToeu6YRiWn+7AnusW4u1hI72BYmxctxH7nrKx3LCo5DUL/jlm4UjI5E5P3PB0OOsRlMGhoYelQWAnNx4pNoE3G7NpmqWhAADwvUhfZedMbu665vgYhDH/DEMwAPGFHDIU2rZd7uy6bkdRdsqDa2QsDY6maW7L5lReQMrVoJgK5FKBXCoq89OZn2crOIUXaCvQwBIcGnqYQwSTH2yhrmtredjZj/ZPG734+XysrWDzLlwBjUYLiqlALhXIpWK3XIkTy07zTANLcDQzo/kZgBBnwJFBh2maqqqy9qyEN8p+ya1kjGmapus615NRRrpoAICTmecdAxDmt1/B/+j3PAj4Eo46W+Qnvrvn4MLTwXmS2w8jacaDoJgK5FKBXCr+yLUrY6OJZ2H682mJ2Z3pOxIcuh+xElzLYJkq8WbKe0IAAPs5Nbvzn09/nAqF2QpnUV43dDQz49J/ENx5G+U9IQCA/Rx2KpiFrUDGxjTldUOX5FEog9goHcRAMRXIpQK5VFRL+0ApoNvVeeIXaRnQwBIcNRS8OQ5X51G4k8JMwhtAMRXIpQK5VITlOmAr+B+Z2ZoLZYQ30sASHJr1MI5j27ZVVdm1oc3ePAoAAHAJblTj3hkQaXd6kZkVwHLCUErf93YOwu4kCmdB5OqDoJgK5FKBXCoCcu1aW9J8TWAjfUeCQ/fT9/3jloFHeU8IAOAcDtsKidmS5v22wlmU1w2dP+vhWcp7QgAAp3GBrYCh4FFeN3QomNFLklgYBMFqQTEVyKUCuVRE5XI7sF2SLksuI6qRBpbghNUjA+2mCGOqjLu4ExRTgVwqkEtFSq5dsY0b14t6L+Xd0YkcMhRsGCMAALwG11ZQnLS+BgTTH4qkNMMw4T7S3mmRVvOloJgK5FKBXCo2ySVvy10ZGxORCuadwQrMekhwdK2H2Ed93z8S5FjeEwIAuIRdgY2kdl6lvG7oUDCjLAHlrh5pt9u2zW3mJAAAHCSdrvHOmsBtHPUoeEmWpmlq23aeZ7txQh014D56EBRTgVwqkEuFQq4Dy0vGLvHGCZP0HQmO5lEIjFT9JFd4JMtCeU8IAOBCjq1DnU7BZF5lK5xFed3Q0UWhmPUAAPBi9i4v+XNS4CzXOHh1cgUQDk2PlIRLXddZt4HkX5IhCfPyZSTLswqvBsVUIJcK5FJxtVyJqZJ/DviZMPkWaGAJjuZRMMYMwzAMg+yp69r6GMZxPFS1p6HRaEExFcilArlU6OSyaRU0a0uuLiz5ruQKNLAEpdlQWIUAAHvQByukF5Y03xqsUF43dChGYRmgME1TMRmzi7mR20AxFcilArlU3COX2x2uBitkDg0swSFDwUuW0DRN27Z1XR+tVB4UZhLeAIqpQC4VyKVij1xuVOPmXrMYW4EGluBQjMI4jnb1SAlTyG3VaQAA2APrRcEPRx+qJFYyxnRdl0MqRpJmPAiKqUAuFcil4pBcZ2dWeEX+JfqOBEfzKDRNI7MbynMkFPakbwDFVCCXCuRScUiuY2MQLx3sp4El2GP4rLaDBxUvz5QDALgb7yW/7aW6cb2onP0Kp1BeN7QnRuHtCRI2Ut7DvhoUU4FcKpBLxVG55Fylb2A1C5OQZ1oFGliCE6SZpknGHezGg/CwAQBO40ByhZcGKxynvG7oaB6FqqrsxIe+76uqejyksYrwbK0AAL6BRLCCtQ/eld0Zjq4e6eZsNsb0fT8MQxkxCuVZhVeDYiqQSwVyqThTrlMzNmbrVKDvSHDUUFgmTnhkdWn36oU9IQCAJ3EdA0XbCmdRXjd0dHokAACUjNvnnZexEV7EIUNBlpm2QQl2oYfHQxpPgcatBcVUIJcK5FJxslyHbYVf+7OMVKCBJTjqIZGgBPunF7JwP+X5fAAAsuDYGMQvH0PRaRXK64ZOu58c5kaaEp8QAEAu6BMxxWZLFhypUF43dFqMgrUSmqZ51qlwFniitKCYCuRSgVwqrpJL3//FZkvmZh/QwBIcMnzsilAeZUyPBACAMJo5k9/mVCivGzrkUWjbtq5ryejcdd04jnVdd113Ut0AACBvNvwQf/t6UXBOHgVBpj88a0yRNONBUEwFcqlALhV3yKWJbYylVcjHqUDfkeCcGAUvLqGMGIXCnvQNoJgK5FKBXCrukEtzifwfX/41fJCjhoJ4EZqm+Xw+J1QHAADegu1c9w5A5JlTATwOGQrjOH4+n77vZcqDXXsph3mSx2E4TQuKqUAuFcil4gG5Xv6AaGAJzhxKmaZpmqZnV48sb3AIACBr9MEKZU9/KK8bOjPhksnAl1DeEwIAyJ3NsyW/IVFjed3QzqGHvu+rqpIYRlnioW3btm1L8t6UdC/3gGIqkEsFcqm4Wy59vxiMVHgQGliCPYaCrO9Q17UxRuyDuq7neZaECo87Fc6iMJPwBlBMBXKpQC4Vj8m11t2urhT1FDSwBHs8JFVV2cWfxGiwhUiuxmfzKMQ+oh0AAFwFkQo/MPTwBxuxmKH/YI6gLQdPlBYUU4FcKpBLxQNy6ZeijlXykamSNLAEpy0KVR6FmYQ3gGIqkEsFcql4Rq5jF33WkUADS4ChAAAAJ7EtBdPqkpLkX8qKf+87zUuWkOEAxHHKG2e6GhRTgVwqkEtFFnJV1UEfw51koViu7JFmNWFzGYtCAQDAHralVSh1+enyuqHi7qe4JwQA8D422AqrS0qad9oK5XVDxChEIQhWC4qpQC4VyKUiI7m2TVl/PP9SRorlR2mGT3mmHADAK9mWWaG8AYjyuiE8CgAAcAHbMisU1qcWCYZCFDxRWlBMBXKpQC4VucilMQKenSqZi2JZgqEQBTtXC4qpQC4VyKUiI7m2ZVZIc4OtkJFi+YGhAAAAtxCxFbasFEUKpgfJzlDo+77ve1lxKs00TV7ep3PBE6UFxVQglwrkUpGXXJuXgVhW+7ZIxrwUy4yMDIVpmqqqmqZJlqBcNQLatt1iT+wGT5QWFFOBXCqQS0V2cm2uz1O2QnaK5URGszgkD3Rw9eol0pjsatfu/nzuCAAA/iGZhSk2T9K8bapked1QRh6Fz+djvQiyEXMYyKd1XV9aHzxRWlBMBXKpQC4VucsVql5spah7yF2xR8nFUBCbwFtcKmgoTNOUdjacRWEm4Q2gmArkUoFcKjKVa2+wwg2OhEwVy4NcDIUgQUOhbdtxHBNnVbuw57LBBhtssHHVhpd+8fcxwaTOdsOYP2MQz9/F2kZh7Fxm+h6Wq1c3TVPXdXpV6yOGoT13nueqquZ5dvewkd5AMdWGJZP6ZL5ROeO+bKxuZP1lnGcjHWrkmXrd7T1fmXMVK4ysDYUlsry1GAp2u+/7tOmwj1If+XWgmArkUoFcKsqTazbzH3eCqa4YiShPsRPJxVCwUx7cLn/Z/XddZ7etoXCFlQAAAFfhOBVMyGEgToXq9/SBq20FiJHRLI6maT6fj9TH3TYRt4E7ndJSnTcv5cSivgQUU4FcKpBLxQvksuMLoXra0QfvLq6bJ0nfkSAXj4L5Sbhk24eNWJymSZwHN1PYk74BFFOBXCqQS8UL5LJOhfCHfqTCn/0/ToULqpO9Ys+RneETnCe5nfJMOQCAMsnMqXAW5XVDxd0P7qPnQDEVyKUCuVS8Ri6xBnYZCuZUW4G+I0HWeRSepbAnfQMopgK5VCCXipfJFRxliNzCRY6Elyl2LxgKAACQL8tghWwHHUoFQyFKqTm2rgPFVCCXCuRS8Rq57O/4qkrENsZu58TAxtco9gQYClHwRGlBMRXIpQK5VLxVrrWcjNfxVsVuAUMBAACeY56DwYw/H/75yM/rzOjDjWAoRMETpQXFVCCXCuRS8T659v6gP2v04X2K3QiGQhQ8UVpQTAVyqUAuFSXJFXMqXHQVWIKhAAAA2bDZGmD04TYwFKLgidKCYiqQSwVyqXi3XMv5kNc7Fd6t2MVgKETBE6UFxVQglwrkUvFKuR4NU3ilYneR0aJQZxEzDGkHAABZE18pKrb2NNxAgR6FOYK2HDxRWlBMBXKpQC4Vr5crWf9/loH4CVM47lR4vWJXUqChcBYYrVpQTAVyqUAuFSXIFY9UuIISFLsMDAUAAMiGZIe97M5PdCpADAyFKHiitKCYCuRSgVwq3i2XuwDEXbxbsYvBUIiCJ0oLiqlALhXIpeKr5DolocJXKaYFQwEAADJjzamAA+BOMBSi0BC1oJgK5FKBXCqKkusWW6Eoxc4GQyEKnigtKKYCuVQgl4oS5IrcwkW3VoJil4GhAAAAWRIZgIh16kx8uAgMhSh4orSgmArkUoFcKgqU6+I7KlCx88BQiIInSguKqUAuFcilohy53BtZ9OXLFI0HrlOKYheAoQAAABmj6cIZfbgCDIUoeKK0oJgK5FKBXCpKk2uZkPFsB0Bpip0KhkIUPFFaUEwFcqlALhXFynXZ6EOxip0BhgIAALyHuK3w509GH84GQyEKnigtKKYCuVQgl4oC5bp49KFAxc4DQyEKnigtKKYCuVQgl4oy5VqkVfBu88joQ5mKncS/n67A+cQMQ9oBAMA3UJnqlJWiQCjQozBH0JaDJ0oLiqlALhXIpaJYuSK5Go/fb7GKnUGBhsJZ4IHQgmIqkEsFcqn4Wrl2OxK+VrEtYCgAAMB78OISfv5k7sN1YChEwROlBcVUIJcK5FLxFXKxzPRdYChEwROlBcVUIJcK5FLxLXJVlVk4FfaNPnyLYrvAUAAAgFexrVNn9OEsMBSi4InSgmIqkEsFcqkoX65tkQrbKV+xA2AoRMETpQXFVCCXCuRS8eVy7Rh9+HLF0mAoAADAa4lkafzzIaMPZ4ChEAVPlBYUU4FcKpBLxTfLte/ev1mxVTAUouCJ0oJiKpBLBXKp+Aq5kks/aEcfvkKxvWAoAAAAQBQMhSh4orSgmArkUoFcKr5FrogbYMftf4tiuyhw9cizSHuiaFVBspIlc19i5tXLDeRSgVxaUCwBhsJ+aFg5k5XJAgDXUlXiXZjnebnoA0tOH4Shhyjbe5ppmqZpCu73NlbL2XFA7OpXcNuFvgFMGRXIpeIb5Tq28PQ3KrYZDIUo2x0Gbdu2bevt7Pu+bdumaWRbNlyqqur73js+0Vibpmnb1uuqp2kKXt3DnhWsyXZWLwTbwSOlArlUfJFc8TtVORK+SDE9BRoKVYSrr+v2+ub3j+++7z+fz/JT95SNP9YTV0lABw8AxXJ2OmfwKNBQmCNoy1E1srquh2Fw93w+n7quZVt+xHumg/3UHj+Oo1mYAt5VPINjGAavnGma+r73fBXLYr1j3J1Lp0XwYDgIbzEVyKXiS+WK3PWW/Ixfqtg2CjQUzkJlWyy7Ya//ruvaPebz+Sz78qZpuq7zDA4Xz+CQDXcoQcYvJGqhqio5wP5XNj6fj3wl5BhblBwvYxm2TK/ALVLARnB1qkAuFV8u147b/3LFVoj9/n4pt92ReyFjzDiOdV13XSd7ZLuu67quZY94C5bbtgQ5Vz4ax3F5RSlTipU9su2W5p7rHmkP6LpuWXMp3Kuq7HcLFNNnizg58KKqAsAJGDP/frnJS8DMxsy3vg3Ke/ngUYii/QHdNI11BngOA/Pzu9+OArj+BtcxsByk8HDDHYZhWLolzG/nwWq17ZH2dFsH6+fwyodTwEOjArlUfK9ce+c+fK9iGyCPQpRZ6Ynq+14MheW4g1DXte2V5Ve7IDvdeEPPAlhiLYmmaTyrwj0xVo3kfUSPOTJdApZoG9iXg1wqkMtjNZsCiiXAo3AmEogwTVOwTxVngPcz3RgzDIMds5h/PP8Jp0LXdXKVYHc+Oez2AYgrwrsLkigAQL5E5j6QbekEnhjvuJAT7yhdlAmN9Fs/gex3B/7tkcYYd+cyXkEO806cf2IU3HLci3o18a5unBgFt2TzExvhxh94BbqBFy9qMPlXNf8aZgVyqfhSuQ6EKdzWd7wRPApRZr0nSn6CJ3z7EksYDCzwDtsSXrD0W4zjKFmbqqr6fD6uD8BOgggiH8mJbdtKPaXAYRjuSUTxbexoYN8Mcqn4arl2vay+WrE1qsLUqaqb7ui2C+1gOW1Sdq4GGQRP3HhubuT8gADgEqyJMM/GjU+c5T83vRDKe/kUdz/nPaF0UeU1hcLI/wHlX8OsQC4V3yuXGAchQ8EkbYXb+o43wtBDlMKeNOQGDUwFcqn4drmqyihF+HbFkmAoAABA0RBhdYzs8ijY2YOJQXE7BTF92EHKcx9BVtDAVCCXiu+Va57/jD5Ulcw92BiI/b2KbSAjj4K33EAsB0BVVZLXKH3YcWg0cCk0MBXIpQK5tKBYgoxsKDd7sWQ5XNbN2788rNRZD24qxuVHKreKO0kyWNryFPtoVq8eq2ei2Ivg9wHA97IMadwQz3jexYt7+VyQm2En5vdiSCa0NpKXwii4utKJ9dn+6d8nEbzWMjGDm8nR+3PLfXl4sscOCDYYs1g1KlbJm9teVm07SP41zArkUvHtci0zL/3ZFZXltr7jjeQy9BCb+r88zN15aVLhOSeT0DWPZCnqI2MurmVQ17W7zIQJmR3uc4m5ASRBtT1FMjW5D2hp9u2ufxmggArkUoFcPpX9fzhkAcUS5GIoBEnbATLuYHMIWqpd2HO3bzxF3/diK5xSWizJUhBJGRl8Lt7aE03T2EWwnmXfw2WDDTZevSF/+HvuvHpBZG0oxHovCXschmEcx+UP632uFXuu3ZBHHvzo5PvUI3d9Vje8MWO0PdjzQFi8xbWPrEp1Iokn+OxG/jXMaqNyxn3ZWN1YfX0Vv2H5Z0/l77lOscLI2lAI0ve9LEYw/3aJn06pj3yJJ6Nd3MHifrpc/VIQj4Kc2zTN0kSwi1AIr8sJfTrf08BOAblUfLtc+tv/dsWS5JJHIRhXH+yNxJGQWzfzr3+dZnI93l5lsezEAbL01HIShHg4xJEwDIP3pLquy+2pAUDhVLpsChAkF0PBGCNRddJNSo9i+xX5her+Tk3P8TuFqrwpLhG8IYzVNFZN08gAhKuPtRuaprFrUbrHXJod6418TwM7BeRSgVxaUCxBRoaCRB5Yu0+mPsp+O4IuG94Y+UVPV1Xs33//fcpFN5q9Mf//PoIxoasVGIbBrUDbtp6n58SIyyLhraQCuVQgl5l/UjRuPfzrFYuTkaFgjJnneRmE3zRN8aEi2xHH/ufzsYaU0Uc1yq9/OUusrh1RhzIAYf90HULCMAzLDBAAAPfjjj5UprptyelCmMvixDtKF+V9emfCpbqu0ymS0vmXvIPdDA3B0myBy5KlYrGES27JwWKXmRVOJP+2nX8NswK5VCDXPM+zUaRduq3veCOljcrcNs7kXeisixYQcaNKzHAdjDgCfDvVPxMijX27/jEYLnw5lPfyKe5+MBSMMfHBiMf779so77sKADp+GwpGXrAYCnryilHIivc+bC/Rtcv3GAr5894G9gjIpQK5jEnFMy7DFFAsQWnSPOVRgNzgAQFAYPTh561wnVOhvJfP+zIzAgAA7GCe58iaUJACQyFKAXGFkDM0MBXIpQK5/rD5lz2KJcBQiFKY7whygwamArlUIJcWFEuAofAOph+CHx0pc0tpdue5FQAAuInlYtMMQmznnnQNt3HiHaWLMs8lXDK/Ex+ZtQxLqwV6mZG8FEyS3dl+6t24JIhUXf0e8qyVS/41zArkUoFc/2DTLNkdobRLt/UdbwSPQpQ5J0+U23nLGgo7ki4LTdN8Ph9b2jiOn8/HLe3z+aSdBLsvDS5ZNbD8QS4VyPUPSylCrgQUS1CgoVBFeLpep9H3/ZH1lrx1ImQpSNcykJUhY6ez1BMAvJKCeoGbKdBQiDlPtOXkbFvIb/odwQHBZSf7vneLSi9N2TRNXdckbjpOzg0sQ5BLBXJtwQ1TQLEEBRoKZ1GkJ2qapi0rOsp4RMwQkYW/GYA4SJEN7DqQSwVy/WKDGiiWAEPhHKp//euUf5k4x2Q8ggEIACiTLF60rwFDIcrpnqgcWqZEMno7p2laDiWIwyDmNkgPT8AWcHWqQC4VyBUmLguKJcBQiKLyRM1//7367+8Nx2zMI7a7nw4GN/R9v7QejDHjOA7DEBuAmOeZAYgj4OpUgVwqkEsLiiXAUHgZ8ut/GAZ35oIqqrGu67Zt3RxKn89HkiV4SNxi0IYQuq5LfAoAkAsRO4C0S1vAUIiSlSfq8/nIJE+JGxjH0XUn2E+F9K98iWds29YW2HVd7BQSKlxHVg0sf5BLBXJFiSztiGIJSlsN86llps+66J2NVYyAUuMMylvpFQAO4Sw5/edNO8t/Tn5RlPfyKe5+MBSMMXFPQKlmwZLyvqsAcJQ/9gGGgo5/P12BfFE97KzcVrHlo8w3GQr5U97b5FKQSwVyaUGxBKVJ85RHAXKDBwQAPj8eBSO/7vAobINgRgAA+CZycgC/AgyFKFmNJkB50MBUIJcK5NKCYgkwFKIU5juC3KCBqUAuFcgVJi4LiiUgmPEFeJGJ+wISg3maAQC+EfwHGkqLuTgxiiRd1J3TI5c7EymSzI8l4ZoXVVXVdb1jWer3kn88Uf41zArkUoFcUX5ep5UxNopxNvNtfccbYeghSlZPuuu6+QdZuXF7r/+FVsIryKqB5Q9yqUCuKBFlUCxBgYZCFeHpep2Gu7CT1/0vrYGglTBNU9/3rk/ClmZ32mOWK0h55wIAvI55nlnnYSMFxih8lfuobVu77oN06qsjDrLStKz2NAyD3KN0/LLfPcYYIwtQySXE3pJzE2mdYAuvaGD5gFwqkEsLiiUo0KNwFlk1Gvv7vu976a2la3ftgGEYbLiiLBMVLOfz+czzPE3TPM91XVvfgN0v2+M4iilgj5GP5Bh37UrYR1YNLH+QSwVyaUGxBBgK5/Cv6l+n/IuteWp/wU/TJPEKsl+WnDY/vbg7IlDX9TzPn8/H3dn3vdgWQtM01s4Q/4Ewz7ONiPQWkpZT3BMBAN5HQePRV1Oas+WpWQ+bFjWvzJZUoct8olVVJaY5SE3cmQ4ycGDHFOz4gvk9pmCRjr9pGnsJOcs41oMbxCCmQ3rmxePk70jMv4ZZgVwqkCvFYuIDsx7SFBijcBaqJ/33/PeGg9YNhR1BlzI0IIMF7k7ZkMAFt+F6UQtLx8A0TW5cghvPaB0J0zS1bSvmhbbCIBT2Krka5FKBXCnm2XMnVMX9Zj4Xhh5ejx19iPXZ7qiEmBS246+qanWdSSlczrV2DPYBALwdTIONYChEecuMSrEAuq5LHDOOo6ReaJqm67q2bWXKqBvMaLETHJqmkWMk0EGOtNNN67rGXDjCWxpYJiCXCuTayo9OKJagNH/LU8tMX5qZcctZdqRgIzaaYcsBbvrn1RMzobxhQgA4k6oyv8MUziu4tJdPcffzfYaCG7oIlvK+qwBwJhgKmyGYMYrqYT/ltpLrktXgjZT3NrkU5FKBXFpQLAGGQpRXNJpXVBKC8OxUIJcK5NKCYgkIZgQAAIAoeBSirHqiiJKFI+DqVIFcKpBLC4olwFCIkm40NCk4CE1IBXKpQK6NzD8TJFEsAUMPAADwfWAZbAZDIQojC1pQTAVyqUAuFcilBcUSYChEwROlBcVUIJcK5FKBXFpQLEGBMQoxw5B2AAAAoKVAQ4GlQp8CxVQglwrkUoFcWlAsAUMPUWg0WlBMBXKpQC4VyKUFxRJgKAAAAEAUDIUoBMFqQTEVyKUCuVQg1yYcLwKKJcBQiIInSguKqUAuFcilArm0oFgCDAUAAACIgqEQBU+UFhRTgVwqkEsFcmlBsQQYCgAAABAFQwEAAACiYCgAAABAFAwFAAAAiIKhAAAA3wqzIjfw1rUe+r43xjRN0zTNw1XZxomJxPMs6lyyvcc8Fcv2HpHrqaLOJc97zFau8nifR2GapqqqpmmapqltW7EYAAAA4AreZ5GJC2GaJmNM3/fDMMy/03Dmaa7mWTHu8cHS8izq3NKKL+rc0vIs6tzSMiyqMpX8L7eK5cP77qeqqnEc7YjD8s88H3aeFeMeHywtz6LOLa34os4tLc+izi0tw6L+GArGzCaviuXDy4YexJHgxSXITgAAADidtwYzuniGwomZOM9N6plnxbjHB0vLs6hzSyu+8DDwmAAAB6hJREFUqHNLy7Ooc0vLrajZmGo+rbQiKcFQcB0MhTl8AADgauZf/wOflw09AAAAwJ28zFBwpzx4OwEAAOB0XmYoGGPqum7bVrbFRMBQAAAAuIhXzuJwQ07cuZEn8rrMj3eyRZy+76dpan64q2o5sr0tSRqxL88htkUuUYnWZZRfxi9vWqv0fY9EYeZ3Mo7jOI4XlWyMqeu6rmtjTNd1V1zlpWwUR5oWGmrbkhx8R82yZKNcXdfJYdLMLnoP5M++L+PXyrWK6Ik+Qd5qKFyHfKNkW15Jj1YnL7aI4+3/Zg1Vbcm+0G+oWJ5slMt9m0vnd0vtsmPHl9E9BSzjOGJ3pvnS71gCr63QdFy2iOO9jMROv6Ny+bG9Ldlfyd/8Ht8i1zfbnR475PryBhZjHMeu60Qr3vZB3hfMeClkfkywURwZaE8c8CVsb0vTNHlLlnwh21tXXdcSoyBD7/dULzc2yiUj7iLUNE2fz+fLQzqCSPQG0QkJMBTW+dqX0RbS4siqXWKqg4nI1bat+F3AYynX5/P5fD5t27J+7JJg6+q6bhiGtm3btq3rGrlgBxgK62CDJ4iJI6uBD8MwjiPvJstSrqZp6rqmjQWJyTLPsxgK0gveW6l8Wcollrq408dxxKMA+8BQgPPp+75tWwnD5sWURn4iy/Q2u40TK4aN3hdoXWnEn2fzzYit8HSl4H1gKPyCzI8JNoojI+44EjbKJVFU7jz478wNsFGuL1QmCG8quJUnIymzxJ1w9c2Tr4IkxKnrWjycdrq2yxOVfZ4tcnnHf3NQ+ha5vMnu5osnlG6RaznrgRdaAsOshwglrB55LjK4bpM/EmXmEhNHAqplWzZsmm1h/sqQ/i1ygWWLXE3TdF3ntq6vHabZIpfMd/BS2d5cTyiAV6ZwvoHg7CMQEEcFcqnYKBeqCsgFN4ChAAAAAFEIZgQAAIAoGAoAAAAQBUMBAAAAomAoAACk6Pu+cnAThFRVdfq0C1l6QHWKO/2haRqpVd/36ejFBzN2TNPkXlrqLFyRcMyb+uFeNy216vkWHCuKoQAAEMWuVyITyiVptO1dLkq/reoppTIy71HmRo7j2PyQOPFBQ8FdpEO6cDdPhje5+hRsQk9VcjN7VszU8Cg2y9xTCRwAAPLHGGOtBOHqtEXavFtuVqVXrOruVjJYYXNl4qN9ac22CPsK8feBRwEAIIX3+36aJpu2yHVN2xEKcWjbLMviS7eudXu8u3Oji9t10duLyrJYUoL8Fl8OPbjXsju9tZVt4e6PbCnKfuQe747I2DwN7rlywPIu+r73VpT17l08Ioma24otx4M8Vd2dcmlZUUX+tEMPnv72RmS/J2xVVf/3f//nHiy3E8yrXQhPWyoAAPliuzTPryCYn9++cpibxVx+topJ4SU4t+fK/nEcvXzMwZ+8Uo3l8a5HIbbt1Ufuxb2QWxnjJMa2F/Xu0W67lfF+UiduxHUYyCWCSc1nx6OzWrHEjbgVc2tV17UUbn7nArcXDV7Cuy9jzF9//eUVWBgYCgAAKbquc5esdHsC25Es97tdlO0C3R7LPT7WkwUP8C4dG3qw+70VH+R23At5B0jh3o2s3q/d767Esez7g/5518HgWgxexdxztzwIu9DMqqEQ09D8Dp6QnX/99Zfd/u9//xvUtjBY6wEAIIXr1hZX/zAM8yKnresYdw0LE4mHl4UYhNW1P8Sh7cXKeXMHEue69VkG3KULj13C3T87jgG5LyltY7CkHQQReWWReinH/NZ/tWJ1XQ/DIMdsDy2U69qzvMfn8Z///McY83//93//+c9//ve//4mtIDRNIyNBhUGMAgBAFG8U3wYoHB+KrqqqbVvpnLwx+y3I6uQH65Ao3G5vMRQsEgRgjJGpIqsXEjvJPX2eZ+nszc8Kc+4VV+96mqZ5nqXD9oIq0oiJIxddPauu6//973+yXaRl4IGhAAAQRX5ounu2dM8bPQTzPG9JeGAv2jtsrIkc5tZn6YfYV7gX/eeuO2XdA6t1s3GCQWSYwKvYllqJwSFzWVdPEUSljff+3//+9/P5LC2hMiMZMRQAABLUdS2/++2eoN9bDpPtHb3FahfodsDGmeywBS8af9kxe/36Fv+5d7+uISL+gJj3PmijuDulNOmDxT9ha76lYm6GBhVy1kZHiIw+eOMOQnrY4q08GSABAJA9y1e//cj8jt2z2Ig5L3bPC5Sz2M5pjk8WsHMy7SmyfzWYcf4dLWhCYX3eAbGAxNj9LgMzE8H/weNjpQVrnqiYd7w7Y8It0IZzutdaZshY3q/9M5hOw1yZAeJBWGYaAGAd17W+eszGKfVemVuCE7dUY/e52sKDx8toQqJnsVGcy6JiVz+lYrtLc0+0Zy0f8eqNvxcMBQCAo7jdhnQYbtagr6Kqqrqu00ZSVVVv16eqqr/++kvGIAQxEIvM4oyhAABwFC8oT2b3PVedZ7AirHYrEsL50tA/CXtcGkNVVWx/WuyNAQDczJFxgTLYmNrh7UgShadrcR8YCgAAABCF6ZEAAAAQBUMBAAAAomAoAAAAQBQMBQAAAIiCoQAAAABRMBQAAAAgCoYCAAAARMFQAAAAgCgYCgAAABAFQwEAAACiYCgAAABAlP8Hetgu4Hbx71sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader)\n",
    "c1.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## close outputfile to save output file\n",
    "outputFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
